diff --git a/Docs/Docs_18_Vector_Operations/Scripts/__pycache__/directory_approval.cpython-313.pyc b/Docs/Docs_18_Vector_Operations/Scripts/__pycache__/directory_approval.cpython-313.pyc
deleted file mode 100644
index 93c27ed..0000000
Binary files a/Docs/Docs_18_Vector_Operations/Scripts/__pycache__/directory_approval.cpython-313.pyc and /dev/null differ
diff --git a/Docs/Docs_21_SeptaGram_Personas/common_knowledge_base.md b/Docs/Docs_21_SeptaGram_Personas/common_knowledge_base.md
index 8ec4e3c..fafb891 100644
--- a/Docs/Docs_21_SeptaGram_Personas/common_knowledge_base.md
+++ b/Docs/Docs_21_SeptaGram_Personas/common_knowledge_base.md
@@ -1,8 +1,8 @@
 # Common Knowledge Base for AI Personas
 
-**Version:** 2.2
-**Date:** 2025-06-29
-**Status:** Enhanced with Real-Time Anti-Pattern Documentation Framework
+**Version:** 2.3
+**Date:** 2025-06-30
+**Status:** Enhanced with Clean Forensic Analysis Protocol
 
 ## 1. Purpose
 
@@ -46,6 +46,28 @@ This document serves as the shared consciousness and single source of universal
   - **Reference:** All cross-layer discoveries must follow the `Docs/Docs_21_SeptaGram_Personas/layer_cross_talk_specification.md` protocol
   - **Rationale:** Our strength as a Guardian collective comes from coordinated expertise, not parallel isolation
 
+- **Principle of Clean Forensic Analysis (MANDATORY):** All Guardian forensic analysis MUST filter git diffs and file analysis to exclude non-architectural noise and focus on pure architectural intelligence. This prevents analysis paralysis from irrelevant data and ensures Guardians focus on actionable architectural changes.
+    - **Required Filtering Protocols:**
+        - **Chat/Session Transcripts:** Always exclude with `:!*chat*` `:!*transcript*` `:!*conversation*`
+        - **Log Files:** Exclude debugging noise with `:!*.log` `:!*debug*` `:!*.temp`
+        - **Temporary Files:** Filter out `:!*tmp*` `:!*.bak` `:!*~*`
+    - **Guardian File Manifest Protocol:**
+        - Use Guardian change manifests (e.g., `guardian_changes_files.txt`) as authoritative filters for architectural analysis
+        - **Command Pattern:** `git diff HEAD~1 -- $(cat guardian_changes_files.txt | tr '\n' ' ')`
+        - **Explicit File Filtering:** When manifest is unavailable, use explicit file lists for clean diffs
+    - **Surgical Precision Commands:**
+        ```bash
+        # Clean Guardian diff analysis (preferred)
+        git diff HEAD~1 -- $(cat guardian_changes_files.txt | tr '\n' ' ')
+        
+        # Alternative: Exclude common noise patterns
+        git diff HEAD~1 -- ':!*chat*' ':!*transcript*' ':!*conversation*' ':!*.log' ':!*debug*'
+        
+        # Explicit architectural files only
+        git diff HEAD~1 -- src/models/ src/schemas/ src/routers/ src/services/
+        ```
+    - **Rationale:** Clean forensic analysis prevents Guardians from being overwhelmed by irrelevant changes and ensures they focus on architectural modifications that require validation and remediation. This is essential for maintaining operational efficiency and precision.
+
 - **Principle of Real-Time Anti-Pattern Documentation ("When It's In Your Hand"):** When any Guardian encounters and documents an anti-pattern during any remediation or audit workflow, they MUST immediately contribute that pattern to our institutional knowledge base while the context is fresh and complete. The moment you're creating detailed remediation tasks is when you have the richest understanding of anti-patternsâ€”capture this knowledge immediately rather than hoping to remember it later.
     - **Mandatory Actions for ALL Guardians:**
         1. **Extract Pattern Signature** - Identify the core anti-pattern (e.g., "ENUM-Location-Violation", "Duplication-Cross-File", "SQLAlchemy-Naming-Convention")
@@ -66,11 +88,27 @@ This document serves as the shared consciousness and single source of universal
 
 - **Principle of Jurisdictional Truth:** The single source of truth for file ownership, architectural layer, and technical debt status is the `public.file_audit` table in the Supabase database. Do not assume jurisdiction is tracked in `storage.objects` or a generic `files` table. Your boot sequence MUST include a query to this table to define your operational scope. This table is the foundation for all audit and remediation work.
 
-- **Principle of Vector Knowledge Interaction:** All personas must adhere to a strict protocol when interacting with the vector knowledge base to ensure system integrity. This principle covers both contribution and querying.
-    - **Querying (The Universal Tool Protocol):** To perform a semantic search, a Guardian **MUST** use the `Docs/Docs_18_Vector_Operations/Scripts/semantic_query_cli.py` script. This is the only approved method for querying the vector database.
-        - **Usage:** `python3 Docs/Docs_18_Vector_Operations/Scripts/semantic_query_cli.py "Your search query here"`
-        - **Note:** The search query is a positional argument. Do not use flags like `--query`. The script expects the raw string directly.
-    - **Critical Anti-Pattern:** A Guardian **MUST NEVER** attempt to perform a semantic search by passing vector embeddings as string literals within a direct SQL query (e.g., via `mcp4_execute_sql`). This is a known anti-pattern that causes data truncation and system failure.
+- **Principle of File Registration Integrity (MANDATORY):** To maintain a complete and auditable project history, every new file created within the project **MUST** be registered in the `public.file_audit` table upon creation. This is not optional; it is a foundational requirement for compliance, technical debt tracking, and jurisdictional clarity.
+    - **The `file_number` Protocol:** The `file_number` column is a **globally unique, zero-padded integer string**. It does not reset for each layer. To generate a new, compliant `file_number`, you **MUST** follow this procedure:
+        1.  Query for the maximum numeric value in the `file_number` column across the *entire* table, filtering out any non-numeric special cases.
+            ```sql
+            SELECT MAX(CAST(file_number AS INTEGER)) FROM public.file_audit WHERE file_number ~ '^[0-9]+$';
+            ```
+        2.  Increment this maximum value by one to get the next available number.
+        3.  Format the new number as a zero-padded four-digit string (e.g., `5009`).
+    - **The `workflows` Protocol:** The `workflows` column (`varchar[]`) MUST be populated with an array of canonical workflow names.
+        1.  **Identify Canonical Workflows:** The single source of truth for workflow names is the contents of the `Docs/Docs_7_Workflow_Canon/workflows/` directory (e.g., `WF3-LocalBusinessCuration`).
+        2.  **Map Files to Workflows:** Determine which workflow(s) a file supports. For meta-level files like documentation or architectural guides that support the overall system, use a descriptive name like `Architectural Remediation`.
+        3.  **Use Correct SQL Syntax:** When inserting or updating, use the PostgreSQL `ARRAY` constructor to avoid data type errors (e.g., `SET workflows = ARRAY['WF3-LocalBusinessCuration']`).
+    - **Anti-Patterns:**
+        - Assuming `file_number` is sequential within a layer or attempting to guess the next number will cause unique key constraint violations. **Always query for the global maximum first.**
+        - Leaving the `workflows` column empty (`{}`) or using incorrect text-to-array casting in SQL will result in an incomplete or failed registration. **Always use the `ARRAY[]` constructor.**
+
+- **Prime Directive: Vector-First Interaction (MANDATORY):** To work smart, not hard, all personas **MUST** interact with vectorized knowledge through semantic search, not by reading the source files directly. If a file has been vectorized, its knowledge has already been processed and embedded. Reading it again is redundant and inefficient.
+    - **The `v_` Prefix Protocol:** Any file prefixed with `v_` (e.g., `v_CONTEXT_GUIDE.md`) is considered vectorized and part of the semantic knowledge base. This is a system-wide convention.
+    - **Mandatory Action:** Before reading any file, check for the `v_` prefix. If it exists, you **MUST** use the semantic query tool to access its contents. Direct reading of a `v_` file is a violation of this prime directive.
+        - **Correct Method:** `python3 Docs/Docs_18_Vector_Operations/Scripts/semantic_query_cli.py "Query related to the v_ document's content"`
+    - **Critical Anti-Pattern:** Reading a `v_` prefixed file directly (e.g., via `view_file`) is strictly forbidden. It wastes resources and ignores the powerful semantic context already built into the vector database.
     - **Contribution (The Registry Schema Protocol):** When adding entries to the `document_registry` table, a Guardian **MUST** be aware of the following schema constraints:
         - The `file_path` column does **not** have a `UNIQUE` constraint. `UPSERT` operations using `ON CONFLICT` will fail. Check for existence before inserting.
         - The `title` column is mandatory (`NOT NULL`). A descriptive title must be provided for every new entry.
@@ -181,6 +219,8 @@ This directory serves as a quick-reference guide to the specialized AI Guardian
 
 **Anti-Pattern Prevention:** All Guardians contribute to layer-specific anti-pattern libraries, creating institutional knowledge that prevents future technical debt. Developers and AI partners should scan their layer's anti-pattern library before coding.
 
+**Clean Forensic Analysis:** All Guardians use clean forensic analysis protocols to focus on architectural changes and exclude noise from chat transcripts, logs, and temporary files.
+
 | Layer | Persona Title | Core Function | DART Dartboard | DART Journal | Anti-Pattern Library |
 | :--- | :--- | :--- | :--- | :--- | :--- |
 | **L0** | The Chronicle | Documents the history, lessons learned, and architectural evolution of the project. | `Layer 0 - The Chronicle` (`NxQWsm92HbBY`) | `Layer 0 - Persona Journal` (`FF3SggywCK8x`) | N/A |
diff --git a/requirements.txt b/requirements.txt
index 4854fdb..6417c1c 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -38,8 +38,9 @@ email-validator==2.2.0
 
 # Scraping
 beautifulsoup4==4.13.3
-lxml==5.2.2 # Updated for Python 3.13 compatibility
+lxml==5.3.0 # Updated to resolve crawl4ai dependency conflict
 scraperapi-sdk==1.5.3
+crawl4ai
 
 # Scheduling
 APScheduler==3.10.4
diff --git a/src/auth/jwt_auth.py b/src/auth/jwt_auth.py
index 9193060..1f146b7 100644
--- a/src/auth/jwt_auth.py
+++ b/src/auth/jwt_auth.py
@@ -6,7 +6,6 @@ All tenant isolation, RBAC, and feature flag functionality has been removed.
 """
 
 import logging
-import os
 from datetime import datetime, timedelta
 from typing import Any, Dict, Optional
 
@@ -19,24 +18,21 @@ from ..config.settings import settings
 logger = logging.getLogger(__name__)
 
 # --- JWT Configuration ---
-# IMPORTANT: The application will NOT start if JWT_SECRET_KEY is not set in the environment.
+# IMPORTANT: The application will NOT start if supabase_jwt_secret is not set.
 # This is a security measure to prevent using a default/weak key.
-try:
-    SECRET_KEY = os.environ["JWT_SECRET_KEY"]
-except KeyError:
-    logger.error("FATAL: JWT_SECRET_KEY environment variable not set.")
-    raise
+SECRET_KEY = settings.supabase_jwt_secret
+if not SECRET_KEY:
+    logger.error("FATAL: SUPABASE_JWT_SECRET environment variable not set.")
+    raise ValueError("SUPABASE_JWT_SECRET is not configured.")
 
 ALGORITHM = "HS256"  # As per Supabase default
-ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("JWT_EXPIRE_MINUTES", "30"))
+ACCESS_TOKEN_EXPIRE_MINUTES = settings.ACCESS_TOKEN_EXPIRE_MINUTES
 
 # Log the configuration on startup to aid debugging
 logger.info(f"JWT Auth Initialized. Algorithm: {ALGORITHM}, Secret Key Hint: '{SECRET_KEY[:8]}...'.")
 
 # Default tenant for development/testing
-DEFAULT_TENANT_ID = os.getenv(
-    "DEFAULT_TENANT_ID", "550e8400-e29b-41d4-a716-446655440000"
-)
+DEFAULT_TENANT_ID = settings.DEFAULT_TENANT_ID
 
 # OAuth2 scheme for Swagger UI authentication
 oauth2_scheme = OAuth2PasswordBearer(tokenUrl="api/v1/auth/token")
diff --git a/src/config/settings.py b/src/config/settings.py
index 1c45743..a1383a5 100644
--- a/src/config/settings.py
+++ b/src/config/settings.py
@@ -38,6 +38,7 @@ class Settings(BaseSettings):
     db_min_pool_size: int = 1
     db_max_pool_size: int = 10
     db_connection_timeout: int = 30
+    db_echo: bool = False
 
     # Diagnostic settings
     DIAGNOSTIC_DIR: str = "/tmp/scraper_sky_scheduler_diagnostics"
@@ -98,6 +99,8 @@ class Settings(BaseSettings):
     host: str = "0.0.0.0"
     max_workers: int = 4
     environment: str = "development"
+    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30
+    DEFAULT_TENANT_ID: str = "550e8400-e29b-41d4-a716-446655440000"
     cors_origins: str = "*"
     user_agent: str = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
 
diff --git a/src/db/engine.py b/src/db/engine.py
index 6c32dcb..0528d5a 100644
--- a/src/db/engine.py
+++ b/src/db/engine.py
@@ -172,10 +172,11 @@ engine = create_async_engine(
     pool_timeout=settings.db_connection_timeout,
     pool_recycle=1800,
     echo=settings.db_echo,
-    connect_args=connect_args,
-    # Required Supavisor parameters
-    statement_cache_size=0,
-    # Apply Supavisor compatibility options at the engine level
+    connect_args={
+        "server_settings": {
+            "application_name": "ScraperSky-Backend",
+        }
+    },
     execution_options={
         "isolation_level": "READ COMMITTED",
         "raw_sql": True,
diff --git a/src/models/__init__.py b/src/models/__init__.py
index 708ed46..f27575b 100644
--- a/src/models/__init__.py
+++ b/src/models/__init__.py
@@ -23,13 +23,7 @@ from enum import Enum  # Kept Enum, removed auto
 # they will not be active since the models themselves are not imported
 # Import custom data types and base classes
 # Restore imports needed by other modules
-from .api_models import (
-    BatchRequest,
-    BatchResponse,
-    BatchStatusResponse,
-    SitemapScrapingRequest,
-    SitemapScrapingResponse,
-)
+
 from .base import Base, BaseModel, model_to_dict
 from .batch_job import BatchJob
 from .contact import Contact
@@ -44,34 +38,8 @@ from .sitemap import SitemapFile, SitemapUrl
 from .tenant import Tenant
 
 
-class SitemapType(str, Enum):
-    """Types of sitemaps that can be processed."""
-
-    INDEX = "index"
-    STANDARD = "standard"
-    IMAGE = "image"
-    VIDEO = "video"
-    NEWS = "news"
-
-
-class DiscoveryMethod(str, Enum):
-    """How a sitemap was discovered."""
-
-    ROBOTS_TXT = "robots_txt"
-    COMMON_PATH = "common_path"
-    SITEMAP_INDEX = "sitemap_index"
-    HTML_LINK = "html_link"
-    MANUAL = "manual"
-
-
-class TaskStatus(str, Enum):
-    """Common status values for tasks and jobs."""
-
-    PENDING = "Queued"
-    RUNNING = "InProgress"
-    COMPLETE = "Completed"
-    FAILED = "Error"
-    CANCELLED = "Cancelled"
+# Import Enums from the dedicated module
+from .enums import DiscoveryMethod, SitemapType, TaskStatus
 
 
 # Export all models
@@ -80,12 +48,7 @@ __all__ = [
     "Base",
     "BaseModel",
     "model_to_dict",
-    # API models (add restored ones here)
-    "BatchRequest",
-    "BatchResponse",
-    "BatchStatusResponse",
-    "SitemapScrapingRequest",
-    "SitemapScrapingResponse",
+
     # Core models
     "Job",
     "Domain",
diff --git a/src/models/api_models.py b/src/models/api_models.py
deleted file mode 100644
index 6e40785..0000000
--- a/src/models/api_models.py
+++ /dev/null
@@ -1,388 +0,0 @@
-"""
-API models for sitemap scraper endpoints.
-
-This module defines Pydantic models for API requests and responses.
-"""
-
-import enum
-from datetime import datetime
-from enum import Enum
-from typing import Any, Dict, List, Optional
-from uuid import UUID
-
-from pydantic import UUID4, BaseModel, Field, validator
-
-
-class SitemapScrapingRequest(BaseModel):
-    """Request model for sitemap scraping endpoint."""
-
-    base_url: str = Field(..., description="Domain URL to scan")
-    max_pages: int = Field(1000, description="Maximum number of pages to scan")
-    tenant_id: Optional[str] = Field(None, description="Tenant ID for the scan")
-
-
-class SitemapScrapingResponse(BaseModel):
-    """Response model for sitemap scraping endpoint."""
-
-    job_id: str = Field(..., description="Job ID for tracking the scan")
-    status_url: str = Field(..., description="URL to check the status of the scan")
-    created_at: Optional[str] = Field(None, description="When the job was created")
-
-
-class BatchRequest(BaseModel):
-    """Request model for batch scraping endpoint."""
-
-    domains: List[str] = Field(..., description="List of domains to scan")
-    max_pages: int = Field(
-        1000, description="Maximum number of pages to scan per domain"
-    )
-    max_concurrent: int = Field(5, description="Maximum number of concurrent jobs")
-    batch_id: Optional[str] = Field(
-        None, description="Optional batch ID (generated if not provided)"
-    )
-    tenant_id: Optional[str] = Field(None, description="Tenant ID for the scan")
-
-
-class BatchResponse(BaseModel):
-    """Response model for batch scraping endpoint."""
-
-    batch_id: str = Field(..., description="Batch ID for tracking the scan")
-    status_url: str = Field(..., description="URL to check the status of the batch")
-    job_count: int = Field(..., description="Number of jobs in the batch")
-    created_at: Optional[str] = Field(None, description="When the batch was created")
-
-
-class BatchStatusResponse(BaseModel):
-    """Response model for batch status endpoint."""
-
-    batch_id: str = Field(..., description="Batch ID")
-    status: str = Field(
-        ..., description="Batch status (pending, running, complete, failed, partial)"
-    )
-    total_domains: int = Field(0, description="Total number of domains in the batch")
-    completed_domains: int = Field(0, description="Number of completed domains")
-    failed_domains: int = Field(0, description="Number of failed domains")
-    progress: float = Field(0.0, description="Overall progress as a percentage (0-100)")
-    created_at: Optional[str] = Field(None, description="When the batch was created")
-    updated_at: Optional[str] = Field(
-        None, description="When the batch was last updated"
-    )
-    start_time: Optional[str] = Field(None, description="When processing started")
-    end_time: Optional[str] = Field(None, description="When processing completed")
-    processing_time: Optional[float] = Field(
-        None, description="Total processing time in seconds"
-    )
-    domain_statuses: Optional[Dict[str, Any]] = Field(
-        None, description="Status of individual domains"
-    )
-    error: Optional[str] = Field(None, description="Error message if batch failed")
-    metadata: Optional[Dict[str, Any]] = Field(
-        None, description="Additional batch metadata"
-    )
-
-
-# Sitemap Analyzer Models
-
-
-class SitemapExtractOptions(BaseModel):
-    """Options for sitemap extraction."""
-
-    follow_sitemapindex: bool = Field(
-        True, description="Whether to follow sitemap index files"
-    )
-    check_robots_txt: bool = Field(
-        True, description="Whether to check robots.txt for sitemaps"
-    )
-    max_depth: int = Field(3, description="Maximum depth to follow sitemap index files")
-    max_sitemaps: int = Field(100, description="Maximum number of sitemaps to process")
-    verify_ssl: bool = Field(True, description="Whether to verify SSL certificates")
-    timeout_seconds: int = Field(60, description="Timeout for HTTP requests in seconds")
-    extract_metadata: bool = Field(
-        True, description="Whether to extract metadata from sitemaps"
-    )
-    store_raw_xml: bool = Field(False, description="Whether to store raw XML content")
-
-
-class SitemapType(str, Enum):
-    """Types of sitemaps that can be encountered."""
-
-    INDEX = "index"
-    STANDARD = "standard"
-    IMAGE = "image"
-    VIDEO = "video"
-    NEWS = "news"
-    UNKNOWN = "unknown"
-
-
-class DiscoveryMethod(str, Enum):
-    """How a sitemap was discovered."""
-
-    ROBOTS_TXT = "robots_txt"
-    COMMON_PATH = "common_path"
-    SITEMAP_INDEX = "sitemap_index"
-    HTML_LINK = "html_link"
-    MANUAL = "manual"
-
-
-class SitemapAnalyzerRequest(BaseModel):
-    """Request model for sitemap analyzer endpoint."""
-
-    domain: str = Field(..., description="Domain to analyze")
-    user_id: Optional[str] = Field(None, description="User ID performing the analysis")
-    extract_options: Optional[SitemapExtractOptions] = Field(
-        None, description="Options for sitemap extraction"
-    )
-    include_content: bool = Field(
-        False, description="Whether to include sitemap content in response"
-    )
-    store_results: bool = Field(
-        True, description="Whether to store results in database"
-    )
-    # tenant_id field removed - using default tenant ID
-
-    @validator("domain")
-    def domain_must_be_valid(cls, v):
-        """Validate domain format."""
-        # Simple validation, could be more complex in production
-        if not v or len(v) < 4 or "." not in v:
-            raise ValueError("Must be a valid domain")
-        return v
-
-
-class SitemapAnalyzerResponse(BaseModel):
-    """Response model for sitemap analyzer endpoint."""
-
-    job_id: str = Field(..., description="Job ID for tracking the analysis")
-    status: str = Field(
-        ..., description="Status of the job (pending, running, complete, failed)"
-    )
-    status_url: str = Field(..., description="URL to check the status of the analysis")
-    domain: Optional[str] = Field(None, description="Domain being analyzed")
-    created_at: Optional[str] = Field(None, description="When the job was created")
-
-
-class SitemapFileResponse(BaseModel):
-    """Response model for sitemap file data."""
-
-    id: str = Field(..., description="Sitemap file ID")
-    url: str = Field(..., description="URL of the sitemap file")
-    sitemap_type: Optional[str] = Field(
-        None, description="Type of sitemap (index, standard, etc.)"
-    )
-    discovery_method: Optional[str] = Field(
-        None, description="How the sitemap was discovered"
-    )
-    page_count: Optional[int] = Field(
-        None, description="Number of pages in the sitemap"
-    )
-    size_bytes: Optional[int] = Field(None, description="Size of the sitemap in bytes")
-    url_count: Optional[int] = Field(0, description="Number of URLs in the sitemap")
-    last_modified: Optional[str] = Field(
-        None, description="Last modified date of the sitemap"
-    )
-    has_lastmod: Optional[bool] = Field(
-        False, description="Whether the sitemap contains lastmod information"
-    )
-    has_priority: Optional[bool] = Field(
-        False, description="Whether the sitemap contains priority information"
-    )
-    has_changefreq: Optional[bool] = Field(
-        False, description="Whether the sitemap contains changefreq information"
-    )
-    created_at: Optional[str] = Field(
-        None, description="When the sitemap was processed"
-    )
-
-    class Config:
-        from_attributes = True
-
-
-class SitemapUrlResponse(BaseModel):
-    """Response model for sitemap URL data."""
-
-    id: str = Field(..., description="URL entry ID")
-    url: str = Field(..., description="The URL from the sitemap")
-    lastmod: Optional[str] = Field(None, description="Last modified date from sitemap")
-    changefreq: Optional[str] = Field(None, description="Change frequency from sitemap")
-    priority: Optional[float] = Field(None, description="Priority from sitemap")
-    created_at: Optional[str] = Field(None, description="When the URL was processed")
-
-    class Config:
-        from_attributes = True
-
-
-class SitemapStatusResponse(BaseModel):
-    """Response model for sitemap analysis status endpoint."""
-
-    job_id: str = Field(..., description="Job ID")
-    status: str = Field(
-        ..., description="Job status (pending, running, complete, failed)"
-    )
-    progress: float = Field(0.0, description="Progress as a percentage (0-100)")
-    domain: Optional[str] = Field(None, description="Domain being analyzed")
-    created_at: Optional[str] = Field(None, description="When the job was created")
-    updated_at: Optional[str] = Field(None, description="When the job was last updated")
-    sitemap_count: Optional[int] = Field(0, description="Number of sitemaps found")
-    url_count: Optional[int] = Field(0, description="Total number of URLs found")
-    error: Optional[str] = Field(None, description="Error message if job failed")
-    status_url: Optional[str] = Field(None, description="URL to check the status")
-    results_url: Optional[str] = Field(
-        None, description="URL to get the results if complete"
-    )
-
-    class Config:
-        from_attributes = True
-
-
-# --- Models for Places Staging Selection --- #
-
-
-class PlaceStagingStatusEnum(str, Enum):
-    """Possible statuses for a place in the staging table. Mirrors DB PlaceStatusEnum."""
-
-    NEW = "New"  # Initial status after discovery
-    Selected = "Selected"
-    Maybe = "Maybe"
-    Not_a_Fit = "Not a Fit"
-    ARCHIVED = "Archived"  # If user decides not to process
-
-
-class PlaceStagingRecord(BaseModel):
-    """Response model representing a single record from the places_staging table."""
-
-    # Corresponds to Place model, adjust fields as needed for UI display
-    place_id: str = Field(..., description="Google Place ID (Primary Key for Staging)")
-    business_name: Optional[str] = Field(None, description="Business Name")
-    address: Optional[str] = Field(None, description="Address")
-    category: Optional[str] = Field(None, description="Primary Category")
-    search_location: Optional[str] = Field(
-        None, description="Location used for the search"
-    )
-    latitude: Optional[float] = Field(None)
-    longitude: Optional[float] = Field(None)
-    rating: Optional[float] = Field(None)
-    reviews_count: Optional[int] = Field(None)
-    price_level: Optional[int] = Field(None)
-    status: Optional[PlaceStagingStatusEnum] = Field(
-        PlaceStagingStatusEnum.NEW, description="Current status for deep scan selection"
-    )
-    updated_at: datetime
-    last_deep_scanned_at: Optional[datetime] = Field(None)
-    search_job_id: UUID = Field(..., description="FK to the discovery Job")
-    tenant_id: UUID
-
-    class Config:
-        from_attributes = True
-        use_enum_values = True  # Ensure enum values are used in response
-
-
-class PlaceStagingListResponse(BaseModel):
-    """Response model for listing staged places."""
-
-    items: List[PlaceStagingRecord]
-    total: int
-    # Add pagination fields if needed (page, size, etc.)
-
-
-class PlaceStatusUpdateRequest(BaseModel):
-    """Request model to update the status of a staged place."""
-
-    status: PlaceStagingStatusEnum = Field(
-        ..., description="The new status to set for the place"
-    )
-
-
-# --- End Models for Places Staging Selection --- #
-
-
-# --- Models for Local Businesses Selection --- #
-
-
-class LocalBusinessApiStatusEnum(str, Enum):
-    """Possible statuses for a local business, matching PlaceStatusEnum."""
-
-    New = "New"
-    Selected = "Selected"
-    Maybe = "Maybe"
-    Not_a_Fit = "Not a Fit"  # Ensure space is handled if API sends it
-    Archived = "Archived"
-
-
-class LocalBusinessBatchStatusUpdateRequest(BaseModel):
-    """Request model to update the status for one or more local businesses."""
-
-    local_business_ids: List[UUID] = Field(
-        ...,
-        min_length=1,
-        description="List of one or more Local Business UUIDs to update.",
-    )
-    status: LocalBusinessApiStatusEnum = Field(
-        ..., description="The new main status to set."
-    )
-
-
-# --- End Models for Local Businesses Selection --- #
-
-
-# --- Models for Domain Curation --- #
-
-
-# Import DB Enums required for response/request models
-# Note: Adjust path if your models are structured differently
-# try:
-from .domain import SitemapAnalysisStatusEnum, SitemapCurationStatusEnum
-
-# except ImportError:
-#     # Fallback or specific handling if the import path differs
-#     # This might happen if api_models.py is not in the same directory level as domain.py
-#     # For now, assuming they are siblings or accessible via relative path
-#     SitemapCurationStatusEnum = enum.Enum('SitemapCurationStatusEnum', {'New': 'New', 'Selected': 'Selected', 'Maybe': 'Maybe', 'Not_a_Fit': 'Not a Fit', 'Archived': 'Archived'})
-#     SitemapAnalysisStatusEnum = enum.Enum('SitemapAnalysisStatusEnum', {'queued': 'queued', 'processing': 'processing', 'submitted': 'submitted', 'failed': 'failed'})
-
-
-# Mirrors DB Enum SitemapCurationStatusEnum for API Input
-class SitemapCurationStatusApiEnum(str, enum.Enum):
-    New = "New"
-    Selected = "Selected"
-    Maybe = "Maybe"
-    Not_a_Fit = "Not a Fit"
-    Archived = "Archived"
-
-
-# Request model for the batch update endpoint
-class DomainBatchCurationStatusUpdateRequest(BaseModel):
-    domain_ids: List[UUID4] = Field(
-        ..., min_length=1, description="List of one or more Domain UUIDs to update."
-    )
-    sitemap_curation_status: SitemapCurationStatusApiEnum = Field(
-        ..., description="The new curation status to set for the sitemap workflow."
-    )
-
-
-# Pydantic model mirroring Domain for API responses (adjust fields as needed for UI)
-class DomainRecord(BaseModel):
-    id: UUID4
-    domain: str
-    sitemap_curation_status: Optional[SitemapCurationStatusEnum] = None
-    sitemap_analysis_status: Optional[SitemapAnalysisStatusEnum] = None
-    sitemap_analysis_error: Optional[str] = None
-    # Include other relevant Domain fields needed by the UI grid
-    status: Optional[str] = None  # Example: original domain status
-    created_at: datetime
-    updated_at: datetime
-
-    class Config:
-        from_attributes = True
-        use_enum_values = True  # Return enum values as strings
-
-
-# Standard paginated response wrapper
-class PaginatedDomainResponse(BaseModel):
-    items: List[DomainRecord]
-    total: int
-    page: int
-    size: int
-    pages: int
-
-
-# --- End Models for Domain Curation --- #
diff --git a/src/models/batch_job.py b/src/models/batch_job.py
index 82b2279..3464ccd 100644
--- a/src/models/batch_job.py
+++ b/src/models/batch_job.py
@@ -7,13 +7,23 @@ Represents batch processing jobs in ScraperSky.
 import uuid
 from typing import Any, Dict, List, Optional, Union
 
-from sqlalchemy import UUID, Column, DateTime, Float, Integer, String
+from sqlalchemy import (
+    UUID,
+    Column,
+    DateTime,
+    Enum as sa_Enum,
+    Float,
+    ForeignKey,
+    Integer,
+    String,
+)
 from sqlalchemy.dialects.postgresql import JSONB
 from sqlalchemy.dialects.postgresql import UUID as PGUUID
 from sqlalchemy.orm import relationship
 from sqlalchemy.sql import func
 
 from .base import Base, BaseModel, model_to_dict
+from .enums import BatchJobStatus
 from .tenant import DEFAULT_TENANT_ID
 
 
@@ -25,7 +35,7 @@ class BatchJob(Base, BaseModel):
     metadata about the batch and relationships to individual jobs.
 
     Fields:
-        id: Integer primary key (overriding UUID from BaseModel)
+        id: UUID primary key (from BaseModel)
         batch_id: UUID identifier for the batch (standardized from string)
         tenant_id: The tenant ID field (always using default tenant ID)
         processor_type: Type of processing (sitemap, metadata, contacts, etc.)
@@ -50,22 +60,22 @@ class BatchJob(Base, BaseModel):
 
     __tablename__ = "batch_jobs"
 
-    # Override id with Integer primary key
-    id = Column(Integer, primary_key=True, autoincrement=True)
+    # id is inherited from BaseModel (UUID, primary_key=True)
 
-    # UUID identifier (missing column that's causing the error)
-    id_uuid = Column(
-        UUID(as_uuid=True), default=uuid.uuid4, nullable=False, unique=True
-    )
-
-    # Core identifiers - Updated batch_id to UUID type
+    # Core identifiers
     batch_id = Column(UUID(as_uuid=True), nullable=False, index=True, unique=True)
     tenant_id = Column(
-        PGUUID, nullable=False, index=True, default=lambda: uuid.UUID(DEFAULT_TENANT_ID)
+        PGUUID,
+        ForeignKey("tenants.id"),
+        nullable=False,
+        index=True,
+        default=lambda: uuid.UUID(DEFAULT_TENANT_ID),
     )
     processor_type = Column(String, nullable=False)
-    status = Column(String, nullable=False, default="pending")
-    created_by = Column(PGUUID)
+    status = Column(
+        sa_Enum(BatchJobStatus), nullable=False, default=BatchJobStatus.PENDING
+    )
+    created_by = Column(PGUUID, ForeignKey("users.id"))
 
     # Progress tracking
     total_domains = Column(Integer, default=0)
@@ -124,11 +134,11 @@ class BatchJob(Base, BaseModel):
         # Update status based on progress
         if self.progress >= 1.0:
             if self.failed_domains == self.total_domains:
-                self.status = "failed"
+                self.status = BatchJobStatus.FAILED
             elif self.failed_domains > 0:
-                self.status = "partial"
+                self.status = BatchJobStatus.PARTIAL
             else:
-                self.status = "complete"
+                self.status = BatchJobStatus.COMPLETE
 
             # Set end time if not already set
             if not self.end_time:
@@ -180,7 +190,7 @@ class BatchJob(Base, BaseModel):
             batch_id=batch_id,
             tenant_id=uuid.UUID(DEFAULT_TENANT_ID),
             processor_type=processor_type,
-            status="pending",
+            status=BatchJobStatus.PENDING,
             created_by=created_by_uuid,
             total_domains=total_domains,
             completed_domains=0,
diff --git a/src/models/contact.py b/src/models/contact.py
index 699f82d..9895b42 100644
--- a/src/models/contact.py
+++ b/src/models/contact.py
@@ -10,6 +10,7 @@ from sqlalchemy import (
     Boolean,
     Column,
     ForeignKey,
+    Integer,
     Text,
     UniqueConstraint,
 )
@@ -46,7 +47,7 @@ class ContactProcessingStatus(str, enum.Enum):
 
 
 # HubSpot sync workflow status enums
-class HubotSyncStatus(str, enum.Enum):
+class HubSpotSyncStatus(str, enum.Enum):
     New = "New"
     Queued = "Queued"
     Processing = "Processing"
@@ -55,7 +56,7 @@ class HubotSyncStatus(str, enum.Enum):
     Skipped = "Skipped"
 
 
-class HubSyncProcessingStatus(str, enum.Enum):
+class HubSpotSyncProcessingStatus(str, enum.Enum):
     Queued = "Queued"
     Processing = "Processing"
     Complete = "Complete"
@@ -65,10 +66,8 @@ class HubSyncProcessingStatus(str, enum.Enum):
 class Contact(Base, BaseModel):
     __tablename__ = "contacts"
 
-    # Define columns based on the agreed schema
-    id: Column[uuid.UUID] = Column(
-        PGUUID(as_uuid=True), primary_key=True, default=uuid.uuid4
-    )
+    # id is inherited from BaseModel (UUID, primary_key=True)
+
     domain_id: Column[uuid.UUID] = Column(
         PGUUID(as_uuid=True),
         ForeignKey("domains.id", ondelete="CASCADE"),
@@ -84,22 +83,20 @@ class Contact(Base, BaseModel):
     email: Column[str] = Column(Text, nullable=False, index=True)
     email_type: Column[Optional[ContactEmailTypeEnum]] = Column(
         SQLAlchemyEnum(
-            ContactEmailTypeEnum, name="contact_email_type_enum", create_type=False
+            ContactEmailTypeEnum, name="contact_email_type", create_type=False
         ),
         nullable=True,
     )
     has_gmail: Column[Optional[bool]] = Column(Boolean, default=False, nullable=True)
     context: Column[Optional[str]] = Column(Text, nullable=True)
     source_url: Column[Optional[str]] = Column(Text, nullable=True)
-    source_job_id: Column[Optional[uuid.UUID]] = Column(
-        PGUUID(as_uuid=True), ForeignKey("jobs.job_id"), nullable=True
+    source_job_id: Column[Optional[int]] = Column(
+        Integer, ForeignKey("jobs.id"), nullable=True
     )
 
     # Define relationships
     domain = relationship("Domain", back_populates="contacts")
     page = relationship("Page", back_populates="contacts")
-    # Assuming Job model might have a collection of contacts found by it
-    # If not, this can be a one-way relationship
     job = relationship(
         "Job"
     )  # Consider adding back_populates="contacts" to Job model if needed
@@ -107,7 +104,7 @@ class Contact(Base, BaseModel):
     # Contact curation workflow status fields
     contact_curation_status: Column[ContactCurationStatus] = Column(
         SQLAlchemyEnum(
-            ContactCurationStatus, name="contactcurationstatus", create_type=False
+            ContactCurationStatus, name="contact_curation_status", create_type=False
         ),
         nullable=False,
         default=ContactCurationStatus.New,
@@ -117,7 +114,7 @@ class Contact(Base, BaseModel):
 
     contact_processing_status: Column[Optional[ContactProcessingStatus]] = Column(
         SQLAlchemyEnum(
-            ContactProcessingStatus, name="contactprocessingstatus", create_type=False
+            ContactProcessingStatus, name="contact_processing_status", create_type=False
         ),
         nullable=True,
         index=True,
@@ -126,17 +123,19 @@ class Contact(Base, BaseModel):
     contact_processing_error: Column[Optional[str]] = Column(Text, nullable=True)
 
     # HubSpot sync workflow status fields
-    hubspot_sync_status: Column[HubotSyncStatus] = Column(
-        SQLAlchemyEnum(HubotSyncStatus, name="hubotsyncstatus", create_type=False),
+    hubspot_sync_status: Column[HubSpotSyncStatus] = Column(
+        SQLAlchemyEnum(HubSpotSyncStatus, name="hubspot_sync_status", create_type=False),
         nullable=False,
-        default=HubotSyncStatus.New,
+        default=HubSpotSyncStatus.New,
         server_default="New",
         index=True,
     )
 
-    hubspot_processing_status: Column[Optional[HubSyncProcessingStatus]] = Column(
+    hubspot_processing_status: Column[Optional[HubSpotSyncProcessingStatus]] = Column(
         SQLAlchemyEnum(
-            HubSyncProcessingStatus, name="hubsyncprocessingstatus", create_type=False
+            HubSpotSyncProcessingStatus,
+            name="hubspot_sync_processing_status",
+            create_type=False,
         ),
         nullable=True,
         index=True,
@@ -147,5 +146,4 @@ class Contact(Base, BaseModel):
     # Define table arguments for constraints
     __table_args__ = (
         UniqueConstraint("domain_id", "email", name="uq_contact_domain_email"),
-        # Note: Indices from SQL DDL are typically handled by index=True on columns
     )
diff --git a/src/models/domain.py b/src/models/domain.py
index 0a1f3a3..c7cd2f6 100644
--- a/src/models/domain.py
+++ b/src/models/domain.py
@@ -4,7 +4,6 @@ Domain SQLAlchemy Model
 Represents website domains being processed by ScraperSky.
 """
 
-import enum
 import logging
 import uuid
 from typing import Any, Dict, List, Optional, Union
@@ -26,47 +25,18 @@ from sqlalchemy.orm import relationship
 from sqlalchemy.sql import func
 
 from .base import Base, BaseModel, model_to_dict
+from .enums import (
+    DomainStatus,
+    HubSpotSyncProcessingStatus,
+    HubSpotSyncStatus,
+    SitemapAnalysisStatus,
+    SitemapCurationStatus,
+)
 from .tenant import DEFAULT_TENANT_ID
 
 logger = logging.getLogger(__name__)
 
 
-# Python Enum for USER curation status
-class SitemapCurationStatusEnum(enum.Enum):
-    New = "New"
-    Selected = "Selected"
-    Maybe = "Maybe"
-    Not_a_Fit = "Not a Fit"  # Match API potentially needed space
-    Archived = "Archived"
-
-
-# HubSpot sync workflow status enums
-class HubotSyncStatus(str, enum.Enum):
-    New = "New"
-    Queued = "Queued"
-    Processing = "Processing"
-    Complete = "Complete"
-    Error = "Error"
-    Skipped = "Skipped"
-
-
-class HubSyncProcessingStatus(str, enum.Enum):
-    Queued = "Queued"
-    Processing = "Processing"
-    Complete = "Complete"
-    Error = "Error"
-
-
-# Define the enum for the sitemap analysis background process status
-class SitemapAnalysisStatusEnum(enum.Enum):
-    Queued = "Queued"
-    Processing = "Processing"
-    Completed = (
-        "Completed"  # Using Completed instead of submitted as per standardization
-    )
-    Error = "Error"  # Using Error instead of failed as per standardization
-
-
 class Domain(Base, BaseModel):
     """
     Domain model representing websites being processed.
@@ -130,8 +100,8 @@ class Domain(Base, BaseModel):
     )
 
     # Status and metadata
-    created_by = Column(PGUUID)
-    status = Column(String, nullable=False, default="active")
+    created_by = Column(PGUUID, ForeignKey("users.id"))
+    status = Column(SQLAlchemyEnum(DomainStatus), nullable=False, default=DomainStatus.PENDING)
     domain_metadata = Column(JSONB, name="meta_json")
     notes = Column(Text)
 
@@ -166,8 +136,8 @@ class Domain(Base, BaseModel):
 
     # Batch processing reference
     batch_id = Column(
-        PGUUID,
-        ForeignKey("batch_jobs.batch_id", ondelete="SET NULL"),
+        Integer,
+        ForeignKey("batch_jobs.id", ondelete="SET NULL"),
         index=True,
         nullable=True,
     )
@@ -183,8 +153,8 @@ class Domain(Base, BaseModel):
     # --- New fields for User-Triggered Sitemap Analysis --- #
     sitemap_analysis_status = Column(
         SQLAlchemyEnum(
-            SitemapAnalysisStatusEnum,
-            name="SitemapAnalysisStatusEnum",
+            SitemapAnalysisStatus,
+            name="sitemap_analysis_status",
             create_type=False,
         ),
         nullable=True,
@@ -196,28 +166,28 @@ class Domain(Base, BaseModel):
     # --- New fields for Sitemap Curation and Analysis --- #
     sitemap_curation_status = Column(
         SQLAlchemyEnum(
-            SitemapCurationStatusEnum,
-            name="SitemapCurationStatusEnum",
+            SitemapCurationStatus,
+            name="sitemap_curation_status",
             create_type=False,
         ),
         nullable=True,
-        default=SitemapCurationStatusEnum.New,
+        default=SitemapCurationStatus.NEW,
         index=True,
     )
     # ---------------------------------------------------- #
 
     # --- HubSpot sync workflow fields --- #
     hubspot_sync_status = Column(
-        SQLAlchemyEnum(HubotSyncStatus, name="hubotsyncstatus", create_type=False),
+        SQLAlchemyEnum(HubSpotSyncStatus, name="hubspot_sync_status", create_type=False),
         nullable=False,
-        default=HubotSyncStatus.New,
+        default=HubSpotSyncStatus.NEW,
         server_default="New",
         index=True,
     )
 
     hubspot_processing_status = Column(
         SQLAlchemyEnum(
-            HubSyncProcessingStatus, name="hubsyncprocessingstatus", create_type=False
+            HubSpotSyncProcessingStatus, name="hubspot_sync_processing_status", create_type=False
         ),
         nullable=True,
         index=True,
@@ -265,7 +235,7 @@ class Domain(Base, BaseModel):
 
         return result
 
-    def update_batch_info(self, batch_id: str) -> None:
+    def update_batch__info(self, batch_id: int) -> None:
         """
         Update domain's batch information.
 
@@ -279,7 +249,7 @@ class Domain(Base, BaseModel):
         """
         Mark domain as successfully scanned.
         """
-        self.status = "scanned"
+        self.status = DomainStatus.COMPLETED
         self.last_scan = func.now()
 
     def mark_failed(self, error_message: Optional[str] = None) -> None:
@@ -289,7 +259,7 @@ class Domain(Base, BaseModel):
         Args:
             error_message: Optional error message explaining failure
         """
-        self.status = "failed"
+        self.status = DomainStatus.ERROR
         self.last_scan = func.now()
 
         # Store error in metadata if provided
@@ -307,7 +277,7 @@ class Domain(Base, BaseModel):
         domain: str,
         metadata: Dict[str, Any],
         created_by: Optional[str] = None,
-        batch_id: Optional[str] = None,
+        batch_id: Optional[int] = None,
     ) -> "Domain":
         """
         Create a domain record from metadata dictionary.
@@ -352,7 +322,7 @@ class Domain(Base, BaseModel):
             domain=domain,
             tenant_id=tenant_id_uuid,
             created_by=created_by_uuid,
-            status="active",
+            status=DomainStatus.PENDING,
             domain_metadata=metadata,
             # Basic metadata
             title=metadata.get("title", ""),
@@ -388,137 +358,66 @@ class Domain(Base, BaseModel):
 
         return domain_obj
 
-    @classmethod
-    async def update_from_metadata(
-        cls, session, domain_obj: "Domain", metadata: Dict[str, Any]
-    ) -> "Domain":
-        """
-        Update a domain record from metadata dictionary.
-
-        Args:
-            session: SQLAlchemy session
-            domain_obj: Existing Domain object
-            metadata: New metadata dictionary
-
-        Returns:
-            Updated Domain instance
-        """
-        # Extract contact info and social links from nested structure
-        contact_info = metadata.get("contact_info", {})
-        social_links = metadata.get("social_links", {})
-
-        # Update fields - use setattr for safe attribute setting
-        domain_obj.domain_metadata = metadata
-        domain_obj.title = metadata.get("title", domain_obj.title)
-        domain_obj.description = metadata.get("description", domain_obj.description)
-        domain_obj.favicon_url = metadata.get("favicon_url", domain_obj.favicon_url)
-        domain_obj.logo_url = metadata.get("logo_url", domain_obj.logo_url)
-        domain_obj.language = metadata.get("language", domain_obj.language)
-
-        domain_obj.is_wordpress = metadata.get("is_wordpress", domain_obj.is_wordpress)
-        domain_obj.wordpress_version = metadata.get(
-            "wordpress_version", domain_obj.wordpress_version
-        )
-        domain_obj.has_elementor = metadata.get(
-            "has_elementor", domain_obj.has_elementor
-        )
-        domain_obj.tech_stack = metadata.get("tech_stack", domain_obj.tech_stack)
-
-        # Use empty lists as defaults to avoid None values
-        domain_obj.email_addresses = (
-            contact_info.get("email", domain_obj.email_addresses) or []
-        )
-        domain_obj.phone_numbers = (
-            contact_info.get("phone", domain_obj.phone_numbers) or []
-        )
-
-        domain_obj.facebook_url = social_links.get("facebook", domain_obj.facebook_url)
-        domain_obj.twitter_url = social_links.get("twitter", domain_obj.twitter_url)
-        domain_obj.linkedin_url = social_links.get("linkedin", domain_obj.linkedin_url)
-        domain_obj.instagram_url = social_links.get(
-            "instagram", domain_obj.instagram_url
-        )
-        domain_obj.youtube_url = social_links.get("youtube", domain_obj.youtube_url)
-
-        domain_obj.sitemap_urls = metadata.get("total_urls", domain_obj.sitemap_urls)
-        domain_obj.total_sitemaps = metadata.get(
-            "total_sitemaps", domain_obj.total_sitemaps
-        )
-        domain_obj.last_scan = func.now()
-        domain_obj.status = "scanned"  # Update status to show successful scan
-
-        # Add to session
-        session.add(domain_obj)
-
-        return domain_obj
-
-    @classmethod
-    async def get_by_id(
-        cls, session, domain_id: Union[str, uuid.UUID]
-    ) -> Optional["Domain"]:
-        """
-        Get domain by ID.
-
-        Args:
-            session: SQLAlchemy session
-            domain_id: UUID of the domain to retrieve (string or UUID)
-
-        Returns:
-            Domain instance or None if not found
-        """
-        # Convert string to UUID if needed
-        domain_id_uuid = None
-        if isinstance(domain_id, str):
-            try:
-                domain_id_uuid = uuid.UUID(domain_id)
-            except ValueError:
-                logger.warning(f"Invalid UUID format for domain_id: {domain_id}")
-                return None
-        else:
-            domain_id_uuid = domain_id
-
-        return await session.get(cls, domain_id_uuid)
-
-    @classmethod
-    async def get_by_domain_name(cls, session, domain_name: str) -> Optional["Domain"]:
-        """
-        Get domain by name without tenant filtering.
-
-        Args:
-            session: SQLAlchemy session
-            domain_name: Domain name to look up
-
-        Returns:
-            Domain instance or None if not found
-        """
-        # Import here to avoid circular imports
-        from sqlalchemy import select
-
-        # Normalize domain name
-        domain_name = domain_name.lower().strip()
+@classmethod
+async def update_from_metadata(
+    cls, session, domain_obj: "Domain", metadata: Dict[str, Any]
+) -> "Domain":
+    """
+    Update a domain record from metadata dictionary.
 
-        # Build query without tenant filtering
-        query = select(cls).where(cls.domain == domain_name)
+    Args:
+        session: SQLAlchemy session
+        domain_obj: Existing Domain object
+        metadata: New metadata dictionary
 
-        # Execute query
-        result = await session.execute(query)
-        return result.scalars().first()
+    Returns:
+        Updated Domain instance
+    """
+    # Extract contact info and social links from nested structure
+    contact_info = metadata.get("contact_info", {})
+    social_links = metadata.get("social_links", {})
+
+    # Update fields - use setattr for safe attribute setting
+    domain_obj.domain_metadata = metadata
+    domain_obj.title = metadata.get("title", domain_obj.title)
+    domain_obj.description = metadata.get("description", domain_obj.description)
+    domain_obj.favicon_url = metadata.get("favicon_url", domain_obj.favicon_url)
+    domain_obj.logo_url = metadata.get("logo_url", domain_obj.logo_url)
+    domain_obj.language = metadata.get("language", domain_obj.language)
+
+    domain_obj.is_wordpress = metadata.get("is_wordpress", domain_obj.is_wordpress)
+    domain_obj.wordpress_version = metadata.get(
+        "wordpress_version", domain_obj.wordpress_version
+    )
+    domain_obj.has_elementor = metadata.get(
+        "has_elementor", domain_obj.has_elementor
+    )
+    domain_obj.tech_stack = metadata.get("tech_stack", domain_obj.tech_stack)
 
-    @classmethod
-    async def get_by_batch_id(cls, session, batch_id: str) -> List["Domain"]:
-        """
-        Get all domains that belong to a specific batch without tenant filtering.
+    # Use empty lists as defaults to avoid None values
+    domain_obj.email_addresses = (
+        contact_info.get("email", domain_obj.email_addresses) or []
+    )
+    domain_obj.phone_numbers = (
+        contact_info.get("phone", domain_obj.phone_numbers) or []
+    )
 
-        Args:
-            session: SQLAlchemy session
-            batch_id: Batch ID to look up
+    domain_obj.facebook_url = social_links.get("facebook", domain_obj.facebook_url)
+    domain_obj.twitter_url = social_links.get("twitter", domain_obj.twitter_url)
+    domain_obj.linkedin_url = social_links.get("linkedin", domain_obj.linkedin_url)
+    domain_obj.instagram_url = social_links.get(
+        "instagram", domain_obj.instagram_url
+    )
+    domain_obj.youtube_url = social_links.get("youtube", domain_obj.youtube_url)
 
-        Returns:
-            List of Domain instances
-        """
-        from sqlalchemy import select
+    domain_obj.sitemap_urls = metadata.get("total_urls", domain_obj.sitemap_urls)
+    domain_obj.total_sitemaps = metadata.get(
+        "total_sitemaps", domain_obj.total_sitemaps
+    )
+    domain_obj.last_scan = func.now()
+    domain_obj.status = "scanned"  # Update status to show successful scan
 
-        query = select(cls).where(cls.batch_id == batch_id)
+    # Add to session
+    session.add(domain_obj)
 
-        result = await session.execute(query)
-        return result.scalars().all()
+    return domain_obj
\ No newline at end of file
diff --git a/src/models/enums.py b/src/models/enums.py
index 02a9820..523e922 100644
--- a/src/models/enums.py
+++ b/src/models/enums.py
@@ -1,18 +1,211 @@
-import enum
+from enum import Enum
+
+
+class SitemapType(str, Enum):
+    """Types of sitemaps that can be processed."""
+
+    INDEX = "index"
+    STANDARD = "standard"
+    IMAGE = "image"
+    VIDEO = "video"
+    NEWS = "news"
+    UNKNOWN = "unknown"
+
+
+class DiscoveryMethod(str, Enum):
+    """How a sitemap was discovered."""
+
+    ROBOTS_TXT = "robots_txt"
+    COMMON_PATH = "common_path"
+    SITEMAP_INDEX = "sitemap_index"
+    HTML_LINK = "html_link"
+    MANUAL = "manual"
+
+
+class TaskStatus(str, Enum):
+    """Common status values for tasks and jobs."""
+
+    PENDING = "Queued"
+    RUNNING = "Processing"  # Standardized from "InProgress"
+    COMPLETE = "Complete"  # Standardized from "Completed"
+    FAILED = "Error"
+    CANCELLED = "Cancelled"
+
+
+class PlaceStatus(str, Enum):
+    """Standardized statuses for a place or local business."""
+
+    NEW = "New"
+    QUEUED = "Queued"
+    PROCESSING = "Processing"
+    COMPLETE = "Complete"
+    ERROR = "Error"
+    SKIPPED = "Skipped"
+    SELECTED = "Selected"
+
+
+class SitemapCurationStatus(str, Enum):
+    """Standardized curation statuses for sitemaps."""
+
+    NEW = "New"
+    QUEUED = "Queued"
+    PROCESSING = "Processing"
+    COMPLETE = "Complete"
+    ERROR = "Error"
+    SKIPPED = "Skipped"
 
 # Existing Enums...
 
 
-class SitemapAnalysisStatusEnum(str, enum.Enum):
-    pending = "pending"  # Initial state when domain is created/reset
-    queued = "queued"  # Scheduler picked it up, waiting for adapter
-    processing = "processing"  # Adapter sent to API
-    submitted = "submitted"  # API accepted (202)
-    failed = "failed"  # Adapter or API call failed
 
+class DomainStatus(str, Enum):
+    """Statuses for domain processing."""
+
+    PENDING = "pending"
+    PROCESSING = "processing"
+    COMPLETED = "completed"
+    ERROR = "error"
+
+
+class HubSpotSyncStatus(str, Enum):
+    """Standardized sync statuses for HubSpot."""
+
+    NEW = "New"
+    QUEUED = "Queued"
+    PROCESSING = "Processing"
+    COMPLETE = "Complete"
+    ERROR = "Error"
+    SKIPPED = "Skipped"
+
+
+class HubSpotSyncProcessingStatus(str, Enum):
+    """Standardized processing statuses for HubSpot sync."""
+
+    QUEUED = "Queued"
+    PROCESSING = "Processing"
+    COMPLETE = "Complete"
+    ERROR = "Error"
+
+
+class BatchJobStatus(str, Enum):
+    """Statuses for batch processing jobs."""
+
+    PENDING = "pending"
+    RUNNING = "running"
+    COMPLETE = "complete"
+    FAILED = "failed"
+    PARTIAL = "partial"
+
+
+class JobType(str, Enum):
+    """Types of jobs that can be processed."""
+
+    SITEMAP_SCAN = "sitemap_scan"
+    PLACES_SEARCH = "places_search"
+    DOMAIN_METADATA_EXTRACTION = "domain_metadata_extraction"
+    CONTACT_ENRICHMENT = "contact_enrichment"
+    BATCH_PROCESSING = "batch_processing"
+
+
+class DomainExtractionStatus(str, Enum):
+    """Statuses for domain extraction from local businesses."""
+
+    QUEUED = "queued"
+    PROCESSING = "processing"
+    COMPLETED = "completed"
+    ERROR = "error"
+
+
+class GcpApiDeepScanStatus(str, Enum):
+    """Statuses for the GCP API deep scan process."""
+
+    QUEUED = "Queued"
+    PROCESSING = "Processing"
+    COMPLETED = "Completed"
+    ERROR = "Error"
+
+
+class SearchStatus(str, Enum):
+    """Standardized statuses for search operations."""
+
+    PENDING = "pending"
+    RUNNING = "running"
+    COMPLETED = "completed"
+    FAILED = "failed"
+
+
+class PageCurationStatus(str, Enum):
+    """Standardized curation statuses for pages."""
+
+    NEW = "New"
+    QUEUED = "Queued"
+    PROCESSING = "Processing"
+    COMPLETE = "Complete"
+    ERROR = "Error"
+    SKIPPED = "Skipped"
+
+
+class PageProcessingStatus(str, Enum):
+    """Standardized processing statuses for pages."""
+
+    QUEUED = "Queued"
+    PROCESSING = "Processing"
+    COMPLETE = "Complete"
+    ERROR = "Error"
+
+
+class SitemapFileStatus(str, Enum):
+    """Standardized statuses for sitemap files."""
+
+    PENDING = "Pending"
+    PROCESSING = "Processing"
+    COMPLETED = "Completed"
+    ERROR = "Error"
+
+
+class SitemapImportCurationStatus(str, Enum):
+    """Standardized curation statuses for sitemap imports."""
+
+    NEW = "New"
+    SELECTED = "Selected"
+    MAYBE = "Maybe"
+    NOT_A_FIT = "Not a Fit"
+    ARCHIVED = "Archived"
+
+
+class SitemapImportProcessStatus(str, Enum):
+    """Standardized processing statuses for sitemap imports."""
+
+    QUEUED = "Queued"
+    PROCESSING = "Processing"
+    COMPLETED = "Completed"
+    ERROR = "Error"
+    SUBMITTED = "Submitted"
+
+
+class SitemapAnalysisStatus(str, Enum):
+    """Standardized statuses for the sitemap analysis process."""
+
+    PENDING = "Pending"
+    ANALYZING = "Analyzing"
+    COMPLETED = "Completed"
+    FAILED = "Failed"
+
+
+class PlaceStagingStatus(str, Enum):
+    """Standardized statuses for the place staging and curation process."""
+
+    PENDING_REVIEW = "Pending Review"
+    APPROVED = "Approved"
+    REJECTED = "Rejected"
+    NEEDS_MORE_INFO = "Needs More Info"
+
+
+class SitemapDeepCurationStatus(str, Enum):
+    """Standardized curation statuses for deep sitemap scrapes."""
 
-class DomainStatusEnum(str, enum.Enum):
-    pending = "pending"  # Ready for metadata extraction
-    processing = "processing"  # Metadata extraction in progress
-    completed = "completed"  # Metadata extraction successful
-    error = "error"  # Metadata extraction failed
+    NEW = "New"
+    SELECTED = "Selected"
+    MAYBE = "Maybe"
+    NOT_A_FIT = "Not a Fit"
+    ARCHIVED = "Archived"
diff --git a/src/models/job.py b/src/models/job.py
index 853b1c9..0951118 100644
--- a/src/models/job.py
+++ b/src/models/job.py
@@ -8,19 +8,19 @@ import uuid
 from typing import Any, Dict, List, Optional, Union
 
 from sqlalchemy import (
-    UUID,
     Column,
     Float,
     ForeignKey,
     Integer,
     String,
 )
+from sqlalchemy import Enum as SQLAlchemyEnum
 from sqlalchemy.dialects.postgresql import JSONB
 from sqlalchemy.dialects.postgresql import UUID as PGUUID
 from sqlalchemy.orm import relationship
 
 from .base import Base, BaseModel, model_to_dict
-from .tenant import DEFAULT_TENANT_ID
+from .enums import JobType, TaskStatus
 
 
 class Job(Base, BaseModel):
@@ -30,7 +30,7 @@ class Job(Base, BaseModel):
     Fields:
         id: UUID primary key (inherited from BaseModel)
         job_type: Type of job (sitemap_scan, places_search, etc.)
-        tenant_id: The tenant ID field (always using default tenant ID)
+        tenant_id: The tenant ID field (inherited from BaseModel)
         created_by: The user who created this job
         status: Current job status (pending, running, complete, failed)
         domain_id: Optional reference to associated domain
@@ -43,22 +43,18 @@ class Job(Base, BaseModel):
 
     __tablename__ = "jobs"
 
-    # Override the id column to use an Integer primary key
-    id = Column(Integer, primary_key=True, autoincrement=True)
-
-    # UUID identifier (missing column that's causing the error)
-    job_id = Column(UUID(as_uuid=True), default=uuid.uuid4, nullable=False, unique=True)
-
     # Core fields
-    job_type = Column(String, nullable=False)
-    tenant_id = Column(String, nullable=False, default=DEFAULT_TENANT_ID)
-    tenant_id_uuid = Column(
-        PGUUID, index=True, default=lambda: uuid.UUID(DEFAULT_TENANT_ID)
+    job_type = Column(
+        SQLAlchemyEnum(JobType, name="job_type", create_type=False), nullable=False
     )
 
     # Status and metadata
-    created_by = Column(PGUUID)
-    status = Column(String, nullable=False)
+    created_by = Column(PGUUID, ForeignKey("users.id"))
+    status = Column(
+        SQLAlchemyEnum(TaskStatus, name="task_status", create_type=False),
+        nullable=False,
+        default=TaskStatus.PENDING,
+    )
     domain_id = Column(PGUUID, ForeignKey("domains.id", ondelete="SET NULL"))
     progress = Column(Float, default=0.0)
     result_data = Column(JSONB)
@@ -67,7 +63,7 @@ class Job(Base, BaseModel):
 
     # Batch processing field
     batch_id = Column(
-        String, ForeignKey("batch_jobs.batch_id", ondelete="SET NULL"), index=True
+        Integer, ForeignKey("batch_jobs.id", ondelete="SET NULL"), index=True
     )
 
     # Relationships
@@ -79,7 +75,7 @@ class Job(Base, BaseModel):
         return model_to_dict(self)
 
     def update_progress(
-        self, progress_value: float, status: Optional[str] = None
+        self, progress_value: float, status: Optional[TaskStatus] = None
     ) -> None:
         """
         Update job progress and optionally status.
@@ -92,17 +88,20 @@ class Job(Base, BaseModel):
 
         if status:
             self.status = status
-        elif self.progress >= 1.0 and self.status not in ["complete", "failed"]:
-            self.status = "complete"
+        elif self.progress >= 1.0 and self.status not in [
+            TaskStatus.COMPLETE,
+            TaskStatus.FAILED,
+        ]:
+            self.status = TaskStatus.COMPLETE
 
     @classmethod
     async def create_for_domain(
         cls,
         session,
-        job_type: str,
+        job_type: JobType,
         domain_id: Optional[uuid.UUID] = None,
         created_by: Optional[uuid.UUID] = None,
-        batch_id: Optional[str] = None,
+        batch_id: Optional[int] = None,
         metadata: Optional[Dict[str, Any]] = None,
     ) -> "Job":
         """
@@ -121,10 +120,8 @@ class Job(Base, BaseModel):
         """
         job = cls(
             job_type=job_type,
-            tenant_id=DEFAULT_TENANT_ID,
-            tenant_id_uuid=uuid.UUID(DEFAULT_TENANT_ID),
             created_by=created_by,
-            status="pending",
+            status=TaskStatus.PENDING,
             domain_id=domain_id,
             progress=0.0,
             job_metadata=metadata or {},
@@ -134,46 +131,9 @@ class Job(Base, BaseModel):
         session.add(job)
         return job
 
-    @classmethod
-    async def get_by_id(
-        cls, session, job_id: Union[int, uuid.UUID, str]
-    ) -> Optional["Job"]:
-        """Get a job by its integer ID without tenant filtering.
-
-        Args:
-            session: Database session.
-            job_id: Integer ID of the job.
-
-        Returns:
-            Optional[Job]: Job if found, None otherwise.
-        """
-        from sqlalchemy import select
-
-        # Ensure we are querying by the integer primary key 'id'
-        query = select(cls).where(cls.id == int(job_id))  # Convert to int just in case
-        result = await session.execute(query)
-        return result.scalars().first()
-
-    @classmethod
-    async def get_by_job_id(cls, session, job_uuid: uuid.UUID) -> Optional["Job"]:
-        """Get a job by its public UUID (job_id) without tenant filtering.
-
-        Args:
-            session: Database session.
-            job_uuid: The UUID identifier (job_id) of the job.
-
-        Returns:
-            Optional[Job]: Job if found, None otherwise.
-        """
-        from sqlalchemy import select
-
-        query = select(cls).where(cls.job_id == job_uuid)
-        result = await session.execute(query)
-        return result.scalars().first()
-
     @classmethod
     async def get_recent_jobs(
-        cls, session, job_type: Optional[str] = None, limit: int = 10
+        cls, session, job_type: Optional[JobType] = None, limit: int = 10
     ) -> List["Job"]:
         """Get recent jobs without tenant filtering.
 
@@ -197,7 +157,7 @@ class Job(Base, BaseModel):
         return result.scalars().all()
 
     @classmethod
-    async def get_by_batch_id(cls, session, batch_id: str) -> List["Job"]:
+    async def get_by_batch_id(cls, session, batch_id: int) -> List["Job"]:
         """
         Get all jobs that belong to a specific batch without tenant filtering.
 
diff --git a/src/models/local_business.py b/src/models/local_business.py
index e149b04..cdd3a71 100644
--- a/src/models/local_business.py
+++ b/src/models/local_business.py
@@ -1,53 +1,32 @@
-import enum
+"""
+SQLAlchemy model for LocalBusiness, representing a physical business location.
+"""
+
 import uuid
 from datetime import datetime
+from typing import Any, Dict
 
 from sqlalchemy import (
     Boolean,
     Column,
-    DateTime,
-    Enum,
+    ForeignKey,
     Integer,
     Numeric,
     String,
     Text,
 )
-from sqlalchemy.dialects.postgresql import ARRAY, JSONB, UUID
-from sqlalchemy.sql import func
-
-# Import the actual PlaceStatusEnum definition
-from .place import PlaceStatusEnum
-
-try:
-    from .base import Base
-except ImportError:
-    print(
-        "Warning: Could not import Base using relative path '.base'. Trying absolute path 'src.models.base'."
-    )
-    try:
-        from src.models.base import Base
-    except ImportError:
-        raise ImportError(
-            "Could not import Base from either '.base' or 'src.models.base'. Ensure the path is correct."
-        )
-
+from sqlalchemy import Enum as SQLAlchemyEnum
+from sqlalchemy.dialects.postgresql import ARRAY, JSONB, UUID as PGUUID
+from sqlalchemy.orm import relationship
 
-# Define the enum for the domain extraction background process status
-# Values MUST match the database enum values exactly (case-sensitive)
-class DomainExtractionStatusEnum(enum.Enum):
-    Queued = "Queued"
-    Processing = "Processing"
-    Completed = "Completed"
-    Error = "Error"
+from .base import BaseModel
+from .enums import DomainExtractionStatus, PlaceStatus
 
 
-class LocalBusiness(Base):
+class LocalBusiness(BaseModel):
     __tablename__ = "local_businesses"
 
-    id = Column(
-        UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid()
-    )
-    tenant_id = Column(UUID(as_uuid=True), nullable=False, index=True)
+    # Preserving columns from existing file
     place_id = Column(String, unique=True, nullable=True, index=True)
     lead_source = Column(Text, nullable=True)
     business_name = Column(Text, nullable=True, index=True)
@@ -93,45 +72,29 @@ class LocalBusiness(Base):
     parking = Column(ARRAY(Text), nullable=True)
     pets = Column(ARRAY(Text), nullable=True)
     additional_json = Column(JSONB, nullable=True, server_default="{}")
-    created_at = Column(
-        DateTime(timezone=True), server_default=func.now(), nullable=False
-    )
-    updated_at = Column(
-        DateTime(timezone=True),
-        server_default=func.now(),
-        onupdate=func.now(),
-        nullable=False,
-    )
 
-    # Use PlaceStatusEnum as it defines the shared user-facing statuses
+    # Refactored status columns
     status = Column(
-        Enum(
-            PlaceStatusEnum,
-            name="place_status_enum",
-            create_type=False,
-            native_enum=True,
-        ),
-        default=PlaceStatusEnum.New,
+        SQLAlchemyEnum(PlaceStatus, name="place_status", create_type=False),
+        default=PlaceStatus.NEW,
         nullable=False,
         index=True,
     )
-
-    # New Enum specifically for tracking domain extraction workflow for this business
-    # THIS ENUM MUST ADHERE TO PASCALCASE STANDARD
     domain_extraction_status = Column(
-        Enum(
-            DomainExtractionStatusEnum,  # Reference the updated Enum
-            name="DomainExtractionStatusEnum",  # Keep DB type name consistent for now
-            create_type=False,
-            native_enum=True,
-            values_callable=lambda obj: [e.value for e in obj],
+        SQLAlchemyEnum(
+            DomainExtractionStatus, name="domain_extraction_status", create_type=False
         ),
         nullable=True,
         index=True,
     )
-    domain_extraction_error = Column(String, nullable=True)  # To store error messages
+    domain_extraction_error = Column(String, nullable=True)
+
+    # Add relationship to Domain
+    domain_id = Column(PGUUID, ForeignKey("domains.id"), nullable=True)
+    domain = relationship("Domain", back_populates="local_businesses")
 
     def to_dict(self):
+        """Converts the model to a dictionary, handling special types."""
         result = {}
         for c in self.__table__.columns:
             value = getattr(self, c.name)
@@ -141,6 +104,8 @@ class LocalBusiness(Base):
                 result[c.name] = value.isoformat()
             elif hasattr(value, "quantize"):
                 result[c.name] = float(value) if value is not None else None
+            elif isinstance(value, (PlaceStatus, DomainExtractionStatus)):
+                result[c.name] = value.value
             else:
                 result[c.name] = value
         return result
diff --git a/src/models/page.py b/src/models/page.py
index 3c61e29..f0ef904 100644
--- a/src/models/page.py
+++ b/src/models/page.py
@@ -17,118 +17,56 @@ from sqlalchemy import (
     String,
     Text,
 )
-from sqlalchemy.dialects.postgresql import ENUM as PgEnum
+from sqlalchemy import Enum as SQLAlchemyEnum
 from sqlalchemy.dialects.postgresql import JSONB
 from sqlalchemy.dialects.postgresql import UUID as PGUUID
 from sqlalchemy.orm import relationship
 
-from .base import Base, BaseModel
+from .base import BaseModel
+from .enums import PageCurationStatus, PageProcessingStatus
 
 
-# --- Page Curation Workflow Enums ---
-class PageCurationStatus(str, Enum):
-    New = "New"
-    Queued = "Queued"
-    Processing = "Processing"
-    Complete = "Complete"
-    Error = "Error"
-    Skipped = "Skipped"
-
-
-class PageProcessingStatus(str, Enum):
-    Queued = "Queued"
-    Processing = "Processing"
-    Complete = "Complete"
-    Error = "Error"
-
-
-class Page(Base, BaseModel):
+class Page(BaseModel):
     __tablename__ = "pages"
 
-    # From simple_inspect output:
-    # id uuid NO gen_random_uuid() âœ“
-    # tenant_id uuid NO  -- NOTE: Assuming tenant isolation removed,
-    #                      but copying schema for now
-    # domain_id uuid NO â†’ domains.id
-    # url text NO
-    # title text YES
-    # description text YES
-    # h1 text YES
-    # canonical_url text YES
-    # meta_robots text YES
-    # has_schema_markup boolean YES false
-    # schema_types ARRAY YES
-    # has_contact_form boolean YES false
-    # has_comments boolean YES false
-    # word_count integer YES
-    # inbound_links ARRAY YES
-    # outbound_links ARRAY YES
-    # last_modified timestamp with time zone YES
-    # last_scan timestamp with time zone YES
-    # page_type text YES
-    # lead_source text YES
-    # additional_json jsonb YES '{}'::jsonb
-    # created_at timestamp with time zone YES now() -- Handled by BaseModel
-    # updated_at timestamp with time zone YES now() -- Handled by BaseModel
-
-    id: Column[uuid.UUID] = Column(
-        PGUUID(as_uuid=True), primary_key=True, default=uuid.uuid4
-    )
-    tenant_id: Column[uuid.UUID] = Column(PGUUID(as_uuid=True), nullable=False)
     domain_id: Column[uuid.UUID] = Column(
         PGUUID(as_uuid=True), ForeignKey("domains.id"), nullable=False
     )
     url: Column[str] = Column(Text, nullable=False)
-    title: Column[Optional[str]] = Column(Text, nullable=True)  # type: ignore
-    description: Column[Optional[str]] = Column(Text, nullable=True)  # type: ignore
-    h1: Column[Optional[str]] = Column(Text, nullable=True)  # type: ignore
-    canonical_url: Column[Optional[str]] = Column(Text, nullable=True)  # type: ignore
-    meta_robots: Column[Optional[str]] = Column(Text, nullable=True)  # type: ignore
-    has_schema_markup: Column[Optional[bool]] = Column(  # type: ignore
-        Boolean, default=False, nullable=True
-    )
-    schema_types: Column[Optional[List[Optional[str]]]] = Column(  # type: ignore
-        ARRAY(String), nullable=True
-    )
-    has_contact_form: Column[Optional[bool]] = Column(  # type: ignore
-        Boolean, default=False, nullable=True
-    )
-    has_comments: Column[Optional[bool]] = Column(Boolean, default=False, nullable=True)  # type: ignore
-    word_count: Column[Optional[int]] = Column(Integer, nullable=True)  # type: ignore
-    inbound_links: Column[Optional[List[Optional[str]]]] = Column(  # type: ignore
-        ARRAY(String), nullable=True
-    )
-    outbound_links: Column[Optional[List[Optional[str]]]] = Column(  # type: ignore
-        ARRAY(String), nullable=True
-    )
-    last_modified: Column[Optional[datetime]] = Column(  # type: ignore
-        DateTime(timezone=True), nullable=True
-    )
-    last_scan: Column[Optional[datetime]] = Column(  # type: ignore
-        DateTime(timezone=True), nullable=True
-    )
-    page_type: Column[Optional[str]] = Column(Text, nullable=True)  # type: ignore
-    lead_source: Column[Optional[str]] = Column(Text, nullable=True)  # type: ignore
-    additional_json: Column[Optional[dict]] = Column(JSONB, default=dict, nullable=True)  # type: ignore
+    title: Column[Optional[str]] = Column(Text, nullable=True)
+    description: Column[Optional[str]] = Column(Text, nullable=True)
+    h1: Column[Optional[str]] = Column(Text, nullable=True)
+    canonical_url: Column[Optional[str]] = Column(Text, nullable=True)
+    meta_robots: Column[Optional[str]] = Column(Text, nullable=True)
+    has_schema_markup: Column[Optional[bool]] = Column(Boolean, default=False, nullable=True)
+    schema_types: Column[Optional[List[Optional[str]]]] = Column(ARRAY(String), nullable=True)
+    has_contact_form: Column[Optional[bool]] = Column(Boolean, default=False, nullable=True)
+    has_comments: Column[Optional[bool]] = Column(Boolean, default=False, nullable=True)
+    word_count: Column[Optional[int]] = Column(Integer, nullable=True)
+    inbound_links: Column[Optional[List[Optional[str]]]] = Column(ARRAY(String), nullable=True)
+    outbound_links: Column[Optional[List[Optional[str]]]] = Column(ARRAY(String), nullable=True)
+    last_modified: Column[Optional[datetime]] = Column(DateTime(timezone=True), nullable=True)
+    last_scan: Column[Optional[datetime]] = Column(DateTime(timezone=True), nullable=True)
+    page_type: Column[Optional[str]] = Column(Text, nullable=True)
+    lead_source: Column[Optional[str]] = Column(Text, nullable=True)
+    additional_json: Column[Optional[dict]] = Column(JSONB, default=dict, nullable=True)
 
-    # Foreign key to track the source sitemap file (optional)
-    sitemap_file_id: Column[Optional[uuid.UUID]] = Column(  # type: ignore
+    sitemap_file_id: Column[Optional[uuid.UUID]] = Column(
         PGUUID(as_uuid=True), ForeignKey("sitemap_files.id"), nullable=True, index=True
     )
 
-    # --- Page Curation Workflow Columns ---
-    page_curation_status: Column[PageCurationStatus] = Column(  # type: ignore
-        PgEnum(PageCurationStatus, name="pagecurationstatus", create_type=False),
+    page_curation_status: Column[PageCurationStatus] = Column(
+        SQLAlchemyEnum(PageCurationStatus, name="page_curation_status", create_type=False),
         nullable=False,
-        default=PageCurationStatus.New,
+        default=PageCurationStatus.NEW,
         index=True,
     )
-    page_processing_status: Column[Optional[PageProcessingStatus]] = Column(  # type: ignore
-        PgEnum(PageProcessingStatus, name="pageprocessingstatus", create_type=False),
+    page_processing_status: Column[Optional[PageProcessingStatus]] = Column(
+        SQLAlchemyEnum(PageProcessingStatus, name="page_processing_status", create_type=False),
         nullable=True,
         index=True,
     )
-    page_processing_error: Column[Optional[str]] = Column(Text, nullable=True)  # type: ignore
+    page_processing_error: Column[Optional[str]] = Column(Text, nullable=True)
 
     # Relationships
     domain = relationship("Domain", back_populates="pages")
diff --git a/src/models/place.py b/src/models/place.py
index ae84245..a2b3adb 100644
--- a/src/models/place.py
+++ b/src/models/place.py
@@ -4,59 +4,35 @@ SQLAlchemy model for places from the Google Places API.
 This module defines the database model for the places_staging table.
 """
 
-import enum
-from datetime import datetime
-
 from sqlalchemy import (
     Boolean,
     Column,
     DateTime,
-    Enum,
     Float,
     ForeignKey,
     Integer,
     String,
     Text,
 )
-from sqlalchemy.dialects.postgresql import ARRAY, JSONB, UUID
+from sqlalchemy import Enum as SQLAlchemyEnum
+from sqlalchemy.dialects.postgresql import ARRAY, JSONB, UUID as PGUUID
+from sqlalchemy.orm import relationship
 
-from .base import Base
+from .base import BaseModel
+from .enums import GcpApiDeepScanStatus, PlaceStatus
 from .tenant import DEFAULT_TENANT_ID
 
 
-# Define the enum for the ORIGINAL user-facing place status
-# Values MUST match the database enum values exactly (case-sensitive)
-class PlaceStatusEnum(enum.Enum):
-    New = "New"
-    Selected = "Selected"
-    Maybe = "Maybe"
-    Not_a_Fit = "Not a Fit"
-    Archived = "Archived"
-
-
-# --- Define the NEW enum specifically for deep scan status --- #
-# This replaces the previous DeepScanStatusEnum which was repurposed
-# Values MUST match the database enum 'gcp_api_deep_scan_status_enum' exactly
-class GcpApiDeepScanStatusEnum(enum.Enum):
-    Queued = "Queued"
-    Processing = "Processing"
-    Completed = "Completed"
-    Error = "Error"
-
-
-# ------------------------------------------------------------- #
-
-
-class Place(Base):
+class Place(BaseModel):
     """
-    SQLAlchemy model for places_staging table.
+    SQLAlchemy model for the places table.
 
-    This table stores place data from Google Places API searches.
+    This table stores curated place data sourced from Google Places API searches.
     """
 
-    __tablename__ = "places_staging"
+    __tablename__ = "places"
 
-    id = Column(Integer, primary_key=True, autoincrement=True)
+    # Fields from original model
     place_id = Column(String(255), nullable=False, unique=True, index=True)
     name = Column(String(255), nullable=False)
     formatted_address = Column(String(512))
@@ -67,53 +43,40 @@ class Place(Base):
     rating = Column(Float)
     user_ratings_total = Column(Integer)
     price_level = Column(Integer)
-    # Use the extended enum, assuming native DB enum type exists (Reverting to standard)
-    # Ensure the default matches the new enum value
-    status = Column(
-        Enum(
-            PlaceStatusEnum,
-            name="place_status_enum",
-            create_type=False,
-            native_enum=True,
-        ),
-        default=PlaceStatusEnum.New,
-        nullable=False,
-        index=True,
-    )
-    tenant_id = Column(
-        UUID(as_uuid=True), nullable=False, index=True, default=DEFAULT_TENANT_ID
-    )
-    created_by = Column(UUID(as_uuid=True), nullable=True)
-    user_id = Column(UUID(as_uuid=True), nullable=True)
-    user_name = Column(String(255))
-    search_job_id = Column(UUID(as_uuid=True), ForeignKey("jobs.id"))
     search_query = Column(String(255))
     search_location = Column(String(255))
     raw_data = Column(JSONB)
-    search_time = Column(DateTime, default=datetime.utcnow)
     notes = Column(Text)
     priority = Column(Integer, default=0)
     tags = Column(ARRAY(String))
     revisit_date = Column(DateTime)
     processed = Column(Boolean, default=False)
     processed_time = Column(DateTime)
-    updated_by = Column(String(255))
-    updated_at = Column(DateTime, default=datetime.utcnow)
-    # New field to store deep scan errors
     deep_scan_error = Column(Text, nullable=True)
 
-    # --- Add the NEW column for deep scan status --- #
+    # Refactored fields
+    tenant_id = Column(
+        PGUUID, ForeignKey("tenants.id"), nullable=False, index=True, default=DEFAULT_TENANT_ID
+    )
+    search_job_id = Column(PGUUID, ForeignKey("jobs.id"), nullable=True)
+    created_by_id = Column(PGUUID, ForeignKey("profiles.id"), nullable=True)
+    updated_by_id = Column(PGUUID, ForeignKey("profiles.id"), nullable=True)
+
+    status = Column(
+        SQLAlchemyEnum(PlaceStatus, name="place_status", create_type=False),
+        default=PlaceStatus.NEW,
+        nullable=False,
+        index=True,
+    )
     deep_scan_status = Column(
-        Enum(
-            GcpApiDeepScanStatusEnum,  # Use the NEW enum
-            name="gcp_api_deep_scan_status_enum",  # Map to the NEW DB enum name
-            create_type=False,  # Assume the type exists in DB
-            native_enum=True,  # Use native PG enum
+        SQLAlchemyEnum(
+            GcpApiDeepScanStatus, name="gcp_api_deep_scan_status", create_type=False
         ),
-        nullable=True,  # Allow null for places not involved in deep scan
-        index=True,  # Index for efficient querying by scheduler
+        nullable=True,
+        index=True,
     )
-    # ------------------------------------------------- #
 
-    def __repr__(self):
-        return f"<Place(id={self.id}, name={self.name}, place_id={self.place_id})>"
+    # Relationships
+    job = relationship("Job")
+    created_by = relationship("Profile", foreign_keys=[created_by_id])
+    updated_by = relationship("Profile", foreign_keys=[updated_by_id])
diff --git a/src/models/place_search.py b/src/models/place_search.py
index 86ace17..9bed677 100644
--- a/src/models/place_search.py
+++ b/src/models/place_search.py
@@ -1,39 +1,49 @@
 """
 SQLAlchemy model for place searches.
 
-This module defines the database model for the place_searches table.
+This module defines the PlaceSearch model, which stores metadata related to
+Google Places API search queries.
 """
 
 import uuid
 from datetime import datetime
 
-from sqlalchemy import JSON, Column, DateTime, String
-from sqlalchemy.dialects.postgresql import UUID
+from sqlalchemy import Column, ForeignKey, String
+from sqlalchemy import Enum as SQLAlchemyEnum
+from sqlalchemy.dialects.postgresql import JSONB, UUID as PGUUID
+from sqlalchemy.orm import relationship
 
-from .base import Base
+from .base import BaseModel
+from .enums import SearchStatus
 from .tenant import DEFAULT_TENANT_ID
 
 
-class PlaceSearch(Base):
+class PlaceSearch(BaseModel):
     """
-    SQLAlchemy model for place_searches table.
-
-    This table stores search metadata from Google Places API queries.
+    Represents a search query made to the Google Places API.
     """
 
     __tablename__ = "place_searches"
 
-    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
+    id = Column(PGUUID, primary_key=True, default=uuid.uuid4)
+    query = Column(String(255), nullable=False)
+    location = Column(String(255))
+    results = Column(JSONB)
+
     tenant_id = Column(
-        UUID(as_uuid=True), nullable=False, index=True, default=DEFAULT_TENANT_ID
+        PGUUID, ForeignKey("tenants.id"), nullable=False, index=True, default=DEFAULT_TENANT_ID
+    )
+    user_id = Column(PGUUID, ForeignKey("profiles.id"), nullable=False)
+
+    status = Column(
+        SQLAlchemyEnum(SearchStatus, name="search_status", create_type=False),
+        default=SearchStatus.PENDING,
+        nullable=False,
     )
-    user_id = Column(UUID(as_uuid=True))
-    location = Column(String(255), nullable=False)
-    business_type = Column(String(100), nullable=False)
-    params = Column(JSON)
-    created_at = Column(DateTime, default=datetime.utcnow)
-    status = Column(String(50), default="pending")
-    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
+
+
+
+    user = relationship("Profile")
 
     def __repr__(self):
-        return f"<PlaceSearch(id={self.id}, location={self.location}, business_type={self.business_type})>"
+        return f"<PlaceSearch(id={self.id}, query='{self.query}', status='{self.status}')>"
diff --git a/src/models/profile.py b/src/models/profile.py
index 56477f0..1303fbf 100644
--- a/src/models/profile.py
+++ b/src/models/profile.py
@@ -10,21 +10,22 @@ from typing import Optional
 from uuid import UUID
 
 from pydantic import BaseModel, EmailStr
-from sqlalchemy import Boolean, Column, Text
+from sqlalchemy import Boolean, Column, ForeignKey, Text
 from sqlalchemy.dialects.postgresql import UUID as PGUUID
 
-from .base import Base
 from .base import BaseModel as BaseORMModel
 from .tenant import DEFAULT_TENANT_ID
 
 
-class Profile(Base, BaseORMModel):
+class Profile(BaseORMModel):
     """SQLAlchemy model for profiles table."""
 
     __tablename__ = "profiles"
 
     # Keep tenant_id column for compatibility but no foreign key or relationship
-    tenant_id = Column(PGUUID(as_uuid=True), nullable=False, default=DEFAULT_TENANT_ID)
+    tenant_id = Column(
+        PGUUID(as_uuid=True), ForeignKey("tenants.id"), nullable=False, default=DEFAULT_TENANT_ID
+    )
     name = Column(Text, nullable=True)
     email = Column(Text, nullable=True)
     avatar_url = Column(Text, nullable=True)
diff --git a/src/models/sitemap.py b/src/models/sitemap.py
index 7814dfb..8d7ef22 100644
--- a/src/models/sitemap.py
+++ b/src/models/sitemap.py
@@ -5,7 +5,6 @@ This module defines the database models for sitemap files and URLs,
 providing an ORM interface for interacting with the sitemap_files and sitemap_urls tables.
 """
 
-import enum
 import logging
 import uuid
 from datetime import datetime
@@ -26,72 +25,25 @@ from sqlalchemy.dialects.postgresql import JSONB
 from sqlalchemy.dialects.postgresql import UUID as PGUUID
 from sqlalchemy.orm import relationship
 
-from .base import Base, BaseModel, model_to_dict
+from .base import BaseModel, model_to_dict
+from .enums import (
+    SitemapFileStatus,
+    SitemapImportCurationStatus,
+    SitemapImportProcessStatus,
+)
+from .tenant import DEFAULT_TENANT_ID
 
 logger = logging.getLogger(__name__)
 
 
-# Enum definitions to match database
-class SitemapFileStatusEnum(enum.Enum):
-    """Status values for sitemap_file_status_enum in database"""
-
-    Pending = "Pending"
-    Processing = "Processing"
-    Completed = "Completed"
-    Error = "Error"
-
-
-# Rename Enum related to Sitemap Curation status
-class SitemapImportCurationStatusEnum(enum.Enum):
-    """Status values for Sitemap Import Curation Status"""
-
-    New = "New"
-    Selected = "Selected"
-    Maybe = "Maybe"
-    Not_a_Fit = "Not a Fit"
-    Archived = "Archived"
-
-
-# Rename Enum related to Sitemap Processing status
-class SitemapImportProcessStatusEnum(enum.Enum):
-    """Status values mapped to sitemap_import_status_enum in database (MUST MATCH DB DEFINITION)"""
-
-    # Setting to CAPITALIZED values to match corrected DB schema
-    Queued = "Queued"
-    Processing = "Processing"
-    Completed = "Completed"
-    Error = "Error"
-    Submitted = "Submitted"
-
-
-class SitemapFile(Base, BaseModel):
+class SitemapFile(BaseModel):
     """
     SitemapFile model representing sitemap XML files discovered and processed by the system.
-
-    Fields:
-        id: UUID primary key (inherited from BaseModel)
-        domain_id: Domain ID this sitemap belongs to
-        url: URL of the sitemap file
-        sitemap_type: Type of sitemap (INDEX, STANDARD, IMAGE, VIDEO, NEWS)
-        discovery_method: How this sitemap was discovered (ROBOTS_TXT, COMMON_PATH, etc.)
-        page_count: Number of pages in the sitemap
-        size_bytes: Size of the sitemap file in bytes
-        has_lastmod: Whether the sitemap contains lastmod information
-        has_priority: Whether the sitemap contains priority information
-        has_changefreq: Whether the sitemap contains changefreq information
-        last_modified: When the sitemap was last modified according to HTTP headers
-        url_count: Number of URLs in the sitemap
-        tenant_id: The tenant this sitemap belongs to (for multi-tenancy)
-        created_by: The user who created this sitemap record
-        job_id: Associated job ID if created by a background job
-        status: Processing status of the sitemap
-        tags: JSON field for additional tags and categorization
-        notes: Optional text notes about this sitemap
     """
 
     __tablename__ = "sitemap_files"
 
-    # Core fields (id comes from BaseModel)
+    # Core fields
     domain_id = Column(PGUUID, ForeignKey("domains.id"), nullable=False, index=True)
     url = Column(Text, nullable=False)
     sitemap_type = Column(Text, nullable=False)
@@ -115,52 +67,43 @@ class SitemapFile(Base, BaseModel):
 
     # Security and ownership
     tenant_id = Column(
-        PGUUID,
-        nullable=True,
-        index=True,
-        default="550e8400-e29b-41d4-a716-446655440000",
+        PGUUID, ForeignKey("tenants.id"), nullable=False, index=True, default=DEFAULT_TENANT_ID
     )
-    created_by = Column(PGUUID, nullable=True)
-    updated_by = Column(PGUUID, nullable=True)
-    user_id = Column(PGUUID, nullable=True)
-    user_name = Column(Text, nullable=True)
+    created_by_id = Column(PGUUID, ForeignKey("profiles.id"), nullable=True)
+    updated_by_id = Column(PGUUID, ForeignKey("profiles.id"), nullable=True)
+    job_id = Column(PGUUID, nullable=True, index=True)
 
     # Process tracking
-    job_id = Column(PGUUID, nullable=True, index=True)
     status = Column(
-        SQLAlchemyEnum(
-            SitemapFileStatusEnum, name="sitemap_file_status_enum", create_type=False
-        ),
+        SQLAlchemyEnum(SitemapFileStatus, name="sitemap_file_status", create_type=False),
         nullable=False,
-        default=SitemapFileStatusEnum.Pending,
+        default=SitemapFileStatus.PENDING,
         index=True,
     )
     is_active = Column(Boolean, nullable=True, default=True)
     process_after = Column(DateTime(timezone=True), nullable=True)
     last_processed_at = Column(DateTime(timezone=True), nullable=True)
 
-    # Renamed section comment: Sitemap Import columns (previously Deep Scrape)
-    # Note: deep_scrape_curation_status column potentially needs DB migration/rename later
-    deep_scrape_curation_status = Column(
+    # Sitemap Import columns
+    sitemap_import_curation_status = Column(
         SQLAlchemyEnum(
-            SitemapImportCurationStatusEnum,  # Use renamed Enum
-            name="SitemapCurationStatusEnum",  # Keep DB name for now unless migrated
+            SitemapImportCurationStatus,
+            name="sitemap_import_curation_status",
             create_type=False,
         ),
         nullable=True,
-        default=SitemapImportCurationStatusEnum.New,  # Use renamed Enum
+        default=SitemapImportCurationStatus.NEW,
         index=True,
     )
-    sitemap_import_error = Column(Text, name="sitemap_import_error", nullable=True)
+    sitemap_import_error = Column(Text, nullable=True)
     sitemap_import_status = Column(
         SQLAlchemyEnum(
-            SitemapImportProcessStatusEnum,  # Use renamed Enum
-            name="sitemap_import_status_enum",
+            SitemapImportProcessStatus,
+            name="sitemap_import_process_status",
             create_type=False,
         ),
         nullable=True,
         index=True,
-        name="sitemap_import_status",
     )
 
     # Additional metadata
@@ -172,171 +115,22 @@ class SitemapFile(Base, BaseModel):
         "SitemapUrl", back_populates="sitemap", cascade="all, delete-orphan"
     )
     domain = relationship("Domain", back_populates="sitemap_files")
+    created_by = relationship("Profile", foreign_keys=[created_by_id])
+    updated_by = relationship("Profile", foreign_keys=[updated_by_id])
 
     def to_dict(self) -> Dict[str, Any]:
         """Convert sitemap file to dictionary with proper serialization."""
         return model_to_dict(self)
 
-    @classmethod
-    async def create_new(
-        cls,
-        session,
-        domain_id: str,
-        url: str,
-        sitemap_type: Optional[str] = None,
-        discovery_method: Optional[str] = None,
-        tenant_id: Optional[str] = None,
-        created_by: Optional[str] = None,
-        job_id: Optional[str] = None,
-        **kwargs,
-    ) -> "SitemapFile":
-        """
-        Create a new sitemap file record.
-
-        Args:
-            session: SQLAlchemy session
-            domain_id: Domain ID this sitemap belongs to
-            url: URL of the sitemap file
-            sitemap_type: Type of sitemap (INDEX, STANDARD, etc)
-            discovery_method: How this sitemap was discovered
-            tenant_id: Tenant ID for security
-            created_by: User ID of creator
-            job_id: Associated job ID if any
-            **kwargs: Additional fields to set on the sitemap file
-
-        Returns:
-            New SitemapFile instance
-        """
-        try:
-            # Convert string UUIDs to UUID objects if provided
-            tenant_id_obj = None
-            if tenant_id:
-                try:
-                    tenant_id_obj = uuid.UUID(tenant_id)
-                except (ValueError, TypeError) as err:
-                    logger.warning(f"Invalid UUID format for tenant_id: {tenant_id}")
-                    raise ValueError(f"Invalid UUID format for tenant_id: {tenant_id}") from err
-
-            created_by_obj = None
-            if created_by:
-                try:
-                    created_by_obj = uuid.UUID(created_by)
-                except (ValueError, TypeError) as err:
-                    logger.warning(f"Invalid UUID format for created_by: {created_by}")
-                    raise ValueError(f"Invalid UUID format for created_by: {created_by}") from err
-
-            # Create the sitemap file
-            sitemap_file = cls(
-                domain_id=domain_id,
-                url=url,
-                sitemap_type=sitemap_type,
-                discovery_method=discovery_method,
-                tenant_id=tenant_id_obj,
-                created_by=created_by_obj,
-                job_id=job_id,
-                status="pending",
-                **kwargs,
-            )
-
-            session.add(sitemap_file)
-            return sitemap_file
-
-        except Exception as e:
-            logger.error(f"Error creating sitemap file: {str(e)}")
-            raise
-
-    @classmethod
-    async def get_by_id(
-        cls, session, sitemap_id: Union[str, uuid.UUID]
-    ) -> Optional["SitemapFile"]:
-        """
-        Get a sitemap file by its ID.
-
-        Args:
-            session: SQLAlchemy session
-            sitemap_id: Sitemap ID (UUID or string)
-
-        Returns:
-            SitemapFile instance or None if not found
-        """
-        from sqlalchemy import select
-
-        # Convert string UUID to UUID object if needed
-        if isinstance(sitemap_id, str):
-            try:
-                sitemap_id = uuid.UUID(sitemap_id)
-            except ValueError:
-                logger.warning(f"Invalid UUID format for sitemap_id: {sitemap_id}")
-                return None
-
-        query = select(cls).where(cls.id == sitemap_id)
-
-        result = await session.execute(query)
-        return result.scalars().first()
-
-    @classmethod
-    async def get_by_domain_id(cls, session, domain_id: str) -> List["SitemapFile"]:
-        """
-        Get all sitemap files for a domain.
-
-        Args:
-            session: SQLAlchemy session
-            domain_id: Domain ID to get sitemaps for
-
-        Returns:
-            List of SitemapFile instances
-        """
-        from sqlalchemy import select
-
-        query = (
-            select(cls)
-            .where(cls.domain_id == domain_id)
-            .order_by(cls.created_at.desc())
-        )
-
-        result = await session.execute(query)
-        return result.scalars().all()
-
-    @classmethod
-    async def get_by_job_id(cls, session, job_id: str) -> List["SitemapFile"]:
-        """
-        Get all sitemap files for a specific job.
-
-        Args:
-            session: SQLAlchemy session
-            job_id: Job ID to get sitemaps for
-
-        Returns:
-            List of SitemapFile instances
-        """
-        from sqlalchemy import select
-
-        query = select(cls).where(cls.job_id == job_id).order_by(cls.created_at.desc())
-
-        result = await session.execute(query)
-        return result.scalars().all()
 
-
-class SitemapUrl(Base, BaseModel):
+class SitemapUrl(BaseModel):
     """
     SitemapUrl model representing URLs found in sitemaps.
-
-    Fields:
-        id: UUID primary key (inherited from BaseModel)
-        sitemap_id: Foreign key to the parent sitemap file
-        url: The URL found in the sitemap
-        lastmod: Last modified date from sitemap (if available)
-        changefreq: Change frequency from sitemap (if available)
-        priority: Priority value from sitemap (if available)
-        tenant_id: The tenant this URL belongs to (for multi-tenancy)
-        created_by: The user who created this record
-        tags: JSON field for additional tags and categorization
-        notes: Optional text notes about this URL
     """
 
     __tablename__ = "sitemap_urls"
 
-    # Core fields (id comes from BaseModel)
+    # Core fields
     sitemap_id = Column(
         PGUUID,
         ForeignKey("sitemap_files.id", ondelete="CASCADE"),
@@ -352,13 +146,10 @@ class SitemapUrl(Base, BaseModel):
 
     # Security and ownership
     tenant_id = Column(
-        PGUUID,
-        nullable=True,
-        index=True,
-        default="550e8400-e29b-41d4-a716-446655440000",
+        PGUUID, ForeignKey("tenants.id"), nullable=False, index=True, default=DEFAULT_TENANT_ID
     )
-    created_by = Column(PGUUID, nullable=True)
-    updated_by = Column(PGUUID, nullable=True)
+    created_by_id = Column(PGUUID, ForeignKey("profiles.id"), nullable=True)
+    updated_by_id = Column(PGUUID, ForeignKey("profiles.id"), nullable=True)
 
     # Additional metadata
     tags = Column(JSONB, nullable=True)
@@ -366,37 +157,38 @@ class SitemapUrl(Base, BaseModel):
 
     # Relationships
     sitemap = relationship("SitemapFile", back_populates="urls")
+    created_by = relationship("Profile", foreign_keys=[created_by_id])
+    updated_by = relationship("Profile", foreign_keys=[updated_by_id])
 
     def to_dict(self) -> Dict[str, Any]:
         """Convert sitemap URL to dictionary with proper serialization."""
         return model_to_dict(self)
-
     @classmethod
-    async def create_new(
+    async def create_from_sitemap(
         cls,
         session,
-        sitemap_id: Union[str, uuid.UUID],
+        sitemap_id: "Union[str, uuid.UUID]",
         url: str,
-        tenant_id: str,
-        lastmod: Optional[datetime] = None,
+        lastmod: "Optional[datetime]" = None,
         changefreq: Optional[str] = None,
         priority: Optional[float] = None,
-        created_by: Optional[str] = None,
+        tenant_id: "Optional[Union[str, uuid.UUID]]" = None,
+        created_by: "Optional[Union[str, uuid.UUID]]" = None,
         **kwargs,
     ) -> "SitemapUrl":
         """
-        Create a new sitemap URL record.
+        Create a new SitemapUrl instance from sitemap data.
 
         Args:
             session: SQLAlchemy session
-            sitemap_id: ID of the parent sitemap
-            url: The URL from the sitemap
-            tenant_id: Tenant ID for security
-            lastmod: Last modified date if available
-            changefreq: Change frequency if available
-            priority: Priority if available
-            created_by: User ID of creator
-            **kwargs: Additional fields to set
+            sitemap_id: ID of the parent sitemap file
+            url: The URL string
+            lastmod: Last modification timestamp
+            changefreq: Change frequency
+            priority: URL priority
+            tenant_id: Tenant ID
+            created_by: Profile ID of the creator
+            **kwargs: Additional fields
 
         Returns:
             New SitemapUrl instance
@@ -437,7 +229,7 @@ class SitemapUrl(Base, BaseModel):
                 changefreq=changefreq,
                 priority=priority,
                 tenant_id=tenant_id_obj,
-                created_by=created_by_obj,
+                created_by_id=created_by_obj,
                 **kwargs,
             )
 
diff --git a/src/models/sitemap_file.py b/src/models/sitemap_file.py
deleted file mode 100644
index 8f73383..0000000
--- a/src/models/sitemap_file.py
+++ /dev/null
@@ -1,90 +0,0 @@
-"""
-Pydantic Schemas for SitemapFile Model
-"""
-
-import uuid
-from datetime import datetime
-from typing import List, Optional
-
-from pydantic import BaseModel, Field
-
-# Import the Enum from the model to reuse it
-from ..models.sitemap import SitemapDeepCurationStatusEnum, SitemapFileStatusEnum
-
-
-# Base Schema: Fields common to Create and Read
-class SitemapFileBase(BaseModel):
-    domain_id: uuid.UUID
-    # Use alias to map 'url' from model to 'sitemap_url' in schema
-    url: str = Field(
-        ..., alias="sitemap_url", examples=["https://example.com/sitemap.xml"]
-    )
-    status: SitemapFileStatusEnum = Field(default=SitemapFileStatusEnum.Pending)
-    file_path: Optional[str] = None
-    error_message: Optional[str] = None
-    processing_time: Optional[float] = None
-    url_count: Optional[int] = Field(default=0)
-
-    # Allow ORM mode for direct mapping from SQLAlchemy models
-    class Config:
-        from_attributes = True
-        populate_by_name = True
-
-
-# Schema for Creating a new SitemapFile (inherits from Base)
-# Fields required when creating via API
-class SitemapFileCreate(SitemapFileBase):
-    # created_by will likely be set automatically based on the authenticated user
-    # If the API allows specifying it during creation, add it here:
-    # created_by: Optional[uuid.UUID] = None
-    pass  # Inherits all needed fields from Base for creation
-
-
-# Schema for Updating an existing SitemapFile (all fields optional)
-class SitemapFileUpdate(BaseModel):
-    domain_id: Optional[uuid.UUID] = None
-    # Use alias for update as well
-    url: Optional[str] = Field(None, alias="sitemap_url")
-    status: Optional[SitemapFileStatusEnum] = None
-    file_path: Optional[str] = None
-    error_message: Optional[str] = None
-    processing_time: Optional[float] = None
-    url_count: Optional[int] = None
-    deep_scrape_curation_status: Optional[SitemapDeepCurationStatusEnum] = None
-    # updated_by will likely be set automatically
-    # If API allows manual update, add: updated_by: Optional[uuid.UUID] = None
-
-    # Allow ORM mode for direct mapping from SQLAlchemy models
-    class Config:
-        from_attributes = True
-        populate_by_name = True
-
-
-# Schema for Reading/Returning a SitemapFile (includes fields generated by DB/BaseModel)
-class SitemapFileRead(SitemapFileBase):
-    id: uuid.UUID  # Inherited from BaseModel
-    created_at: datetime  # Inherited from BaseModel
-    updated_at: datetime  # Inherited from BaseModel
-    created_by: Optional[uuid.UUID] = None  # Include if set in model
-    updated_by: Optional[uuid.UUID] = None  # Include if set in model
-    deep_scrape_curation_status: Optional[SitemapDeepCurationStatusEnum] = None
-    domain_name: Optional[str] = None  # Added field for domain name
-
-
-# Schema for Paginated Response (common pattern)
-class PaginatedSitemapFileResponse(BaseModel):
-    items: List[SitemapFileRead]
-    total: int
-    page: int
-    size: int
-    pages: int
-
-
-# Schema for Batch Update operations (example: updating status)
-class SitemapFileBatchUpdate(BaseModel):
-    sitemap_file_ids: List[uuid.UUID]
-    deep_scrape_curation_status: SitemapDeepCurationStatusEnum
-
-    class Config:
-        # REMOVED use_enum_values = True
-        pass
diff --git a/src/routers/batch_page_scraper.py b/src/routers/batch_page_scraper.py
index b15a67e..7294a23 100644
--- a/src/routers/batch_page_scraper.py
+++ b/src/routers/batch_page_scraper.py
@@ -16,12 +16,12 @@ from sqlalchemy.ext.asyncio import AsyncSession
 
 from ..auth.jwt_auth import get_current_user
 from ..config.settings import settings
-from ..models import (
+from ..schemas.batch import (
     BatchRequest,
     BatchResponse,
     BatchStatusResponse,
-    SitemapScrapingResponse,
 )
+from ..schemas.sitemap import SitemapScrapingResponse
 from ..models.tenant import DEFAULT_TENANT_ID
 from ..schemas.job import JobStatusResponse
 from ..services.batch.batch_functions import process_batch_with_own_session
diff --git a/src/routers/batch_sitemap.py b/src/routers/batch_sitemap.py
index 7f0bfd6..ce0f7de 100644
--- a/src/routers/batch_sitemap.py
+++ b/src/routers/batch_sitemap.py
@@ -19,17 +19,17 @@ from datetime import datetime
 from typing import Any, Dict, List, Optional
 
 from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException
-from pydantic import BaseModel, Field, validator
+from pydantic import Field, validator
+
+from ..schemas.batch import BatchRequest, BatchResponse, BatchStatusResponse
 from sqlalchemy import func
 from sqlalchemy.ext.asyncio import AsyncSession
 
 from ..auth.jwt_auth import DEFAULT_TENANT_ID, get_current_user
 from ..config.settings import settings
 from ..models.batch_job import BatchJob
+from ..models.enums import BatchJobStatus
 from ..services.batch.batch_functions import (
-    BATCH_STATUS_COMPLETED,
-    BATCH_STATUS_FAILED,
-    BATCH_STATUS_PROCESSING,
     create_batch,
     get_batch_status,
 )
@@ -46,26 +46,6 @@ router = APIRouter(
 )
 
 
-# Define our own status response model using Pydantic
-class BatchStatusResponse(BaseModel):
-    batch_id: str
-    status: str
-    total_domains: int
-    completed_domains: int
-    failed_domains: int
-    progress: float
-    created_at: Optional[datetime] = None
-    updated_at: Optional[datetime] = None
-    start_time: Optional[datetime] = None
-    end_time: Optional[datetime] = None
-    processing_time: Optional[float] = None
-    domain_statuses: Dict[str, Any] = Field(default_factory=dict)
-    error: Optional[str] = None
-    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)
-
-    class Config:
-        from_attributes = True  # For SQLAlchemy model compatibility
-
 
 # Development mode utilities
 def is_development_mode() -> bool:
@@ -102,44 +82,11 @@ user_dependency = get_development_user if is_development_mode() else get_current
 
 
 # Request and response models
-class SitemapBatchRequest(BaseModel):
-    domains: List[str] = Field(..., description="List of domains to process")
-    max_pages: int = Field(1000, description="Maximum pages to process per domain")
-
-    @validator("domains")
-    def validate_domains(cls, domains):
-        # Basic validation of domains
-        if not domains:
-            raise ValueError("At least one domain is required")
-
-        if len(domains) > 100:
-            raise ValueError("Maximum 100 domains allowed per batch")
-
-        for domain in domains:
-            if not domain or len(domain) < 3:
-                raise ValueError(f"Invalid domain: {domain}")
-
-        return domains
-
-    @validator("max_pages")
-    def validate_max_pages(cls, max_pages):
-        if max_pages < 1:
-            raise ValueError("max_pages must be at least 1")
-        if max_pages > 10000:
-            raise ValueError("max_pages cannot exceed 10000")
-        return max_pages
-
-
-class SitemapBatchResponse(BaseModel):
-    batch_id: str
-    status: str
-    total_domains: int
-    status_url: str
 
 
-@router.post("/api/v3/sitemap/batch/create", response_model=SitemapBatchResponse)
+@router.post("/api/v3/sitemap/batch/create", response_model=BatchResponse)
 async def create_sitemap_batch(
-    request: SitemapBatchRequest,
+    request: BatchRequest,
     background_tasks: BackgroundTasks,
     session: AsyncSession = Depends(get_session_dependency),
     current_user: dict = Depends(user_dependency),
@@ -185,10 +132,9 @@ async def create_sitemap_batch(
         )
 
         # Return immediate response with batch details
-        return SitemapBatchResponse(
+        return BatchResponse(
             batch_id=batch_id,
-            status="pending",
-            total_domains=len(request.domains),
+            job_count=len(request.domains),
             status_url=f"/api/v3/sitemap/batch/status/{batch_id}",
         )
     except Exception as e:
@@ -248,7 +194,7 @@ async def process_sitemap_batch_with_own_session(
             # Session already manages its own transaction
             batch = await BatchJob.get_by_batch_id(session, batch_id)
             if batch:
-                batch.status = BATCH_STATUS_PROCESSING
+                batch.status = BatchJobStatus.PROCESSING
                 batch.start_time = func.now()
                 await session.flush()
                 logger.info(f"Updated batch {batch_id} status to processing")
diff --git a/src/routers/domains.py b/src/routers/domains.py
index b5b92f5..9e3b265 100644
--- a/src/routers/domains.py
+++ b/src/routers/domains.py
@@ -22,16 +22,13 @@ from src.auth.jwt_auth import get_current_user  # Corrected based on other route
 from src.db.session import (
     get_db_session,  # Import the correct session dependency
 )
-from src.models.api_models import (
+from src.schemas.domain import (
     DomainBatchCurationStatusUpdateRequest,
     DomainRecord,
     PaginatedDomainResponse,
 )
-from src.models.domain import (
-    Domain,
-    SitemapAnalysisStatusEnum,
-    SitemapCurationStatusEnum,
-)
+from src.models.domain import Domain
+from src.models.enums import SitemapAnalysisStatus, SitemapCurationStatus
 
 logger = logging.getLogger(__name__)
 
@@ -66,7 +63,7 @@ async def list_domains(
         ),
     ),
     sort_desc: bool = Query(True, description="Sort in descending order"),
-    sitemap_curation_status: Optional[SitemapCurationStatusEnum] = Query(
+    sitemap_curation_status: Optional[SitemapCurationStatus] = Query(
         None, description="Filter by sitemap curation status"
     ),
     domain_filter: Optional[str] = Query(
@@ -182,9 +179,7 @@ async def update_domain_sitemap_curation_status_batch(
 
     # Map API Enum to DB Enum (should match directly based on definition)
     try:
-        db_curation_status = SitemapCurationStatusEnum[
-            api_status.name
-        ]  # Use .name for reliable mapping
+        db_curation_status = api_status
     except KeyError as e:
         # Log the error including the invalid value received
         logger.error(
@@ -227,8 +222,8 @@ async def update_domain_sitemap_curation_status_batch(
             )
 
             # Conditional logic: If status is 'Selected', queue for analysis
-            if db_curation_status == SitemapCurationStatusEnum.Selected:
-                domain.sitemap_analysis_status = SitemapAnalysisStatusEnum.Queued  # type: ignore
+            if db_curation_status == SitemapCurationStatus.SELECTED:
+                domain.sitemap_analysis_status = SitemapAnalysisStatus.QUEUED  # type: ignore
                 domain.sitemap_analysis_error = None  # type: ignore # Clear previous errors
                 queued_count += 1
                 logger.debug(
diff --git a/src/routers/local_businesses.py b/src/routers/local_businesses.py
index 16dd1cd..a9ae31e 100644
--- a/src/routers/local_businesses.py
+++ b/src/routers/local_businesses.py
@@ -20,41 +20,18 @@ from src.auth.jwt_auth import (
 
 # Assuming DB Session, Auth dependencies are similar to other routers
 from src.db.session import get_db_session  # Adjust path if necessary
-from src.models.api_models import LocalBusinessBatchStatusUpdateRequest
-from src.models.local_business import DomainExtractionStatusEnum, LocalBusiness
-from src.models.place import PlaceStatusEnum  # For DB mapping
+from src.models.local_business import LocalBusiness
+from src.models.enums import DomainExtractionStatus, PlaceStatus
+from src.schemas.local_business import (
+    LocalBusinessBatchStatusUpdateRequest,
+    PaginatedLocalBusinessResponse,
+    LocalBusinessRecord,
+)
 
 logger = logging.getLogger(__name__)
 
 
-# --- Local Pydantic Models for GET Endpoint --- #
-# Define a Pydantic model that mirrors the LocalBusiness SQLAlchemy model
-# This ensures API responses have a defined schema.
-# Include fields needed by the frontend grid.
-class LocalBusinessRecord(BaseModel):
-    id: UUID
-    business_name: Optional[str] = None
-    full_address: Optional[str] = None
-    phone: Optional[str] = None
-    website_url: Optional[str] = None
-    status: Optional[PlaceStatusEnum] = None  # Use the DB enum here
-    domain_extraction_status: Optional[DomainExtractionStatusEnum] = None
-    created_at: datetime
-    updated_at: datetime
-    tenant_id: UUID
-
-    class Config:
-        from_attributes = True  # Enable ORM mode for conversion (Updated from orm_mode)
-        use_enum_values = True  # Ensure enum values are used in response
-
-
-# Response model for paginated results
-class PaginatedLocalBusinessResponse(BaseModel):
-    items: List[LocalBusinessRecord]
-    total: int
-    page: int
-    size: int
-    pages: int
+# Schemas are now correctly imported from src.schemas.local_business
 
 
 # --- Router Definition --- #
@@ -69,7 +46,7 @@ router = APIRouter(prefix="/api/v3/local-businesses", tags=["Local Businesses"])
     description="Retrieves a paginated list of local businesses, allowing filtering and sorting.",
 )
 async def list_local_businesses(
-    status_filter: Optional[PlaceStatusEnum] = Query(
+    status_filter: Optional[PlaceStatus] = Query(
         None,
         alias="status",
         description="Filter by main business status (e.g., New, Selected, Maybe)",
@@ -155,7 +132,7 @@ async def list_local_businesses(
 
         # --- Prepare Response --- #
         # Pydantic models with from_attributes=True handle the conversion
-        response_items = [LocalBusinessRecord.from_orm(item) for item in db_items]
+        response_items = [LocalBusinessRecord.model_validate(item) for item in db_items]
 
         return PaginatedLocalBusinessResponse(
             items=response_items, total=total, page=page, size=size, pages=total_pages
@@ -205,26 +182,11 @@ async def update_local_businesses_status_batch(
             "queued_count": 0,
         }
 
-    # Map the incoming API status enum member to the DB enum member (PlaceStatusEnum)
-    # Compare by NAME for robustness against potential value differences/casing
-    target_db_status_member = next(
-        (member for member in PlaceStatusEnum if member.name == new_api_status.name),
-        None,
-    )
-
-    if target_db_status_member is None:
-        # This case handles potential mismatches, e.g., if API enum has 'Not_a_Fit' and DB has 'Not a Fit'
-        # A more robust mapping might be needed if values differ significantly besides underscores.
-        logger.error(
-            f"API status '{new_api_status.name}' ({new_api_status.value}) has no matching member name in DB PlaceStatusEnum."
-        )
-        raise HTTPException(
-            status_code=status.HTTP_400_BAD_REQUEST,
-            detail=f"Invalid status mapping for '{new_api_status.value}'",
-        )
+    # The API request now uses the centralized PlaceStatus ENUM, so no mapping is needed.
+    target_db_status_member = new_api_status
 
     # Determine if domain extraction should be triggered based on the target DB status
-    trigger_domain_extraction = target_db_status_member == PlaceStatusEnum.Selected
+    trigger_domain_extraction = target_db_status_member == PlaceStatus.SELECTED
     if trigger_domain_extraction:
         logger.info(
             f"Target DB status '{target_db_status_member.name}' will trigger domain extraction queueing."
@@ -233,7 +195,7 @@ async def update_local_businesses_status_batch(
     # Define which existing domain extraction statuses allow re-queueing (e.g., only if failed or not yet processed)
     eligible_queueing_statuses = [
         None,  # Not yet processed
-        DomainExtractionStatusEnum.Error,  # Changed from 'failed' to 'Error' to match new enum
+        DomainExtractionStatus.ERROR,  # Changed from 'failed' to 'Error' to match new enum
         # Add other statuses if needed (e.g., completed if re-running is desired)
     ]
 
@@ -276,7 +238,7 @@ async def update_local_businesses_status_batch(
                     # REMOVED eligibility check: current_extraction_status in eligible_queueing_statuses:
                     # if current_extraction_status in eligible_queueing_statuses:
                     business.domain_extraction_status = (
-                        DomainExtractionStatusEnum.Queued
+                        DomainExtractionStatus.QUEUED
                     )  # type: ignore # Changed from 'queued' to 'Queued' to match new enum
                     business.domain_extraction_error = None  # type: ignore # Clear any previous error
                     actually_queued_count += 1
diff --git a/src/routers/modernized_page_scraper.py b/src/routers/modernized_page_scraper.py
index dafe5be..b495a04 100644
--- a/src/routers/modernized_page_scraper.py
+++ b/src/routers/modernized_page_scraper.py
@@ -20,12 +20,14 @@ from ..auth.jwt_auth import DEFAULT_TENANT_ID, get_current_user
 from ..config.settings import settings
 
 # Explicitly import needed models, excluding JobStatusResponse from here
-from ..models import (
+from ..schemas.batch import (
     BatchRequest,
     BatchResponse,
-    BatchStatusResponse,
+    BatchStatusResponse
+)
+from ..schemas.sitemap import (
     SitemapScrapingRequest,
-    SitemapScrapingResponse,
+    SitemapScrapingResponse
 )
 
 # Import JobStatusResponse from its correct schema location
diff --git a/src/routers/modernized_sitemap.py b/src/routers/modernized_sitemap.py
index 24efbb2..f4eabf3 100644
--- a/src/routers/modernized_sitemap.py
+++ b/src/routers/modernized_sitemap.py
@@ -33,7 +33,7 @@ from ..config.settings import settings
 
 # Import specific models, excluding JobStatusResponse from here
 # Import BatchStatusResponse separately if needed (assuming it's still in api_models)
-from ..models.api_models import (
+from ..schemas.sitemap import (
     SitemapScrapingRequest,
     SitemapScrapingResponse,
 )
diff --git a/src/routers/places_staging.py b/src/routers/places_staging.py
index 495cedc..21631a7 100644
--- a/src/routers/places_staging.py
+++ b/src/routers/places_staging.py
@@ -23,18 +23,13 @@ from ..auth.jwt_auth import get_current_user  # Corrected import for auth depend
 from ..db.session import get_db_session  # Assuming this provides the session
 
 # Import models and services
-from ..models.api_models import (
+from ..models.enums import GcpApiDeepScanStatus, PlaceStatus
+from ..models.place import Place
+from ..schemas.place import (
     PlaceStagingListResponse,
     PlaceStagingRecord,
-    PlaceStagingStatusEnum,
-)
-
-# We need a service or direct model access to interact with Place model
-from ..models.place import (  # Import the Place model AND the new DeepScanStatusEnum
-    GcpApiDeepScanStatusEnum,
-    Place,
-    PlaceStatusEnum,
 )
+from ..models.enums import PlaceStagingStatus
 
 # from ..models.user import User
 
@@ -76,7 +71,7 @@ class PlaceBatchStatusUpdateRequest(BaseModel):
     place_ids: List[str] = Field(
         ..., min_length=1, description="List of one or more Google Place IDs to update."
     )
-    status: PlaceStagingStatusEnum = Field(
+    status: PlaceStagingStatus = Field(
         ..., description="The new main status to set."
     )
     error_message: Optional[str] = Field(
@@ -95,7 +90,7 @@ class PlaceBatchStatusUpdateRequest(BaseModel):
     description="Retrieves a paginated list of all places currently in the staging area, optionally filtered by status.",
 )
 async def list_all_staged_places(
-    status_filter: Optional[PlaceStagingStatusEnum] = Query(
+    status_filter: Optional[PlaceStagingStatus] = Query(
         None, alias="status", description="Filter places by their main status"
     ),
     page: int = Query(1, ge=1, description="Page number"),
@@ -136,7 +131,7 @@ async def list_all_staged_places(
             try:
                 db_status_filter_member = next(
                     member
-                    for member in PlaceStatusEnum
+                    for member in PlaceStatus
                     if member.value == status_filter.value
                 )
                 count_query = count_query.where(Place.status == db_status_filter_member)
@@ -264,11 +259,11 @@ async def update_places_status_batch(
         logger.debug(
             f"Attempting to map API status: Name={new_main_status.name}, Value={new_main_status.value}"
         )
-        logger.debug(f"Database enum members: {[m.name for m in PlaceStatusEnum]}")
+        logger.debug(f"Database enum members: {[m.name for m in PlaceStatus]}")
         target_db_status_member = next(
             (
                 member
-                for member in PlaceStatusEnum
+                for member in PlaceStatus
                 if member.name.lower() == new_main_status.name.lower()
             ),
             None,
@@ -279,7 +274,7 @@ async def update_places_status_batch(
 
         if target_db_status_member is None:
             logger.error(
-                f"API status '{new_main_status.name}' ({new_main_status.value}) has no matching member name in DB PlaceStatusEnum."
+                f"API status '{new_main_status.name}' ({new_main_status.value}) has no matching member name in DB PlaceStatus."
             )
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
@@ -287,7 +282,7 @@ async def update_places_status_batch(
             )
 
         # Determine if deep scan should be triggered based on the *target database status*
-        trigger_deep_scan = target_db_status_member == PlaceStatusEnum.Selected
+        trigger_deep_scan = target_db_status_member == PlaceStatus.Selected
         logger.debug(
             f"Proceeding with DB status: {target_db_status_member.name}, Trigger deep scan? {trigger_deep_scan}"
         )
@@ -296,7 +291,7 @@ async def update_places_status_batch(
                 f"API status '{new_main_status.name}' (mapping to DB '{target_db_status_member.name}') will trigger deep scan queueing."
             )
 
-        eligible_deep_scan_statuses = [None, GcpApiDeepScanStatusEnum.Error]
+        eligible_deep_scan_statuses = [None, GcpApiDeepScanStatus.Error]
 
         # Execute within a transaction
         logger.debug(
@@ -338,12 +333,12 @@ async def update_places_status_batch(
                     trigger_deep_scan
                     and place.deep_scan_status in eligible_deep_scan_statuses
                 ):  # type: ignore
-                    place.deep_scan_status = GcpApiDeepScanStatusEnum.Queued  # type: ignore
+                    place.deep_scan_status = GcpApiDeepScanStatus.Queued  # type: ignore
                     place.deep_scan_error = None  # type: ignore
                     place.updated_at = now  # type: ignore
                     actually_queued_count += 1
                 elif error_message is not None: # Check if error_message is provided
-                    place.deep_scan_status = GcpApiDeepScanStatusEnum.Error.value # Set deep_scan_status to Error value # type: ignore
+                    place.deep_scan_status = GcpApiDeepScanStatus.Error.value # Set deep_scan_status to Error value # type: ignore
                     place.deep_scan_error = error_message # Assign the error message # type: ignore
                     place.updated_at = now # Update timestamp # type: ignore
 
@@ -401,7 +396,7 @@ async def queue_places_for_deep_scan(
     updated_count = 0
     try:
         # Ensure we only queue places whose current deep_scan_status indicates they haven't run successfully
-        eligible_statuses = [None, GcpApiDeepScanStatusEnum.Error]  # Changed from Error
+        eligible_statuses = [None, GcpApiDeepScanStatus.Error]  # Changed from Error
         stmt_update = (
             update(Place)
             .where(
@@ -411,7 +406,7 @@ async def queue_places_for_deep_scan(
                 ),  # Check current status before queuing
             )
             .values(
-                deep_scan_status=GcpApiDeepScanStatusEnum.Queued,  # Ensuring uppercase Queued is used
+                deep_scan_status=GcpApiDeepScanStatus.Queued,  # Ensuring uppercase Queued is used
                 deep_scan_error=None,  # Clear any previous error when queuing
                 updated_at=datetime.utcnow(),
             )
@@ -451,7 +446,7 @@ async def queue_places_for_deep_scan(
 )
 async def list_staged_places(
     discovery_job_id: UUID,
-    status_filter: Optional[PlaceStagingStatusEnum] = Query(
+    status_filter: Optional[PlaceStagingStatus] = Query(
         None,
         alias="status",
         description="Filter places by their current staging status",
diff --git a/src/routers/sitemap_files.py b/src/routers/sitemap_files.py
index fa2bbe1..9b4043f 100644
--- a/src/routers/sitemap_files.py
+++ b/src/routers/sitemap_files.py
@@ -13,9 +13,7 @@ from src.auth.jwt_auth import get_current_user
 
 # Core Dependencies
 from ..db.session import get_db_session
-from ..models.sitemap import (
-    SitemapImportCurationStatusEnum,
-)
+from ..models.enums import SitemapImportCurationStatus
 from ..schemas.sitemap_file import (
     PaginatedSitemapFileResponse,
     SitemapFileBatchUpdate,  # Assuming batch status update needed
@@ -55,7 +53,7 @@ sitemap_files_service = SitemapFilesService()
 async def list_sitemap_files(
     # Updated query parameters as per Spec 23.5 / Implementation 23.6
     domain_id: Optional[uuid.UUID] = Query(None, description="Filter by domain UUID"),
-    deep_scrape_curation_status: Optional[SitemapImportCurationStatusEnum] = Query(
+    deep_scrape_curation_status: Optional[SitemapImportCurationStatus] = Query(
         None,
         description="Filter by sitemap import curation status (New, Selected, etc.)",
     ),
diff --git a/src/routers/vector_db_ui.py b/src/routers/vector_db_ui.py
index 7ef034e..433f123 100644
--- a/src/routers/vector_db_ui.py
+++ b/src/routers/vector_db_ui.py
@@ -12,7 +12,7 @@ import asyncpg
 import os
 from dotenv import load_dotenv
 
-from src.db.session import get_session_dependency
+from src.session.async_session import get_session_dependency
 
 # Load environment variables
 load_dotenv()
diff --git a/src/schemas/sitemap_file.py b/src/schemas/sitemap_file.py
index 5bde302..99cb2c6 100644
--- a/src/schemas/sitemap_file.py
+++ b/src/schemas/sitemap_file.py
@@ -9,7 +9,7 @@ from typing import List, Optional
 from pydantic import BaseModel, Field
 
 # Import the Renamed Enum from the model to reuse it
-from ..models.sitemap import SitemapFileStatusEnum, SitemapImportCurationStatusEnum
+from ..models.enums import SitemapFileStatus, SitemapImportCurationStatus
 
 
 # Base Schema: Fields common to Create and Read
@@ -19,7 +19,7 @@ class SitemapFileBase(BaseModel):
     url: str = Field(
         ..., alias="sitemap_url", examples=["https://example.com/sitemap.xml"]
     )
-    status: SitemapFileStatusEnum = Field(default=SitemapFileStatusEnum.Pending)
+    status: SitemapFileStatus = Field(default=SitemapFileStatus.PENDING)
     file_path: Optional[str] = None
     error_message: Optional[str] = None
     processing_time: Optional[float] = None
@@ -45,13 +45,13 @@ class SitemapFileUpdate(BaseModel):
     domain_id: Optional[uuid.UUID] = None
     # Use alias for update as well
     url: Optional[str] = Field(None, alias="sitemap_url")
-    status: Optional[SitemapFileStatusEnum] = None
+    status: Optional[SitemapFileStatus] = None
     file_path: Optional[str] = None
     error_message: Optional[str] = None
     processing_time: Optional[float] = None
     url_count: Optional[int] = None
     # Use renamed Enum
-    deep_scrape_curation_status: Optional[SitemapImportCurationStatusEnum] = None
+    deep_scrape_curation_status: Optional[SitemapImportCurationStatus] = None
     # updated_by will likely be set automatically
     # If API allows manual update, add: updated_by: Optional[uuid.UUID] = None
 
@@ -69,7 +69,7 @@ class SitemapFileRead(SitemapFileBase):
     created_by: Optional[uuid.UUID] = None  # Include if set in model
     updated_by: Optional[uuid.UUID] = None  # Include if set in model
     # Use renamed Enum
-    deep_scrape_curation_status: Optional[SitemapImportCurationStatusEnum] = None
+    deep_scrape_curation_status: Optional[SitemapImportCurationStatus] = None
     domain_name: Optional[str] = None  # Added field for domain name
 
 
@@ -86,7 +86,7 @@ class PaginatedSitemapFileResponse(BaseModel):
 class SitemapFileBatchUpdate(BaseModel):
     sitemap_file_ids: List[uuid.UUID]
     # Use renamed Enum
-    deep_scrape_curation_status: SitemapImportCurationStatusEnum
+    deep_scrape_curation_status: SitemapImportCurationStatus
 
     class Config:
         # REMOVED use_enum_values = True
diff --git a/src/services/domain_scheduler.py b/src/services/domain_scheduler.py
index eebfc92..6fe44a6 100644
--- a/src/services/domain_scheduler.py
+++ b/src/services/domain_scheduler.py
@@ -16,7 +16,7 @@ from sqlalchemy.future import select
 
 from ..config.settings import settings
 from ..models.domain import Domain  # Import the Domain model
-from ..models.enums import DomainStatusEnum  # Import the new Enum
+from ..models.enums import DomainStatus  # Import the new Enum
 
 # Import the shared scheduler instance
 from ..scheduler_instance import scheduler
@@ -89,7 +89,7 @@ async def process_pending_domains(limit: int = 10):
             # Step 1: Fetch pending domains using ORM
             stmt = (
                 select(Domain)
-                .where(Domain.status == DomainStatusEnum.pending)  # Use Enum member
+                .where(Domain.status == DomainStatus.PENDING)  # Use Enum member
                 .order_by(Domain.updated_at.asc())
                 .limit(limit)
                 .with_for_update(skip_locked=True)
@@ -147,11 +147,11 @@ async def process_pending_domains(limit: int = 10):
 
                 try:
                     # Step 2.1: Update status to 'processing' IN MEMORY using setattr
-                    domain.status = DomainStatusEnum.processing  # Use Enum member
+                    domain.status = DomainStatus.PROCESSING  # Use Enum member
                     domain.last_error = None  # Clear previous error
                     domain.updated_at = datetime.utcnow()  # Keep updated_at fresh
                     logger.debug(
-                        f"Domain {domain_id} status set to '{DomainStatusEnum.processing.value}' in memory."
+                        f"Domain {domain_id} status set to '{DomainStatus.PROCESSING.value}' in memory."
                     )
                     # NO COMMIT here
 
@@ -181,12 +181,12 @@ async def process_pending_domains(limit: int = 10):
                     # Assuming Domain model has an update_from_metadata method
                     # that takes metadata dict and updates relevant fields IN MEMORY
                     await Domain.update_from_metadata(session, domain, metadata)
-                    domain.status = DomainStatusEnum.completed  # Use Enum member
+                    domain.status = DomainStatus.COMPLETED  # Use Enum member
                     domain.updated_at = datetime.utcnow()
 
                     domains_successful += 1
                     logger.info(
-                        f"Successfully processed and marked domain {domain_id} as '{DomainStatusEnum.completed.value}' in memory."
+                        f"Successfully processed and marked domain {domain_id} as '{DomainStatus.COMPLETED.value}' in memory."
                     )
 
                 except Exception as processing_error:
@@ -198,13 +198,13 @@ async def process_pending_domains(limit: int = 10):
                     domains_failed += 1
                     # Update status to 'error' IN MEMORY using setattr
                     try:
-                        domain.status = DomainStatusEnum.error  # Use Enum member
+                        domain.status = DomainStatus.ERROR  # Use Enum member
                         domain.last_error = error_message[
                             :1024
                         ]  # Truncate if necessary
                         domain.updated_at = datetime.utcnow()
                         logger.warning(
-                            f"Marked domain {domain_id} as '{DomainStatusEnum.error.value}' in memory."
+                            f"Marked domain {domain_id} as '{DomainStatus.ERROR.value}' in memory."
                         )
                     except AttributeError:
                         logger.error(
diff --git a/src/services/domain_sitemap_submission_scheduler.py b/src/services/domain_sitemap_submission_scheduler.py
index f8bb232..40a8704 100644
--- a/src/services/domain_sitemap_submission_scheduler.py
+++ b/src/services/domain_sitemap_submission_scheduler.py
@@ -17,7 +17,8 @@ from sqlalchemy.exc import SQLAlchemyError
 from sqlalchemy.future import select
 
 from src.config.settings import settings
-from src.models.domain import Domain, SitemapAnalysisStatusEnum
+from src.models.domain import Domain
+from src.models.enums import SitemapAnalysisStatus
 import asyncio
 from src.services.website_scan_service import WebsiteScanService
 from src.tasks.email_scraper import scan_website_for_emails
@@ -66,7 +67,7 @@ async def process_pending_domain_sitemap_submissions():
                 select(Domain.id)  # Select only IDs initially
                 .where(
                     # Standard ORM Enum comparison
-                    Domain.sitemap_analysis_status == SitemapAnalysisStatusEnum.Queued,
+                    Domain.sitemap_analysis_status == SitemapAnalysisStatus.Queued,
                 )
                 .order_by(Domain.updated_at.asc())
                 .limit(batch_size)
@@ -131,7 +132,7 @@ async def process_pending_domain_sitemap_submissions():
 
                 # 1. Mark as 'processing' (direct assignment)
                 locked_domain.sitemap_analysis_status = (
-                    SitemapAnalysisStatusEnum.Processing
+                    SitemapAnalysisStatus.Processing
                 )
                 locked_domain.sitemap_analysis_error = None
                 await session_inner.flush()  # Flush 'processing' state
@@ -153,18 +154,18 @@ async def process_pending_domain_sitemap_submissions():
                     asyncio.create_task(scan_website_for_emails(job.id))
                     logger.info(f"Queued background task for job {job.id} for domain {domain_id}.")
                     # Mark domain as completed since the job is now queued
-                    locked_domain.sitemap_analysis_status = SitemapAnalysisStatusEnum.Completed
+                    locked_domain.sitemap_analysis_status = SitemapAnalysisStatus.Completed
                     await session_inner.flush()
                     domains_submitted_successfully += 1
                 elif job.status == TaskStatus.RUNNING:
                     logger.info(f"Job {job.id} for domain {domain_id} is already running. No new task queued.")
                     # Mark domain as completed as a job is already active
-                    locked_domain.sitemap_analysis_status = SitemapAnalysisStatusEnum.Completed
+                    locked_domain.sitemap_analysis_status = SitemapAnalysisStatus.Completed
                     await session_inner.flush()
                     domains_submitted_successfully += 1
                 else:
                     logger.error(f"Failed to initiate scan for domain {domain_id}. Job status: {job.status}")
-                    locked_domain.sitemap_analysis_status = SitemapAnalysisStatusEnum.Error
+                    locked_domain.sitemap_analysis_status = SitemapAnalysisStatus.Error
                     locked_domain.sitemap_analysis_error = "Failed to create or find active job for scan."
                     await session_inner.flush()
                     domains_failed += 1
diff --git a/src/services/places/places_storage_service.py b/src/services/places/places_storage_service.py
index b4d301c..60a7794 100644
--- a/src/services/places/places_storage_service.py
+++ b/src/services/places/places_storage_service.py
@@ -12,7 +12,8 @@ from typing import Any, Dict, List, Optional, Tuple
 from sqlalchemy import and_, func, or_, select
 from sqlalchemy.ext.asyncio import AsyncSession
 
-from ...models.place import Place, PlaceStatusEnum
+from ...models.enums import PlaceStatus
+from ...models.place import Place
 from .places_service import PlacesService
 
 logger = logging.getLogger(__name__)
@@ -224,7 +225,7 @@ class PlacesStorageService:
                             "rating": place_data.get("rating"),
                             "user_ratings_total": place_data.get("user_ratings_total"),
                             "price_level": place_data.get("price_level"),
-                            "status": PlaceStatusEnum.New,
+                            "status": PlaceStatus.NEW.value,
                             "search_query": place_data.get("search_query"),
                             "search_location": place_data.get("search_location"),
                             "raw_data": place_data.get("raw_data"),
@@ -480,7 +481,7 @@ class PlacesStorageService:
             # Update the place directly - Use capitalized enum member NAME
             # Handle potential spaces in incoming status string for 'Not a Fit'
             status_name = status.replace(" ", "_")
-            place.status = PlaceStatusEnum[status_name]
+            place.status = PlaceStatus[status_name]
             place.updated_at = datetime.utcnow()
 
             # Add user_id for attribution if provided
@@ -560,7 +561,7 @@ class PlacesStorageService:
             # Convert status string to enum member - Use capitalized enum member NAME
             # Handle potential spaces in incoming status string for 'Not a Fit'
             status_name = status.replace(" ", "_")
-            status_enum_member = PlaceStatusEnum[status_name]
+            status_enum_member = PlaceStatus[status_name]
 
             # Process user ID if provided
             user_uuid = None
diff --git a/src/services/sitemap_files_service.py b/src/services/sitemap_files_service.py
index 8eac1b4..0bf116d 100644
--- a/src/services/sitemap_files_service.py
+++ b/src/services/sitemap_files_service.py
@@ -21,11 +21,11 @@ logger = logging.getLogger(__name__)
 logger.setLevel(logging.DEBUG)
 
 # Corrected import path
-from ..models.sitemap import (
-    SitemapFile,
-    SitemapImportCurationStatusEnum,
-    SitemapImportProcessStatusEnum,
+from ..models.enums import (
+    SitemapImportCurationStatus,
+    SitemapImportProcessStatus,
 )
+from ..models.sitemap import SitemapFile
 from ..schemas.sitemap_file import PaginatedSitemapFileResponse, SitemapFileRead
 
 # If get_session is needed later, import it:
@@ -80,7 +80,7 @@ class SitemapFilesService:
         page: int = 1,
         size: int = 15,
         domain_id: Optional[uuid.UUID] = None,
-        deep_scrape_curation_status: Optional[SitemapImportCurationStatusEnum] = None,
+        deep_scrape_curation_status: Optional[SitemapImportCurationStatus] = None,
         url_contains: Optional[str] = None,
         sitemap_type: Optional[str] = None,
         discovery_method: Optional[str] = None,
@@ -296,7 +296,7 @@ class SitemapFilesService:
         self,
         session: AsyncSession,
         sitemap_file_ids: List[uuid.UUID],
-        new_curation_status: SitemapImportCurationStatusEnum,
+        new_curation_status: SitemapImportCurationStatus,
         updated_by: Optional[uuid.UUID] = None,  # Added user tracking
     ) -> Dict[str, int]:
         """
@@ -334,12 +334,12 @@ class SitemapFilesService:
 
         # 3. Conditionally add process status update
         queue_for_processing = (
-            new_curation_status == SitemapImportCurationStatusEnum.Selected
+            new_curation_status == SitemapImportCurationStatus.Selected
         )
         if queue_for_processing:
             # Use renamed Enum for process status
             update_values[SitemapFile.sitemap_import_status] = (
-                SitemapImportProcessStatusEnum.Queued
+                SitemapImportProcessStatus.Queued
             )
             # Clear any previous error when re-queuing
             update_values[SitemapFile.sitemap_import_error] = None
diff --git a/src/services/sitemap_import_scheduler.py b/src/services/sitemap_import_scheduler.py
index 1aec8bb..c86ac06 100644
--- a/src/services/sitemap_import_scheduler.py
+++ b/src/services/sitemap_import_scheduler.py
@@ -8,7 +8,8 @@ from sqlalchemy import asc
 from src.common.curation_sdk.scheduler_loop import run_job_loop
 
 # Model and Enum Imports
-from src.models.sitemap import SitemapFile, SitemapImportProcessStatusEnum
+from src.models.enums import SitemapImportProcessStatus
+from src.models.sitemap import SitemapFile
 from src.scheduler_instance import scheduler  # Import shared scheduler instance
 
 # Service to be called
@@ -28,11 +29,11 @@ async def process_pending_sitemap_imports() -> None:
     try:
         await run_job_loop(
             model=SitemapFile,
-            status_enum=SitemapImportProcessStatusEnum,
-            queued_status=SitemapImportProcessStatusEnum.Queued,
-            processing_status=SitemapImportProcessStatusEnum.Processing,
-            completed_status=SitemapImportProcessStatusEnum.Completed,
-            failed_status=SitemapImportProcessStatusEnum.Error,
+            status_enum=SitemapImportProcessStatus,
+            queued_status=SitemapImportProcessStatus.Queued,
+            processing_status=SitemapImportProcessStatus.Processing,
+            completed_status=SitemapImportProcessStatus.Completed,
+            failed_status=SitemapImportProcessStatus.Error,
             # Pass the service method as the processing function
             processing_function=service.process_single_sitemap_file,
             # Use RENAMED setting
diff --git a/src/services/sitemap_import_service.py b/src/services/sitemap_import_service.py
index cc32da4..6ddfc8e 100644
--- a/src/services/sitemap_import_service.py
+++ b/src/services/sitemap_import_service.py
@@ -10,7 +10,8 @@ from sqlalchemy.ext.asyncio import AsyncSession
 
 from src.common.sitemap_parser import SitemapParser, SitemapURL
 from src.models.page import Page
-from src.models.sitemap import SitemapFile, SitemapImportProcessStatusEnum
+from src.models.enums import SitemapImportProcessStatus
+from src.models.sitemap import SitemapFile
 
 logger = logging.getLogger(__name__)
 
@@ -45,7 +46,7 @@ class SitemapImportService:
         # Double-check status before processing ( belt-and-suspenders )
         if (
             sitemap_file.sitemap_import_status
-            != SitemapImportProcessStatusEnum.Processing
+            != SitemapImportProcessStatus.Processing
         ):
             logger.warning(
                 f"SitemapFile {sitemap_file_id} is not in Processing state "
@@ -86,7 +87,7 @@ class SitemapImportService:
                     f"(URL: {sitemap_url_str})"
                 )
                 sitemap_file.sitemap_import_status = (
-                    SitemapImportProcessStatusEnum.Completed # type: ignore
+                    SitemapImportProcessStatus.Completed # type: ignore
                 )
                 sitemap_file.sitemap_import_error = None # type: ignore
                 await session.commit()
@@ -157,7 +158,7 @@ class SitemapImportService:
                 )
                 # Update status to Completed even if no URLs inserted
                 sitemap_file.sitemap_import_status = (
-                    SitemapImportProcessStatusEnum.Completed # type: ignore
+                    SitemapImportProcessStatus.Completed # type: ignore
                 )
                 sitemap_file.sitemap_import_error = None # type: ignore
                 await session.commit()
@@ -210,7 +211,7 @@ class SitemapImportService:
                 not sitemap_file_to_fail
             ):  # Only mark complete if no fatal error occurred during page insert
                 sitemap_file.sitemap_import_status = (
-                    SitemapImportProcessStatusEnum.Completed # type: ignore
+                    SitemapImportProcessStatus.Completed # type: ignore
                 )
                 sitemap_file.sitemap_import_error = None # type: ignore
                 logger.info(
@@ -226,7 +227,7 @@ class SitemapImportService:
                 f"HTTP error fetching sitemap {sitemap_url_str} " # Use string URL
                 f"(SitemapFile {sitemap_file_id}): {e.response.status_code} - {e}"
             )
-            sitemap_file.sitemap_import_status = SitemapImportProcessStatusEnum.Error # type: ignore
+            sitemap_file.sitemap_import_status = SitemapImportProcessStatus.Error # type: ignore
             sitemap_file.sitemap_import_error = f"HTTP Error: {e.response.status_code}" # type: ignore
             await session.commit()
         except httpx.RequestError as e:
@@ -235,7 +236,7 @@ class SitemapImportService:
                 f"Request error fetching sitemap {sitemap_url_str} " # Use string URL
                 f"(SitemapFile {sitemap_file_id}): {e}"
             )
-            sitemap_file.sitemap_import_status = SitemapImportProcessStatusEnum.Error # type: ignore
+            sitemap_file.sitemap_import_status = SitemapImportProcessStatus.Error # type: ignore
             sitemap_file.sitemap_import_error = (
                 f"Request Error: {str(e)[:1024]}"  # Truncate long errors # type: ignore
             )
@@ -255,7 +256,7 @@ class SitemapImportService:
                 )
                 if sitemap_file_in_error:
                     sitemap_file_in_error.sitemap_import_status = (
-                        SitemapImportProcessStatusEnum.Error # type: ignore
+                        SitemapImportProcessStatus.Error # type: ignore
                     )
                     sitemap_file_in_error.sitemap_import_error = str(e)[:1024] # type: ignore
                     await session.commit()
diff --git a/src/services/sitemap_scheduler.py b/src/services/sitemap_scheduler.py
index 3b57702..c02a055 100644
--- a/src/services/sitemap_scheduler.py
+++ b/src/services/sitemap_scheduler.py
@@ -16,14 +16,14 @@ from sqlalchemy import func, select, update
 
 from ..config.settings import settings
 
-# SitemapCurationStatusEnum removed (commented out) - Not used in this scheduler service.
-# Was likely added erroneously during previous model refactoring and caused ImportError.
-# from ..models.sitemap import SitemapFile, SitemapUrl, SitemapFileStatusEnum, SitemapCurationStatusEnum
+from ..models.enums import (
+    DomainExtractionStatus,
+    GcpApiDeepScanStatus,
+    TaskStatus,
+)
 from ..models.job import Job
-from ..models.local_business import DomainExtractionStatusEnum, LocalBusiness
-
-# Import the NEW Enum for deep scan status
-from ..models.place import GcpApiDeepScanStatusEnum, Place
+from ..models.local_business import LocalBusiness
+from ..models.place import Place
 
 # Import the shared scheduler instance
 from ..scheduler_instance import scheduler
@@ -73,7 +73,7 @@ async def handle_job_error(job_id: int, error_message: str):
                 update(Job)
                 .where(Job.id == job_id)
                 .values(
-                    status="failed",
+                    status=TaskStatus.FAILED,
                     error=error_message[:1024],  # Truncate error if too long
                     updated_at=func.now(),
                 )
@@ -209,7 +209,7 @@ async def process_pending_jobs(limit: int = 10):
             stmt_select = (
                 select(Place)
                 # Query using the dedicated deep_scan_status field and the NEW enum
-                .where(Place.deep_scan_status == GcpApiDeepScanStatusEnum.Queued)
+                .where(Place.deep_scan_status == GcpApiDeepScanStatus.QUEUED)
                 .order_by(Place.updated_at.asc())  # Process oldest first
                 .limit(limit)
                 # --- Reinstated after debugging --- #
@@ -235,7 +235,7 @@ async def process_pending_jobs(limit: int = 10):
 
                     try:
                         # Mark as Processing immediately using the NEW enum
-                        place.deep_scan_status = GcpApiDeepScanStatusEnum.Processing  # type: ignore
+                        place.deep_scan_status = GcpApiDeepScanStatus.PROCESSING  # type: ignore
                         place.updated_at = datetime.utcnow()  # type: ignore
                         await (
                             session.flush()
@@ -255,7 +255,7 @@ async def process_pending_jobs(limit: int = 10):
                                 f"Deep Scan: Success for Place ID: {place.place_id}"
                             )
                             # Update status to Completed using the NEW enum
-                            place.deep_scan_status = GcpApiDeepScanStatusEnum.Completed  # type: ignore
+                            place.deep_scan_status = GcpApiDeepScanStatus.COMPLETED  # type: ignore
                             place.deep_scan_error = None  # type: ignore
                             place.updated_at = datetime.utcnow()  # type: ignore
                             deep_scans_successful += 1
@@ -265,7 +265,7 @@ async def process_pending_jobs(limit: int = 10):
                                 f"Deep Scan: Failed for Place ID: {place.place_id} - Error: {error_msg}"
                             )
                             # Update status to Error using the NEW enum
-                            place.deep_scan_status = GcpApiDeepScanStatusEnum.Error  # type: ignore
+                            place.deep_scan_status = GcpApiDeepScanStatus.ERROR  # type: ignore
                             place.deep_scan_error = error_msg[:1024]  # type: ignore # Truncate
                             place.updated_at = datetime.utcnow()  # type: ignore
 
@@ -276,7 +276,7 @@ async def process_pending_jobs(limit: int = 10):
                         )
                         try:
                             # Attempt to mark as Error using the NEW enum on exception
-                            place.deep_scan_status = GcpApiDeepScanStatusEnum.Error  # type: ignore
+                            place.deep_scan_status = GcpApiDeepScanStatus.ERROR  # type: ignore
                             place.deep_scan_error = str(deep_scan_e)[:1024]  # type: ignore
                             place.updated_at = datetime.utcnow()  # type: ignore
                         except Exception as inner_e:
@@ -304,7 +304,7 @@ async def process_pending_jobs(limit: int = 10):
                 select(LocalBusiness)
                 .where(
                     LocalBusiness.domain_extraction_status
-                    == DomainExtractionStatusEnum.Queued
+                    == DomainExtractionStatus.QUEUED
                 )
                 .order_by(LocalBusiness.updated_at.asc())  # Process oldest first
                 .limit(limit)
@@ -339,7 +339,7 @@ async def process_pending_jobs(limit: int = 10):
 
                         # Update status to Processing IN MEMORY using setattr
                         business.domain_extraction_status = (
-                            DomainExtractionStatusEnum.Processing
+                            DomainExtractionStatus.PROCESSING
                         )  # type: ignore
                         business.domain_extraction_error = None  # type: ignore
                         await session.flush()  # Flush if needed before service call
@@ -359,7 +359,7 @@ async def process_pending_jobs(limit: int = 10):
                         if success:
                             # Assume service set status to completed or queued_for_analysis if appropriate
                             business.domain_extraction_status = (
-                                DomainExtractionStatusEnum.Completed
+                                DomainExtractionStatus.COMPLETED
                             )  # type: ignore
                             business.domain_extraction_error = None  # type: ignore
                             logger.info(
@@ -369,7 +369,7 @@ async def process_pending_jobs(limit: int = 10):
                         else:
                             # Assume service set status to failed and set error message
                             business.domain_extraction_status = (
-                                DomainExtractionStatusEnum.Error
+                                DomainExtractionStatus.ERROR
                             )  # type: ignore
                             business.domain_extraction_error = error_message[:1024]  # type: ignore
                             logger.warning(
@@ -385,7 +385,7 @@ async def process_pending_jobs(limit: int = 10):
                         # Update status to Failed IN MEMORY using the *existing* session with setattr
                         try:
                             business.domain_extraction_status = (
-                                DomainExtractionStatusEnum.Error
+                                DomainExtractionStatus.ERROR
                             )  # type: ignore
                             business.domain_extraction_error = error_message[:1024]  # type: ignore
                             logger.warning(
diff --git a/src/session/async_session.py b/src/session/async_session.py
index 43bf756..8214246 100644
--- a/src/session/async_session.py
+++ b/src/session/async_session.py
@@ -67,59 +67,47 @@ IS_DEVELOPMENT = is_development_environment()
 # Build connection string from Supabase settings
 def get_database_url() -> str:
     """
-    Build a SQLAlchemy-compatible connection string from environment variables.
-    Falls back to the original hardcoded string if environment variables are missing.
+    Builds a SQLAlchemy-compatible connection string from the settings object.
+
+    This function relies exclusively on the centralized 'settings' object as the
+    single source of truth for all database connection parameters.
+
+    Returns:
+        A string suitable for use with SQLAlchemy's create_async_engine.
+
+    Raises:
+        ValueError: If any of the required database connection settings are missing.
     """
-    # Try to use environment variables first
-    pooler_host = os.environ.get("SUPABASE_POOLER_HOST")
-    pooler_port = os.environ.get("SUPABASE_POOLER_PORT")
-    pooler_user = os.environ.get("SUPABASE_POOLER_USER")
-    password = os.environ.get("SUPABASE_DB_PASSWORD")
-    dbname = "postgres"  # Default database name for Supabase
-
-    # Extract project reference from Supabase URL if available
-    project_ref = None
-    if settings.supabase_url:
-        if "//" in settings.supabase_url:
-            project_ref = settings.supabase_url.split("//")[1].split(".")[0]
-        else:
-            project_ref = settings.supabase_url.split(".")[0]
-
-    # Check if all required env vars are available
-    if all([pooler_host, pooler_port, pooler_user, password]):
-        # Ensure password is a string before using quote_plus
-        safe_password = str(password) if password is not None else ""
-
-        # If pooler_user already includes project_ref, use it directly
-        if pooler_user and "." in pooler_user:
-            user_part = pooler_user
-        # Otherwise, append project_ref if available
-        elif project_ref:
-            user_part = (
-                f"{pooler_user}.{project_ref}"
-                if pooler_user
-                else f"postgres.{project_ref}"
-            )
-        else:
-            user_part = pooler_user or "postgres"
-
-        connection_string = (
-            f"postgresql+asyncpg://{user_part}:{quote_plus(safe_password)}"
-            f"@{pooler_host}:{pooler_port}/{dbname}"
+    required_vars = [
+        'supabase_pooler_host',
+        'supabase_pooler_port',
+        'supabase_pooler_user',
+        'supabase_pooler_password'
+    ]
+
+    missing_vars = [var for var in required_vars if not getattr(settings, var, None)]
+
+    if missing_vars:
+        raise ValueError(
+            f"Missing environment variables for database connection. "
+            f"Please set the following in your .env file: {', '.join(missing_vars)}"
         )
-        logger.info(
-            f"Using Supabase Supavisor connection pooler at {pooler_host}:{pooler_port}"
-        )
-        return connection_string
-
-    # Raise an error if environment variables are missing
-    # (instead of using hardcoded fallback)
-    raise ValueError(
-        "Missing environment variables for database connection. "
-        "Please set SUPABASE_POOLER_HOST, SUPABASE_POOLER_PORT, SUPABASE_POOLER_USER, "
-        "and SUPABASE_DB_PASSWORD in your .env file."
+
+    # All variables are present, build the connection string
+    safe_password = quote_plus(str(settings.supabase_pooler_password))
+    user_part = settings.supabase_pooler_user
+    host_part = settings.supabase_pooler_host
+    port_part = settings.supabase_pooler_port
+    db_name = settings.supabase_db_name or 'postgres'
+
+    connection_string = (
+        f"postgresql+asyncpg://{user_part}:{safe_password}"
+        f"@{host_part}:{port_part}/{db_name}"
     )
 
+    logger.info(f"Successfully built database connection string for Supavisor at {host_part}:{port_part}")
+    return connection_string
+
 
 # Get database URL with no fallback - fail loudly if connection details are missing
 try:
