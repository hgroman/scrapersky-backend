# Scheduler Refactoring Plan: Shared Instance

## 1. Problem Definition

Multiple background task queues (legacy sitemaps, deep scans, domain extractions, domain processing, sitemap submissions) are blocked or invisible due to incorrect scheduler initialization and management.

- **Root Cause:** Each scheduler service module (`domain_scheduler.py`, `sitemap_scheduler.py`, `domain_sitemap_submission_scheduler.py`) initializes its own independent `AsyncIOScheduler` instance.
- **Symptom 1 (Blocking):** The `sitemap_scheduler`'s job (`process_pending_jobs`) combines multiple unrelated tasks sequentially (sitemaps, deep scans, domain extractions). A failure or hang in the first step (sitemap processing, e.g., `buffaloapparel.com`) blocks the entire job run because `max_instances=1` is set for that scheduler instance. Subsequent tasks in that run are never reached, and future runs are skipped.
- **Symptom 2 (Invisibility):** The `/dev-tools/scheduler_status` endpoint only reports jobs from one of these independent scheduler instances (likely the first one initialized in `src/main.py`, the `domain_scheduler`). Jobs added to the other scheduler instances are running but not visible via the status endpoint.
- **Incorrect Previous Plan:** Patching the sitemap loop (timeout/error handling) only addresses Symptom 1 partially and doesn't fix the underlying issue of multiple schedulers (Symptom 2).

## 2. Solution: Consolidate to a Single Shared Scheduler

Refactor the application to use a single, globally accessible `AsyncIOScheduler` instance.

- **Step 1: Create Shared Scheduler Instance:**

  - Define a single `AsyncIOScheduler` instance in a central location. Options:
    - A new module, e.g., `src/scheduler_instance.py`.
    - Directly within `src/main.py` before the `lifespan` function.
    - _Decision:_ Create in `src/scheduler_instance.py` for better organization.
  - Ensure this instance is configured appropriately (e.g., timezone).

- **Step 2: Modify Scheduler Service Modules:**

  - In `src/services/domain_scheduler.py`, `src/services/sitemap_scheduler.py`, and `src/services/domain_sitemap_submission_scheduler.py`:
    - **Remove** the line `scheduler = AsyncIOScheduler()`.
    - **Import** the shared scheduler instance from its central location (e.g., `from ..scheduler_instance import scheduler`).
    - Verify that their respective `setup_..._scheduler` functions now use this _imported shared `scheduler` instance_ when calling `scheduler.add_job(...)`.

- **Step 3: Update `src/main.py`:**

  - **Import** the shared scheduler instance.
  - In the `lifespan` function:
    - **Remove** the lines that assign the return value of the `setup_...` functions (e.g., `domain_scheduler = setup_domain_scheduler()`). The setup functions now modify the shared instance directly.
    - Keep the calls to `setup_domain_scheduler()`, `setup_sitemap_scheduler()`, and `setup_domain_sitemap_submission_scheduler()` to ensure jobs are added to the shared scheduler.
    - Update the logging checks to verify the _shared_ scheduler's state (e.g., `if scheduler.running:`).
    - In the shutdown phase, call `scheduler.shutdown()` _once_ on the shared instance. Remove the individual `shutdown_..._scheduler()` calls.

- **Step 4: Update Dev Tools Endpoint:**
  - Modify the `/api/v3/dev-tools/scheduler_status` endpoint (`src/routers/dev_tools.py`):
    - **Import** the shared scheduler instance.
    - Ensure the endpoint function accesses the _shared `scheduler` instance_ to retrieve the list of jobs (`scheduler.get_jobs()`) and its running state.

## 3. Goal of Refactor

- Centralize scheduler management.
- Ensure all background jobs (`process_pending_domains`, `process_pending_jobs`, `process_pending_domain_sitemap_submissions`) are registered with and managed by the _same_ `AsyncIOScheduler` instance.
- Make all scheduled jobs visible via the `/dev-tools/scheduler_status` endpoint.
- Eliminate the possibility of multiple conflicting scheduler instances.
- Lay the groundwork for potentially separating the combined tasks in `process_pending_jobs` into truly distinct scheduled jobs in the future, if desired.

## 4. Next Step

Implement the refactoring steps detailed in section 2.

## 4.1 Implementation Status (YYYY-MM-DD)

**COMPLETE:** The refactoring plan outlined in Sections 1-3 of this document has been fully implemented across the codebase.

- A shared scheduler instance now exists in `src/scheduler_instance.py`.
- All relevant service modules (`domain_scheduler.py`, `sitemap_scheduler.py`, `domain_sitemap_submission_scheduler.py`) have been modified to import and use this shared instance.
- `src/main.py` has been updated to manage the lifecycle (startup, shutdown) of the single shared scheduler instance and correctly add jobs from the service modules.
- The `/api/v3/dev-tools/scheduler_status` endpoint in `src/routers/dev_tools.py` (detailed in Section 2, Step 4) now correctly reflects jobs from the shared instance.

**Outcome:** The goals outlined in Section 3 (centralized management, unified job visibility, elimination of conflicting instances) have been achieved. The system now operates with a single, shared `AsyncIOScheduler`.

## 4.2 Verification of Implementation (YYYY-MM-DD)

To confirm the successful implementation and correct operation of the shared scheduler architecture, the following verification steps were performed:

### Dependency Tree Analysis

A review of the code confirms the dependencies align with the shared scheduler design:

```
src/main.py
 |
 +-- Depends on: src/scheduler_instance.py (Imports `scheduler`)
 +-- Depends on: src/services/domain_scheduler.py (Imports `setup_domain_scheduler`)
 +-- Depends on: src/services/sitemap_scheduler.py (Imports `setup_sitemap_scheduler`)
 +-- Depends on: src/services/domain_sitemap_submission_scheduler.py (Imports `setup_domain_sitemap_submission_scheduler`)
 |
 +-- Calls `scheduler.start()` (from scheduler_instance.py)
 +-- Calls `setup_domain_scheduler()`
 |   `---> src/services/domain_scheduler.py
 |         |
 |         +-- Depends on: src/scheduler_instance.py (Imports `scheduler`)
 |         `-- Calls `scheduler.add_job(...)`
 |
 +-- Calls `setup_sitemap_scheduler()`
 |   `---> src/services/sitemap_scheduler.py
 |         |
 |         +-- Depends on: src/scheduler_instance.py (Imports `scheduler`)
 |         `-- Calls `scheduler.add_job(...)`
 |
 +-- Calls `setup_domain_sitemap_submission_scheduler()`
 |   `---> src/services/domain_sitemap_submission_scheduler.py
 |         |
 |         +-- Depends on: src/scheduler_instance.py (Imports `scheduler`)
 |         `-- Calls `scheduler.add_job(...)`
 |
 +-- Depends on: src/routers/dev_tools.py (Mounts the router)
     `---> src/routers/dev_tools.py
           |
           +-- Depends on: src/scheduler_instance.py (Imports `scheduler` for status endpoint)
           `-- Calls `scheduler.get_jobs()`
```

### Startup Logic Flowchart

The server startup sequence related to the scheduler follows this logic:

```
[Start Server (e.g., `python run_server.py`)]
      |
      V
[src/main.py: Application Starts]
      |
      V
[src/main.py: `lifespan` context manager begins]
      |
      V
[src/main.py: Imports shared `scheduler` from `src/scheduler_instance.py`]
      |
      V
[src/main.py: Calls `setup_domain_scheduler()` from `src/services/domain_scheduler.py`]
      |   | Adds `process_pending_domains` job to shared scheduler
      V
[src/main.py: Calls `setup_sitemap_scheduler()` from `src/services/sitemap_scheduler.py`]
      |   | Adds `process_pending_jobs` job to shared scheduler
      V
[src/main.py: Calls `setup_domain_sitemap_submission_scheduler()` from `src/services/domain_sitemap_submission_scheduler.py`]
      |   | Adds `process_pending_domain_sitemap_submissions` job to shared scheduler
      V
[src/main.py: Calls `scheduler.start()`]
      |
      V
[Shared Scheduler Instance Running with ALL registered jobs]
      |
      V
[src/main.py: FastAPI app starts serving requests (`yield`)]
      |
      V
[... Server Running ...]
      |
      V
[Server Shutdown Signal Received]
      |
      V
[src/main.py: `lifespan` context manager exits]
      |
      V
[src/main.py: Calls `scheduler.shutdown()`]
      |
      V
[Shared Scheduler Instance Stops]
```

### Runtime Log Verification

Analysis of server logs (`docker-compose logs --tail=150 scrapersky`) after the changes confirms correct operation:

- **Job Execution Start Logs:** Lines like `apscheduler.executors.default - INFO - Running job "Process Pending Domains ..."`, `apscheduler.executors.default - INFO - Running job "Process Sitemaps, DeepScans, DomainExtractions ..."`, and `apscheduler.executors.default - INFO - Running job "Process Pending Domain Sitemap Submissions ..."` demonstrate that the single APScheduler executor (belonging to the shared instance) is picking up all three distinct job types.
- **Job Execution Completion Logs:** Lines like `apscheduler.executors.default - INFO - Job "..." executed successfully` paired with `src.scheduler_instance - INFO - Scheduler job '... ' executed successfully.` for each of the three job IDs (`process_pending_domains`, `process_pending_jobs`, `process_pending_domain_sitemap_submissions`) confirm that the jobs are running to completion within the context of the shared scheduler instance.

**Conclusion:** The dependency structure, startup logic, and runtime logs all verify that the shared scheduler refactoring was implemented successfully and is functioning as intended. All background jobs are managed centrally by the single shared instance.

## 4.3 Job Configuration via Environment Variables

While the scheduler instance is shared, the specific parameters for each job (e.g., how often it runs, how many items it processes) are typically controlled via environment variables, adhering to conventions outlined in `README.md`.

Key examples include:

- `DOMAIN_SCHEDULER_INTERVAL_MINUTES`
- `DOMAIN_SCHEDULER_BATCH_SIZE`
- `DOMAIN_SCHEDULER_MAX_INSTANCES` (Should generally remain 1)
- `SITEMAP_SCHEDULER_INTERVAL_MINUTES`
- `SITEMAP_SCHEDULER_BATCH_SIZE`
- `SITEMAP_SCHEDULER_MAX_INSTANCES` (Should generally remain 1)
- `DOMAIN_SITEMAP_SCHEDULER_INTERVAL_MINUTES`
- `DOMAIN_SITEMAP_SCHEDULER_BATCH_SIZE`
- `DOMAIN_SITEMAP_SCHEDULER_MAX_INSTANCES` (Should generally remain 1)

These variables are usually set in the `.env` file or within the service definition in `docker-compose.yml`.

## 5. Related Dev Tools Fixes (YYYY-MM-DD) - Enhanced Explanation

While investigating separate issues (related to staging/queuing), several critical errors were identified and corrected within the `src/routers/dev_tools.py` file. Although potentially unrelated to the immediate staging problem, fixing these was essential for overall system stability and correct functionality of the development endpoints.

**Issues Corrected:**

1.  **Incorrect Function Import (`get_current_active_user_or_dev`):**

    - **Problem:** The code attempted to import and use a function (`get_current_active_user_or_dev`) from `src/auth/jwt_auth.py` which no longer exists or was incorrectly named. This function was referenced in the `require_dev_mode` dependency.
    - **Impact:** Any attempt to access routes protected by `require_dev_mode` would likely cause a server error (ImportError or NameError) at runtime, rendering the dev tools unusable.
    - **Fix:** Removed the incorrect import and the dependency on this function within `require_dev_mode`. Authentication is handled globally by the router's `Depends(get_current_user)`.

2.  **Incorrect Enum Import Path (`SitemapAnalysisStatusEnum`):**

    - **Problem:** The code imported `SitemapAnalysisStatusEnum` from `..models.enums` instead of its correct location, `..models.domain`.
    - **Impact:** Similar to the function import error, this would cause an `ImportError` if any part of the dev tools code relying on this enum was executed.
    - **Fix:** Corrected the import path to `from ..models.domain import SitemapAnalysisStatusEnum`.

3.  **Incorrect Environment Variable Check in `require_dev_mode`:**
    - **Problem:** The `require_dev_mode` function, intended to restrict dev tools access _only_ to development environments, was checking a non-existent setting `settings.SCRAPER_SKY_DEV_MODE`. The correct setting to check is `settings.environment`.
    - **Impact:** The development mode restriction was fundamentally broken. Depending on default values or other configurations, it might have incorrectly blocked access in development or incorrectly allowed access in production, creating a potential security or operational risk.
    - **Fix:** Updated the check to use `settings.environment.lower() not in ["development", "dev"]`, ensuring the dev tools are correctly gated based on the actual environment configuration.

**Importance of These Fixes:**

- **Prevents Runtime Crashes:** Correcting the imports prevents the server from crashing when dev tool endpoints are accessed.
- **Ensures Security/Correctness:** Fixing the environment check guarantees that development-specific tools are only accessible in the appropriate environment.
- **Code Hygiene:** Removes broken code, reducing potential confusion and preventing these errors from masking other underlying issues.
- **Maintainability:** Ensures the `dev_tools.py` module functions as expected for future development and troubleshooting tasks.

These changes establish a more stable baseline, even as we continue diagnosing the primary staging/queuing issue.

## 6. Monitoring & Troubleshooting the Shared Scheduler

- **Monitoring:** The status of all jobs registered with the shared scheduler can (and should) be monitored via the unified development endpoint:
  `GET /api/v3/dev-tools/scheduler_status`
  This endpoint retrieves job details directly from the shared `scheduler` instance (`scheduler.get_jobs()`) and shows their next run times, triggers, etc.

- **Troubleshooting - Stuck Jobs:** If background tasks seem stalled, check the application logs (`docker-compose logs scrapersky`) for warnings like:
  `Execution of job "..." skipped: maximum number of running instances reached (1)`
  This indicates a previous run of that specific job is stuck or running longer than its interval. Investigate the logs for errors within that job's execution (e.g., network timeouts, processing errors) as diagnosed during the session documented in `project-docs/21-19-and-20-Work-Order-Supplemental/21.1-Supplemental.md`.

## 7. Future Refinements Enabled

This shared scheduler architecture provides a foundation for future improvements. Notably, the monolithic `process_pending_jobs` function (handling sitemaps, deep scans, and domain extractions) could potentially be split into distinct, individually scheduled jobs running on the shared instance. This would offer finer-grained control and resilience for each task type.
