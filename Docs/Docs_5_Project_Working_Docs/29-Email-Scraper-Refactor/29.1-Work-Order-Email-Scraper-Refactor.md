# Work Order: Refactor Email Scraper Functionality

**Objective:** Refactor the email scraping API router (`src/routers/email_scanner.py`) and its associated background task (`src/tasks/email_scraper.py`) to be fully compliant with the project's ORM-Only principle and current architectural patterns (database connections, job management, auth context, tenant isolation removal).

**Target Files:**

- `src/routers/email_scanner.py`
- `src/tasks/email_scraper.py`

**Mandatory Development Workflow & Key Pitfalls:**

- **After EVERY significant code change:**
  1.  Restart the service: `docker-compose restart scrapersky`
  2.  Verify health: `docker-compose ps` (check for `(healthy)` status)
  3.  Check logs for errors: `docker-compose logs scrapersky | cat`
  4.  Perform relevant testing (`curl` examples, manual checks) before proceeding.
- **Key Pitfalls to Avoid:**
  - **Raw SQL:** Ensure NO raw SQL is used; strictly adhere to ORM methods.
  - **Transactions:** Verify correct use of `session.begin()` or `try/commit/except/rollback/finally` patterns.
  - **UUIDs/User IDs:** Use correct UUID types/formats and _valid_ test user IDs.
  - **Enums:** Double-check Enum values and **case sensitivity** match the database.
  - **Check Logs First:** Always consult application logs when troubleshooting issues.

**Core Architectural Principles Reminder:**

- **ORM ONLY:** Strictly use SQLAlchemy ORM methods (e.g., `select`, `update`). **No raw SQL** (e.g., `session.execute(text(...))`) is permitted. See `01-ABSOLUTE_ORM_REQUIREMENT.md` for details.
- **Tenant Isolation Removed:** Logic must operate globally. **Do NOT add `WHERE tenant_id = ...`** clauses to queries. Models retain the field, but it's not used for filtering. See `09-TENANT_ISOLATION_REMOVED.md`.
- **Standard DB Connections & Transactions:** Use `Depends(get_db_session)` in routers and `async with get_background_session() as session:` in background tasks. Adhere strictly to the `try (commit) / except (rollback) / finally (close)` pattern. See `07-DATABASE_CONNECTION_STANDARDS.md` / `13-TRANSACTION_MANAGEMENT_GUIDE.md`.
- **UUIDs:** Use standard UUIDs consistently. Key points:
  - **Generation:** Use `uuid.uuid4()` to generate new UUID objects.
  - **Database:** Define model columns using `sqlalchemy.dialects.postgresql.UUID as PGUUID` with `as_uuid=True` (e.g., `Column(PGUUID(as_uuid=True), default=uuid.uuid4)`).
  - **API Input:** For path parameters, query parameters, or request bodies, use the `uuid.UUID` type hint in FastAPI/Pydantic definitions. FastAPI/Pydantic will handle validation and conversion from the standard string format.
  - **API Output:** Convert UUID objects to strings (`str(uuid_obj)`) before returning them in JSON responses.
  - **Refer to `16-UUID_STANDARDIZATION_GUIDE.md` for full details, examples, and error handling.**
- **Test Users:** Use **valid, existing UUIDs from the `profiles` table** for `created_by`/`updated_by` fields. **Do NOT use placeholders** like `0000...`. See `10-TEST_USER_INFORMATION.md` for available test user UUIDs.

**Key Requirements Checklist:**

- [ ] **1. Eliminate Raw SQL:**

  - `[X]` `src/tasks/email_scraper.py`: Replaced 4 instances of `session.execute(text(...))` with ORM methods.
  - `[X]` `src/routers/email_scanner.py`: Replaced 3 instances of `cur.execute(...)` with ORM methods.
  - **Mandate:** Replace **ALL** instances of raw SQL execution (`cur.execute`, `session.execute(text(...))`) in both files with equivalent SQLAlchemy ORM operations.
  - **Models:** Utilize the appropriate SQLAlchemy models (e.g., `Domain`, potentially `Page`, `Contact`, `Job`) for all database interactions. Refer to `src/models/`.
  - **Enum Handling:** Pay close attention to Enum handling. **Critically important:** Use `.value` when filtering or updating via SQLAlchemy (`.where(Model.enum_field == MyEnum.Member.value)`, `.values(enum_field=MyEnum.Member.value)`). Ensure Python Enum _values_ **exactly match** the database enum labels (case-sensitive). See `Docs/Docs_1_AI_GUIDES/26-Supplemental.md` for common pitfalls.
  - **Tenant Filtering:** **Explicitly remove** any `WHERE tenant_id = ...` clauses from queries.

- [ ] **2. Verify SQLAlchemy Model Definitions:**

  - **Action:** Ensure SQLAlchemy model definitions (`src/models/*`) accurately reflect the current database schema. Verify `Column` types, nullability, `ForeignKey` constraints, and `relationship` definitions. Pay special attention to `SQLAlchemyEnum(..., name="db_enum_name", create_type=False)` usage. Consult `Docs/Docs_1_AI_GUIDES/25-SQLALCHEMY_MODEL_INTEGRITY_GUIDE.md` for detailed guidance.
  - **Checks:**
    - `[X]` `Contact` model created and verified (see `29.3-Database-Preparation.md`).
    - `[X]` `Page` model created and verified (see `29.3-Database-Preparation.md`). Syntax error and type hints fixed.
    - `[X]` `Domain` model relationships updated (see `29.3-Database-Preparation.md`).
    - `[X]` Resolved mapper initialization errors by importing `Page` and `Contact` into `src/models/__init__.py`.
    - `[ ]` Verify `Job` model definition and relationships relevant to email scraping.
    - `[ ]` Verify any other models used in the refactored code (`User` for `created_by`?).
  - **DB Verification:**
    - `[X]` `contacts` table schema and `contact_email_type_enum` verified (see `29.3-Database-Preparation.md`).
    - `[ ]` Verify `jobs` table schema if changes are needed.
    - Use DB inspection tools (`scripts/db/simple_inspect.py` is recommended **(Run from project root!)**) and check DB Enum values directly if needed. Use this query to check enum values (replace `task_status` if needed): `SELECT unnest(enum_range(NULL::task_status)) AS value;`. See SQL query in `26-Supplemental.md`. **Note:** Run inspection scripts from project root (see `scripts/db/DATABASE_CONNECTION_NOTE.md`).

- [x] **3. Update Database Connection Handling:**

  - `[X]` **Router (`email_scanner.py`):** Removed dependency on `..db.sb_connection`. Implemented database access using `Depends(get_session_dependency)`.
  - `[X]` **Task (`email_scraper.py`):** Verified database operations use `async with get_background_session() as session:`.
  - **Transaction Pattern:** Ensure session handling strictly follows the `try/except/finally` pattern with `commit/rollback/close` as detailed in `13-TRANSACTION_MANAGEMENT_GUIDE.md`.

- [x] **4. Fix Background Task Invocation & User Context:**

  - In `email_scanner.py` (router), correct `background_tasks.add_task(scan_website_for_emails, ...)`: Remove invalid `client` parameter.
  - **User Context Decision:** The `user_id` of the authenticated user making the API request **MUST** be passed from the router as an argument to the `scan_website_for_emails` background task. The task will then use this ID for the `created_by` field when creating the `Job` record. Ensure the passed `user_id` corresponds to a _valid_, existing entry in the `users` table (see `10-TEST_USER_INFORMATION.md`).

- [ ] **5. Implement Proper Job Status Tracking & Logic:**

  - Replace the in-memory `scan_jobs: Dict` with the persistent `Job` model. _Implemented._
  - Use the `Job` model to create/update/retrieve job status. _Implemented._
  - **Job Creation:** When creating a new `Job` record, set `status` to `TaskStatus.PENDING.value`, `job_type` to e.g., `'email_scan'`, `progress` to `0.0`, and populate `created_by` using the `user_id` passed from the router. **Crucially, the `domain_id` requested by the user is also stored in this `Job` record.** _Implemented._
  - **Data Flow to Task:** The router (`email_scanner.py`) initiates the background task (`email_scraper.py`) by passing the unique `job_id` (UUID) of the newly created `Job` record, along with the `user_id`. The background task then uses this `job_id` within its own database session to fetch the full `Job` object (`job = await Job.get_by_job_id(session, job_id)`). From this `job` object, the task accesses the necessary `job.domain_id` to identify which domain needs to be scraped. _Implemented._
  - **Task Updates:** The `scan_website_for_emails` task in `src/tasks/email_scraper.py` has been refactored:
    - Accepts `job_id: uuid.UUID` and `user_id: uuid.UUID` as arguments. _Implemented._
    - Fetches the `Job` record using `job_id` at the start. _Implemented._
    - Updates `job.status` to `RUNNING` after successful startup. _Implemented._
    - Updates `job.progress` incrementally during the crawl. _Implemented._
    - Updates `job.status` to `COMPLETE` or `FAILED` in a `finally` block to ensure final status is set. _Implemented._
    - Stores results in `job.result_data` and errors in `job.error`. _Implemented._
    - Consolidated database operations within the main task's session context. _Implemented._
  - **Progress Updates:** The `scan_website_for_emails` task MUST update the `Job.progress` field. A simple approach is setting it to `1.0` upon successful completion. More granular updates (e.g., per page scraped) are optional but preferred if feasible. _Implemented via page count percentage._
  - **User IDs:** If setting `created_by`/`updated_by` fields in the `Job` record or elsewhere, ensure valid test user UUIDs (from `10-TEST_USER_INFORMATION.md`, must exist in DB) are used, respecting FK constraints. Verify these UUIDs exist in the DB. See `26-Supplemental.md` for debugging notes on FK violations. _Implemented for `created_by`._

- [ ] **6. Handle Job Uniqueness:**

  - In the `POST /api/v3/scan/website` router logic, before creating a new job, check if a `Job` record already exists for the requested `domain_id` with a status of `TaskStatus.PENDING.value` or `TaskStatus.RUNNING.value`. _Implemented in `src/routers/email_scanner.py`._
  - If such a job exists, **do not create a new one**. Instead, return the `job_id` of the existing PENDING/RUNNING job with a `202 Accepted` status, similar to the successful creation case. _Implemented._

- [ ] **7. Define Failure Semantics for Task:**

  - The `scan_website_for_emails` task should update the `Job` status to `TaskStatus.FAILED.value` if:
    - A critical, unrecoverable error occurs during scraping (e.g., network error after retries if implemented, fatal parsing error). _Implemented via main try/except block._
    - The target domain is invalid or inaccessible. _Implemented via checks after fetching Job/Domain._
  - Upon failure, the `Job.error` field MUST be populated with a descriptive error message. _Implemented._
  - `Job.result_data` should ideally be `None` or an empty list/dict on failure, unless partial results are meaningful and explicitly decided upon. _Implemented (set to empty list initially, not updated on failure)._
  - Finding zero emails is **not** necessarily a failure; it should likely result in a `TaskStatus.COMPLETE.value` status with empty `result_data`. Define this behavior explicitly. _Implemented (task sets COMPLETE status and stores found emails, which could be an empty list)._

- [ ] **8. Define Email Handling Logic for Task:**

  - The `scan_website_for_emails` task MUST perform the following on found email addresses before storing them:
    - Convert emails to **lowercase**. _Implemented during contact creation._
    - **Deduplicate** the list of found emails. _Implemented by fetching distinct emails linked to the job at the end._
    - Basic **validation** (e.g., using a regex pattern) is recommended to filter out clearly invalid formats. _Implicitly handled by `extract_emails_from_text` regex._
  - **Storage:**
    - Emails found during the scan should be stored directly in the `contacts` table as Contact records linked to the domain.
    - Each email record should include the contact type (service, corporate, etc.) determined by `get_email_type()`.
    - Pages visited during the scan should be stored in the `pages` table with proper metadata.
    - The job record should primarily track status and progress, NOT store the actual discovered data.

- [ ] **9. Resolve Imports and Dependencies:**

  - Ensure correct imports for models and utility functions. Remove obsolete imports.

- [ ] **10. Data Integrity Check (Optional but Recommended):**

  - Check existing `contacts` data for potential issues.

- [ ] **11. Code Review & Linting:**

  - Ensure code passes `ruff check .`.
  - Verify readability and adherence to project style guides.

- [ ] **12. Testing & Verification:**

  - **Mandatory Checks:**
    - Run the `curl` examples provided in the "API Testing Examples" section to verify endpoint functionality, including the job uniqueness check (requesting a scan twice for the same domain).
    - Manually inspect the `jobs` table in the database after initiating scans:
      - Ensure records are created with the correct initial status (`pending`), `job_type`, etc.
      - Verify the `created_by` field is populated with the correct user UUID corresponding to the JWT token used for the API request (e.g., for the `scraper_sky_2024` token, check against the user ID in `10-TEST_USER_INFORMATION.md`).
    - After a task should have completed or failed, re-inspect the relevant `jobs` record to verify the `status`, `result_data`, `error`, and `progress` fields are updated correctly according to the defined semantics.
    - Monitor application logs (`docker-compose logs scrapersky | cat`) during task execution to ensure the `scan_website_for_emails` task runs and completes (or fails) without unexpected errors.
  - **Automated Tests:** Creating/updating relevant `pytest` tests for the API endpoints and potentially the background task logic **is in scope** for this work order.
  - **Conventions:** Refer to `Docs/Docs_1_AI_GUIDES/22-TESTING_CONVENTIONS_GUIDE.md` for project standards if implementing tests.

- [ ] **13. Consult Debugging Guide:**

  - Refer to `Docs/Docs_1_AI_GUIDES/26-Supplemental.md` for common debugging patterns and pitfalls encountered with ORM, Enums, transactions, authentication, and frontend interactions.

- [ ] **14. Concurrency Consideration (Note):**
  - While not a strict requirement for this refactor, consider potential concurrency issues if many scans are expected to run simultaneously. The current background task system might handle queuing. Be mindful of potential resource limits (e.g., memory on Render). No specific action required unless issues arise during testing.

**Job Model & Status Details:**

- **Model:** `Job` (defined in `src/models/job.py`). Key fields:
  - `id`: Integer (Primary Key, Internal)
  - `job_id`: UUID (Unique ID for API use, generated automatically)
  - `job_type`: String (e.g., `'email_scan'`)
  - `status`: String (Stores the current status)
  - `created_by`: UUID (User who initiated the job)
  - `domain_id`: UUID (Optional FK to `domains.id`)
  - `progress`: Float (0.0 to 1.0)
  - `result_data`: JSONB (Stores results, e.g., list of contacts)
  - `error`: String (Stores error message if failed)
  - `created_at`, `updated_at`: DateTime (Inherited from BaseModel)
- **Status Enum:** `TaskStatus` (defined in `src/models/__init__.py`). Use this Enum when working with job status:
  - `from src.models import TaskStatus # <-- Canonical Import`
  - `TaskStatus.PENDING` (`'pending'`)
  - `TaskStatus.RUNNING` (`'running'`)
  - `TaskStatus.COMPLETE` (`'complete'`)
  - `TaskStatus.FAILED` (`'failed'`)
  - `TaskStatus.CANCELLED` (`'cancelled'`)
- **Usage:** The `Job.status` field should be updated using the string values from the `TaskStatus` enum (e.g., `job.status = TaskStatus.COMPLETE.value`).

**API Endpoint Specification (Target State):**

- **Initiate Scan:**
  - **Endpoint:** `POST /api/v3/scan/website`
  - **Source:** `src/routers/email_scanner.py` (likely in a function like `scan_website_for_emails_api`)
  - **Authentication:** Requires valid JWT token.
  - **Request Schema:** Define a Pydantic model (e.g., `EmailScanRequest` in `src/schemas/email_scan.py`) like:
    ```python
    from pydantic import BaseModel
    from uuid import UUID
    class EmailScanRequest(BaseModel):
        domain_id: UUID # Or potentially url: str
    ```
  - **Response Schema (Success: 202 Accepted):** Returns a Pydantic model (e.g., `JobSubmissionResponse` in `src/schemas/job.py`) like:
    ```python
    from pydantic import BaseModel
    from uuid import UUID
    class JobSubmissionResponse(BaseModel):
        job_id: UUID
    ```
    (Note: `job_id` will be stringified in the final JSON response).
  - **Response (Error):** Standard error format (e.g., 4xx/5xx with detail message).
- **Check Scan Status:**

  - **Endpoint:** `GET /api/v3/scan/status/{job_id}`
  - **Source:** `src/routers/email_scanner.py` (likely in a function like `get_scan_status_api`)
  - **Authentication:** Requires valid JWT token.
  - **Path Parameter:** `job_id` (Use `UUID` type hint in router function signature).
  - **Response Schema (Success: 200 OK):** Returns a Pydantic model mirroring the `Job` structure (e.g., `JobStatusResponse` in `src/schemas/job.py` or `src/models/api_models.py`). Key fields:

    ```python
    from pydantic import BaseModel, Field
    from uuid import UUID
    from typing import Optional, Dict, Any
    from datetime import datetime
    from src.models import TaskStatus # Assuming TaskStatus enum used here

    class JobStatusResponse(BaseModel):
        job_id: UUID
        status: TaskStatus # Or str if using raw strings
        progress: float = Field(default=0.0)
        result_data: Optional[Dict[str, Any]] = None # Or List[str] for emails?
        error: Optional[str] = None
        created_at: Optional[datetime] = None
        updated_at: Optional[datetime] = None
        # Add other relevant fields like domain_id if needed

        class Config:
            from_attributes = True # Enable ORM mode if returning Job model directly
    ```

    (Note: UUID/datetime fields will be stringified in the final JSON response).

  - **Response (Error):** Standard error format (e.g., 404 if job not found, 5xx).

**API Testing Examples (curl):**

- **Initiate Scan (Example - Adjust Body as needed):**
  ```bash
  curl -X POST \
       -H "Authorization: Bearer scraper_sky_2024" \
       -H "Content-Type: application/json" \
       -d '{"domain_id": "YOUR_DOMAIN_UUID_HERE"}' \
       "http://localhost:8000/api/v3/scan/website" | cat
  ```
- **Check Scan Status (Replace JOB_UUID):**
  ```bash
  curl -H "Authorization: Bearer scraper_sky_2024" \
       "http://localhost:8000/api/v3/scan/status/YOUR_JOB_UUID_HERE" | cat
  ```

**Definition of Done:**

- All requirements in the checklist above are completed and verified.
- Code passes `ruff check .` and `pytest` tests for modified/new code.
- `curl` examples execute successfully and return expected status codes/data.
- Manual database and log checks confirm correct job creation, status updates, results/errors, and background task execution.
- Code is reviewed and approved.

**Acceptance Criteria:**

- Email scanning functionality is operational via the API defined above.
- Code is fully ORM-based and respects current architectural standards (no tenant filtering, proper transactions, UUIDs, test users).
- Database connections are handled correctly.
- Job status is tracked persistently and accurately reflects task outcomes.
- Code passes linting checks and relevant automated tests.
