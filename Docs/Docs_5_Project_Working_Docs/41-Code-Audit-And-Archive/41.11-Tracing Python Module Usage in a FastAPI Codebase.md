# Tracing Python Module Usage in a FastAPI Codebase

Accurately identifying all **used Python files** in a complex codebase requires combining static analysis with targeted runtime tracing. In this FastAPI project, the primary API (`main.py` via Uvicorn), background jobs scheduled via APScheduler, and standalone CLI scripts each serve as entry points. We need to capture every module imported along these execution paths, so that unused files can be detected and removed. Below we evaluate modern tools for module dependency tracing, identify gaps in the current approach, and propose an improved end-to-end solution.

## Tool Evaluations: Static Dependency Analysis

**Static import analysis** maps out the module dependency graph without running the code. Several tools can help build this graph:

- **Snakefood (sfood):** An older AST-based dependency analyzer that generates a list of imports between files. It can produce a dependency graph or list of required files. However, Snakefood hasn’t been updated for Python 3 (its last update was in 2009) ([Module dependency graph in Python 3 - Stack Overflow](https://stackoverflow.com/questions/23962614/module-dependency-graph-in-python-3#:~:text=I%20presume%20you%20are%20talking,equally%20well%20with%20any%20Python)). While some have gotten it to run under Python 3 with minimal fixes ([Module dependency graph in Python 3 - Stack Overflow](https://stackoverflow.com/questions/23962614/module-dependency-graph-in-python-3#:~:text=I%20presume%20you%20are%20talking,equally%20well%20with%20any%20Python)), its reliance on deprecated libraries makes it less reliable today. In practice, one can still use Snakefood via Python 2.7 as a workaround – e.g. piping its output and diffing against all files to find unused modules ([Finding Unused Python Files · GitHub](https://gist.github.com/kobybum/a162be138b2d19e3eac20547a7f10fa3#:~:text=Generate%20a%20list%20of%20included,project%20%3E%20%2Ftmp%2Fout.deps)) – but this is not a robust long-term solution.

- **Pyan3:** Pyan3 is a static _call graph_ generator, focusing on function-call dependencies rather than just imports. It can produce graphs of which functions call which (and by extension which modules use which), but it’s less direct for simply listing all imported modules. Pyan3 might be useful if we needed to trace execution flow, but for module reachability, dedicated import graph tools are more suitable.

- **ModuleGraph / ModuleFinder:** The built-in `modulefinder` module and the more advanced **ModuleGraph** library analyze bytecode to find import statements. ModuleGraph, used in tools like py2app, is actively maintained and handles many import nuances ([modulegraph · PyPI](https://pypi.org/project/modulegraph/#:~:text=modulegraph%20determines%20a%20dependency%20graph,bytecode%20analysis%20for%20import%20statements)). It can start from a given script and recursively discover all modules it imports (including handling of `importlib` and `__import__` in many cases). ModuleGraph’s strength is robust handling of tricky import patterns and package `__init__.py` files ([modulegraph · PyPI](https://pypi.org/project/modulegraph/#:~:text=modulegraph%20determines%20a%20dependency%20graph,bytecode%20analysis%20for%20import%20statements)). It constructs a dependency graph of modules which we can traverse or export.

- **pydeps:** pydeps is a modern CLI tool (and library) built on static analysis to visualize dependencies ([pydeps · PyPI](https://pypi.org/project/pydeps/#:~:text=pydeps%20finds%20imports%20by%20looking,if)). It finds imports by scanning Python bytecode for import opcodes ([pydeps · PyPI](https://pypi.org/project/pydeps/#:~:text=pydeps%20finds%20imports%20by%20looking,if)), effectively leveraging Python’s own compiler information. pydeps can generate a graph (in GraphViz or SVG) of module dependencies, and it can be configured to include or exclude standard library/internal modules. As an example, running `pydeps` on a project will report only modules that are actually imported (it ignores files that exist but are never imported) ([pydeps · PyPI](https://pypi.org/project/pydeps/#:~:text=pydeps%20finds%20imports%20by%20looking,if)). This makes it a good candidate to replace a custom AST parser – it’s Python 3 compatible and maintained (latest release in 2025) ([pydeps · PyPI](https://pypi.org/project/pydeps/#:~:text=Latest%20version)). It focuses purely on static imports, so any file not reachable via a normal import chain from the specified entry point will be omitted.

- **import-deps (schettino72/import-deps):** A lightweight Python 3 library that uses AST to find imports within a given set of files ([GitHub - schettino72/import-deps: find python module imports](https://github.com/schettino72/import-deps#:~:text=ast_imports)). It allows you to load a set of project modules and query what each imports. For example, given a package `foo` with modules A, B, C, where A imports B and C, the tool can list `foo.B` and `foo.C` as dependencies of `foo.A` ([GitHub - schettino72/import-deps: find python module imports](https://github.com/schettino72/import-deps#:~:text=)). This can be used programmatically to build the full dependency closure. It’s similar in spirit to Snakefood (AST-based), but updated and focused on internal imports (ignoring external modules unless specified).

- **Import Linter (grimp):** Import Linter’s underlying library, Grimp, can build an import graph of a project’s modules. It’s mainly intended for enforcing architectural rules, but it does parse all import statements in the code. This could be leveraged to get a graph of what imports what. However, using Grimp would be akin to writing another custom static analyzer, given that we have easier options like pydeps or ModuleGraph.

In summary, **modern static analysis tools can replace the custom AST import parser**. For example, using **ModuleGraph or pydeps** would directly yield the set of modules reachable from each entry point, including transitive imports. These tools handle Python 3 syntax and many edge cases out of the box. Notably, they will not list files that are present in the directory but never imported by any executed code ([pydeps · PyPI](https://pypi.org/project/pydeps/#:~:text=pydeps%20finds%20imports%20by%20looking,if)). This aligns with our goal of pinpointing unused files.

**Dynamic import detection:** Static tools above only catch imports that appear as static statements in the code. If modules are loaded dynamically (e.g. via `importlib.import_module(name)` or using the built-in `__import__` with variable names), static analysis may miss them or flag them only as generic patterns. A custom AST visitor can help identify _potential_ dynamic imports (by scanning for calls to `import_module` or `__import__`), but it won’t know the actual module string in complex cases. In our current toolkit, `dynamic_imports.py` does exactly this – it records any code pattern that looks like a dynamic import for manual review. There aren’t off-the-shelf static tools that fully resolve dynamic import targets, so this AST approach is reasonable. The goal is to ensure no module is being pulled in via a string name outside the regular import statements.

## Gaps in Coverage and Current Approach

The codebase’s execution paths include the FastAPI app startup, APScheduler background jobs, and various CLI scripts. Each of these might import different modules. Some gaps we need to mind:

- **Multiple Entry Points:** Many dependency tools assume a single entry script. Here, `main.py` (FastAPI app) is one root, but scripts in `scripts/` directory are additional roots that must be traced as well. Any static analysis must be run for each entry point or otherwise configured to include all modules loaded by each. An improvement is to iterate over all known entry scripts and aggregate their import graphs. Failing to do so could miss modules that are used only by a CLI script and never by the main app.

- **Transitive Imports:** A naive AST scan might list only direct imports per file and not automatically follow those imports deeper. The current solution’s **AST-based tracing** likely required manually traversing the import graph. Modern tools like ModuleGraph handle transitive imports naturally (they perform a graph traversal). If the custom static parser only wrote out direct dependencies, one must recursively resolve those to get the full set of reachable files. Any gap in that recursion could cause **missed modules**. For instance, if `main.py` imports `routers/domains.py`, which in turn imports `models/domain.py`, we need to ensure `models/domain.py` is counted. A robust module graph tool will catch that; a simplistic parser might not without extra logic.

- **Conditional Imports and Dead Code:** Static analysis will flag any import statement, even inside an `if` block or a function that isn’t always called. This can lead to false positives – modules that _could_ be imported but maybe never actually execute. For example, if a CLI script only imports a module when a certain flag is set, static analysis will still mark it as used. Our goal is to err on the side of caution (better to treat it as used than incorrectly delete it), but we should be aware that static reachability is an over-approximation. Conversely, runtime analysis (like actually running the app) might under-approximate if not all code paths are exercised. This is why **combining static and runtime data** is valuable: static gives the superset of possible imports; runtime confirms which were truly loaded in a given run.

- **APScheduler Jobs:** APScheduler introduces _time-triggered_ execution paths. In our app, the `setup_domain_scheduler()`, `setup_sitemap_scheduler()`, etc., register jobs on the shared scheduler ([main.py](file://file-58zKNV7SpSbKh7GftzpfKo#:~:text=try%3A%20setup_sitemap_scheduler,exc_info%3DTrue)) ([main.py](file://file-58zKNV7SpSbKh7GftzpfKo#:~:text=try%3A%20setup_domain_sitemap_submission_scheduler,exc_info%3DTrue)). The functions or callables scheduled may reside in other modules. Typically, the scheduler registration code will import whatever function it needs to schedule. For example, `domain_scheduler.py` might contain `scheduler.add_job(func=expire_domains, ...)` and `expire_domains` could be defined in `domain_service.py`. If so, static analysis should catch the import of `domain_service.py` in `domain_scheduler.py` (or if `expire_domains` is defined in the same file, that file is obviously included). However, one potential gap is if the scheduler is configured with a job by string reference or loads jobs from a database – those would be dynamic and not visible to static tools. In our scenario, it appears jobs are added directly via function references (since setup functions are in code), so static analysis **will** catch their modules. The **current solution’s `scheduler_trace.py`** explicitly parses the source for `scheduler.add_job(... )` calls ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=def%20visit_Call%28self%2C%20node%29%3A%20,unparse%28arg)). It logs the file and line where each job is added and the target callable (by name) ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=if%20isinstance%28node,self.filepath)). This is a safety net to ensure we note which modules contain scheduled tasks. It helps identify, for instance, that `services/domain_scheduler.py` has job registrations (implying it’s used as part of the app’s lifecycle even if not imported elsewhere except in `main.py`).

- **Runtime-only Imports:** Some modules might only be imported at runtime due to certain triggers (e.g. a code path on a specific API request or a scheduled job execution). The current approach uses a **runtime import logger** to catch these. By patching Python’s import mechanism (overriding `builtins.__import__`), it logs every module name loaded during a test run or application startup ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=def%20setup_runtime_logger%28log_file%3D%27reports%2Fruntime_imports.log%27%29%3A%20,import__%20logged%20%3D%20set)). This is a clever way to see the actual imports in practice. One limitation is that it only logs the first time a module is imported (to avoid repeats) ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=def%20_logged_import,name%2C%20globals%2C%20locals%2C%20fromlist%2C%20level)). If a module was imported during startup, subsequent conditional imports of it won’t show up because it’s already in `sys.modules`. The logger as given logs module _names_ (which might be packages, not file paths) ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=def%20_logged_import,name%2C%20globals%2C%20locals%2C%20fromlist%2C%20level)), so translating those to file paths needs cross-reference with the project structure. Another practical gap: if certain jobs or code paths weren’t executed during the monitored run, their imports won’t appear. For example, if a scheduled job runs every hour and our test only starts the scheduler but doesn’t wait an hour, any imports happening inside the job function wouldn’t be caught by the logger. Thus, **runtime tracing must be comprehensive** (run the app, trigger scheduled jobs, exercise CLI commands, etc., in a safe environment) to catch all used modules. Integrating this with a test suite (e.g. via a Pytest fixture to enable the import logger) is one approach ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=Usage%20example%20in%20%27tests%2Fconftest)).

In summary, the current approach covers a lot: static AST scanning, explicit scheduler job tracing, and runtime logging ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=This%20guide%20helps%20you%20replace,with%20enhanced%20tools%20that%20detect)). The main gaps to address are to ensure **no transitive import is missed** (where modern tools help), and to automate combining these sources. We should also handle special cases like package `__init__.py` files (which are implicitly imported when any module in the package is used) and avoid false positives from purely dynamic patterns.

## Proposed Improved Solution

To achieve reliable and maintainable module tracing, we recommend a **pipeline that leverages modern static analysis and targeted runtime verification**:

1. **Static Import Graph with Modern Tools:** Use a tool like **pydeps or ModuleGraph** to generate the set of all modules reachable from each entry point. For example:

   - For the FastAPI app: run `pydeps` (as a library or CLI) on `main.py` with options to output a list of imported modules (pydeps can output a JSON or text list of imports instead of a graph) ([pydeps · PyPI](https://pypi.org/project/pydeps/#:~:text=pydeps%20finds%20imports%20by%20looking,if)). If using ModuleGraph via Python, instantiate it on `main.py` and traverse its graph to collect module file paths.
   - Repeat for each script in `scripts/`. Each script should be treated as a separate root, since they might import things that the main app does not.
   - Combine all these modules into a set of **statically discovered modules**. This will be our `used_files_static.json` (similar to the current `used_files.json` report).

   Modern tools will automatically handle nested imports, so if script A imports module B which imports C, all B and C end up in the set ([modulegraph · PyPI](https://pypi.org/project/modulegraph/#:~:text=modulegraph%20determines%20a%20dependency%20graph,bytecode%20analysis%20for%20import%20statements)). They also respect Python import semantics (including relative imports, `__init__.py`, etc.), reducing manual effort.

2. **APScheduler Job Trace (Static):** Retain a lightweight AST scan specifically for APScheduler job registrations (or improve it by introspection). The existing `scheduler_trace.py` logic is appropriate: it parses all `.py` files looking for `scheduler.add_job(...)` calls ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=def%20visit_Call%28self%2C%20node%29%3A%20,unparse%28arg)). This yields a list of locations and target names ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=if%20isinstance%28arg%2C%20ast,target)). We can use this in two ways:

   - Ensure that any module containing a job registration is marked as used (since the app will import that module to register the job). In practice, if `main.py` calls `setup_domain_scheduler()` and that in turn calls `scheduler.add_job`, the module `services/domain_scheduler.py` is already in static imports (it’s imported by main). So this may not add _new_ files beyond static imports, but it’s a good validation.
   - More importantly, we get the **function names** that are scheduled (`target` in the JSON). With those, we can double-check that the modules containing those functions are included. If a job function is defined in a different module and referenced by name, we’d catch it. (In our design, functions are likely defined in the same module or imported normally, so static analysis covers them. But the trace ensures no job is overlooked.)

   _Alternative/Improvement:_ If we actually run the app initialization, we could introspect `scheduler.get_jobs()` afterwards to see job references. Each job has a `func` or `func_ref` attribute which we could inspect to get the function object and its `__module__`. This would tell us exactly which module the job will execute in. This is a runtime check that could augment the AST approach. However, it requires running the scheduler startup in a controlled way. Given that static scanning is simpler and our patterns are straightforward, sticking with the AST trace (which is fast and side-effect free) is acceptable.

3. **Dynamic Import Pattern Scan (Static):** Use an AST-based **dynamic import detector** (like the provided `dynamic_imports.py`) to flag any uses of `importlib.import_module(...)` or `__import__` in the code. This doesn’t directly give us new “used modules”, but it highlights where manual attention might be needed. If such patterns exist, we review them:

   - If the dynamic import is importing something within our project (e.g., `import_module("services." + name)`), we might decide to include all possible targets or restructure the code to avoid truly dynamic imports.
   - In many cases, dynamic imports might be for plugin systems or optional features. Given the note that dynamic imports were “ruled out” in this project, we expect few or none. But this step ensures confidence that we aren’t missing anything. Any modules discovered here could be added to the used set if we determine they are indeed imported at runtime.

4. **Runtime Import Logging (Validation):** Integrate a runtime logging step to **validate the static set and catch surprises**. The idea is to run the application (and CLI scripts, and tests) in an instrumented mode:

   - Activate the `runtime_import_logger.setup_runtime_logger` at the very start of execution (e.g., via a Pytest fixture or by prepending an import hook) ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=def%20setup_runtime_logger%28log_file%3D%27reports%2Fruntime_imports.log%27%29%3A%20,import__%20logged%20%3D%20set)) ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=Usage%20example%20in%20%27tests%2Fconftest)). Then launch the FastAPI app (perhaps through the test suite or uvicorn in a test mode) and execute each CLI script in a dry-run fashion (if possible, with environment variables to prevent real side-effects).
   - Collect the log of modules imported. This will give a list of module names (e.g., `src.routers.domains`, `src.services.domain_service`, etc.). We map these names to file paths in our project.
   - Compare this runtime-loaded set to the static set. Modules present at runtime but not predicted statically would indicate a gap in static analysis (which is rare if static is comprehensive, but possible if some import happens via a truly dynamic mechanism or C extension, etc.). Conversely, any module in the static set not seen at runtime might be because that code path wasn’t executed in this run (not necessarily unused in all contexts, just not exercised).

   In practice, we might see near-complete overlap. The runtime log serves as a **“validation subset”** – those are definitely used (let’s call them _validated used_). The static-only ones are _potentially used_ (they might require specific conditions to load). We will treat both as used for our final report, but we could flag which ones lack runtime confirmation.

5. **Aggregation and Reporting:** Finally, merge the results into a coherent report:

   - Take the union of **all statically discovered files** (from step 1) with any additional files from scheduler trace and dynamic patterns (steps 2 and 3) and the runtime import list (step 4) ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=combined%20%3D%20static)). This union is the set of all modules considered _used or potentially used_. We can sort this list and call it, for instance, `all_used_modules.json`.
   - Determine the full set of Python files in the project (e.g., by scanning the `src/` directory) ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=for%20root%2C%20_%2C%20files%20in,join%28root%2C%20f)). Exclude test files or any irrelevant directories as needed.
   - Compute the set difference: all files minus the used union = **unused candidate files** ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=for%20root%2C%20_%2C%20files%20in,join%28root%2C%20f)). These are the files that static analysis could not reach and we never saw loaded at runtime. It’s wise to manually review this list before deletion, but it should largely be the truly orphaned modules.
   - For a more detailed report, we can categorize modules:
     - **Validated Used:** modules confirmed via runtime logging.
     - **Statically Used (Not Observed at Runtime):** modules that static analysis marked as reachable, but our runtime run didn’t load. (These might correspond to jobs that didn’t trigger, error handlers not invoked, etc.)
     - **Unused:** modules not referenced by any other module (thus not in the static graph).

   This can be output as a JSON with fields, or a table. For example, we might produce a JSON structure like:

   ```json
   {
     "used": [
       { "file": "src/main.py", "validated": true },
       { "file": "src/routers/domains.py", "validated": true },
       { "file": "src/services/legacy_feature.py", "validated": false }
     ],
     "unused": ["src/old_module.py", "src/deprecated/utils.py"]
   }
   ```

   In the above, `legacy_feature.py` was considered used (imported somewhere statically) but not hit at runtime – maybe an optional feature. This kind of structured list helps decide what to do with each category. In simpler form, we can just list used vs unused files in two lists or tables.

   Additionally, we can include in the report the _source of usage_ for each used module (e.g., which entry point or parent module causes it to load). This could be derived from the import graph. For instance, a table might show:

   | Module Path                        | Usage Source                                                                                                               |
   | ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |
   | `src/main.py`                      | Entry point (FastAPI)                                                                                                      |
   | `src/services/domain_scheduler.py` | Imported in `main.py` ([main.py](file://file-58zKNV7SpSbKh7GftzpfKo#:~:text=,instance%20start_scheduler)) (registers jobs) |
   | `src/services/domain_service.py`   | Imported in `domain_scheduler.py` (job function)                                                                           |
   | `src/scripts/some_utility.py`      | Entry point (CLI script)                                                                                                   |
   | `src/unreferenced/old_script.py`   | _No references_ (unused)                                                                                                   |

   Such detail is optional but can be helpful during cleanup to justify why something is considered unused.

## Automation Pipeline (Wrapper Script)

To streamline this process, a **wrapper script** can orchestrate the steps (except the external app execution):

- The script can call the static analysis library functions directly. For example, using ModuleGraph:

  ```python
  import modulegraph
  mg = modulegraph.ModuleGraph()
  mg.add_script('src/main.py')
  for script in Path('scripts').glob('*.py'):
      mg.add_script(str(script))
  mg.run_script()  # build the graph
  used_files = {mod.filename for mod in mg.iter_graph() if mod.filename and mod.filename.startswith(project_path)}
  json.dump(sorted(used_files), open('reports/used_files_static.json','w'), indent=2)
  ```

  (The actual API of ModuleGraph may differ, but the idea is to programmatically collect `used_files`.)

- Next, call the scheduler AST scan (as in `scheduler_trace.py`) and dynamic import scan, writing their JSON outputs. These can be invoked via `subprocess` or integrated as functions.

- For runtime imports, the wrapper might not directly execute the app (to avoid test environment complexities), but it can issue a reminder or instructions: e.g., _“Run `pytest --import-mode=importlib` to generate runtime_imports.log, then continue.”_ If automation is required, the wrapper could spawn a subprocess running the app with an environment variable to trigger the logging (for example, running `python -c "import tools.runtime_import_logger; tools.runtime_import_logger.setup_runtime_logger(); import src.main"`). In either case, once the log is produced, the wrapper reads it and converts to a set of module file paths.

- Finally, the wrapper merges the sets and produces the final JSON reports for used and unused modules, as described. This could be one command or script that a developer runs after writing new code, to update the dependency listings.

By automating these stages, we ensure consistency. The developer’s role becomes simply to run the tool, then review the **unused modules list** and remove or double-check those files. The pipeline can be integrated into CI to prevent introducing truly orphaned code.

## Conclusion

Using a combination of **modern static analysis** (to map out all import relationships) and **targeted runtime tracing** (to validate actual usage), we can accurately trace all Python modules used by the FastAPI application, background scheduler jobs, and CLI scripts. Tools like _ModuleGraph/pydeps_ provide a reliable backbone for static reachability analysis, covering transitive imports and Python 3 syntax seamlessly ([modulegraph · PyPI](https://pypi.org/project/modulegraph/#:~:text=modulegraph%20determines%20a%20dependency%20graph,bytecode%20analysis%20for%20import%20statements)). Augmenting this with APScheduler-specific scanning and runtime import logging covers edge cases like scheduled tasks and dynamic imports ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=This%20guide%20helps%20you%20replace,with%20enhanced%20tools%20that%20detect)). The result is a comprehensive view of the codebase: we can confidently distinguish between modules that are truly in use and those that are unreferenced. By automating the tracing pipeline end-to-end, the team can generate up-to-date reports of used vs. unused modules with minimal effort. This ensures that as the project evolves, **dead code is identified and removed**, improving maintainability without risking the removal of anything still needed. The focus on accurate module reachability across both request-driven and scheduled execution paths guarantees that all active parts of the system are accounted for – no matter how or when they are invoked.

**Sources:**

- Neil G. _“Module dependency graph in Python 3”_ — Stack Overflow ([Module dependency graph in Python 3 - Stack Overflow](https://stackoverflow.com/questions/23962614/module-dependency-graph-in-python-3#:~:text=I%20presume%20you%20are%20talking,equally%20well%20with%20any%20Python))

- Koby Bass. _“Finding Unused Python Files with Snakefood”_ — Medium/Gist ([Finding Unused Python Files · GitHub](https://gist.github.com/kobybum/a162be138b2d19e3eac20547a7f10fa3#:~:text=Generate%20a%20list%20of%20included,project%20%3E%20%2Ftmp%2Fout.deps))

- Ronald Oussoren. _modulegraph documentation_ — PyPI (2025) ([modulegraph · PyPI](https://pypi.org/project/modulegraph/#:~:text=modulegraph%20determines%20a%20dependency%20graph,bytecode%20analysis%20for%20import%20statements))

- PyDeps Documentation — PyPI (2025) ([pydeps · PyPI](https://pypi.org/project/pydeps/#:~:text=pydeps%20finds%20imports%20by%20looking,if))

- **Project’s Static Analysis Improvement Guide** (user-provided) ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=This%20guide%20helps%20you%20replace,with%20enhanced%20tools%20that%20detect)) ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=def%20setup_runtime_logger%28log_file%3D%27reports%2Fruntime_imports.log%27%29%3A%20,import__%20logged%20%3D%20set)) ([41.10-Static Analysis Tool Improvement Guide.md](file://file-RamgK64ZkwtXZo5WphQms9#:~:text=combined%20%3D%20static))
