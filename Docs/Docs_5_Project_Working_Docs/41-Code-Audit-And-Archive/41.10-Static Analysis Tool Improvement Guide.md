# Static Analysis Tool Improvement Guide

## Overview

This guide helps you replace the deprecated static analysis scripts with enhanced tools that detect:

- APScheduler job registrations
- Dynamic import patterns
- Runtime import usage
- A combined report of used vs. unused modules

Follow these steps in order, and adapt file paths as needed for your project.

---

## 1. Rename Deprecated Tools

Before adding new scripts, suffix the old ones with `_deprecated`:

```bash
mv tools/analyze_imports_ast.py tools/analyze_imports_ast_deprecated.py
mv tools/analyze_imports_ast_fixed.py tools/analyze_imports_ast_fixed_deprecated.py
mv tools/compare_used_unused.py tools/compare_used_unused_deprecated.py
```

---

## 2. Add New Analysis Tools

Create the following files under `tools/`:

### 2.1 `tools/scheduler_trace.py`

```python
import ast
import os
import json

class SchedulerVisitor(ast.NodeVisitor):
    """Collects all scheduler.add_job(...) calls."""
    def __init__(self, filepath):
        self.filepath = filepath
        self.jobs = []

    def visit_Call(self, node):
        # Detect scheduler.add_job(...) calls
        if isinstance(node.func, ast.Attribute) and node.func.attr == "add_job":
            target = None
            if node.args:
                arg = node.args[0]
                if isinstance(arg, ast.Name):
                    target = arg.id
                elif isinstance(arg, ast.Attribute):
                    target = ast.unparse(arg)
            self.jobs.append({
                "file": self.filepath,
                "lineno": node.lineno,
                "target": target
            })
        self.generic_visit(node)


def parse_scheduler_jobs(src_dir, out_file):
    jobs = []
    for root, _, files in os.walk(src_dir):
        for fname in files:
            if not fname.endswith('.py'):
                continue
            path = os.path.join(root, fname)
            with open(path, 'r') as f:
                tree = ast.parse(f.read(), path)
            visitor = SchedulerVisitor(path)
            visitor.visit(tree)
            jobs.extend(visitor.jobs)
    with open(out_file, 'w') as f:
        json.dump(jobs, f, indent=2)


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='Trace APScheduler jobs')
    parser.add_argument('--src', default='src/', help='Source directory')
    parser.add_argument('--out', default='reports/scheduler_jobs.json', help='Output JSON file')
    args = parser.parse_args()
    parse_scheduler_jobs(args.src, args.out)
```

### 2.2 `tools/dynamic_imports.py`

```python
import ast
import os
import json

class DynamicImportVisitor(ast.NodeVisitor):
    """Detects dynamic import patterns for human review."""
    def __init__(self, filepath):
        self.filepath = filepath
        self.entries = []

    def visit_Call(self, node):
        func = node.func
        if isinstance(func, ast.Attribute) and func.attr == 'import_module':
            if node.args:
                mod = ast.unparse(node.args[0])
                self.entries.append({'file': self.filepath, 'lineno': node.lineno, 'module': mod})
        elif isinstance(func, ast.Name) and func.id == '__import__':
            if node.args:
                mod = ast.unparse(node.args[0])
                self.entries.append({'file': self.filepath, 'lineno': node.lineno, 'module': mod})
        self.generic_visit(node)


def detect_dynamic_imports(src_dir, out_file):
    results = []
    for root, _, files in os.walk(src_dir):
        for fname in files:
            if not fname.endswith('.py'):
                continue
            path = os.path.join(root, fname)
            tree = ast.parse(open(path).read(), path)
            vis = DynamicImportVisitor(path)
            vis.visit(tree)
            results.extend(vis.entries)
    with open(out_file, 'w') as f:
        json.dump(results, f, indent=2)


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='Detect dynamic imports')
    parser.add_argument('--src', default='src/', help='Source directory')
    parser.add_argument('--out', default='reports/dynamic_imports.json', help='Output JSON file')
    args = parser.parse_args()
    detect_dynamic_imports(args.src, args.out)
```

### 2.3 `tools/runtime_import_logger.py`

```python
import builtins

def setup_runtime_logger(log_file='reports/runtime_imports.log'):
    """Overrides __import__ to log modules as they load."""
    original_import = builtins.__import__
    logged = set()

    def _logged_import(name, globals=None, locals=None, fromlist=(), level=0):
        if name not in logged:
            logged.add(name)
            with open(log_file, 'a') as f:
                f.write(f"{name}
")
        return original_import(name, globals, locals, fromlist, level)

    builtins.__import__ = _logged_import

# Usage example in 'tests/conftest.py':
#
# import pytest
# from tools.runtime_import_logger import setup_runtime_logger
#
# @pytest.fixture(autouse=True)
# def import_logger(tmp_path):
#     setup_runtime_logger(log_file=str(tmp_path / 'runtime_imports.log'))
```

### 2.4 `tools/combined_trace.py`

```python
import json
import os

def load_json(path):
    with open(path) as f:
        return json.load(f)

if __name__ == '__main__':
    static = set(load_json('reports/used_files.json'))
    scheduler = set(entry['file'] for entry in load_json('reports/scheduler_jobs.json'))
    dynamic = set(entry['file'] for entry in load_json('reports/dynamic_imports.json'))
    runtime = set(open('reports/runtime_imports.log').read().splitlines())

    combined = static.union(scheduler).union(dynamic).union(runtime)

    all_py = set()
    for root, _, files in os.walk('src/'):
        for f in files:
            if f.endswith('.py'):
                all_py.add(os.path.join(root, f))

    unused = sorted(all_py - combined)
    with open('reports/unused_candidates_combined.json', 'w') as out:
        json.dump(unused, out, indent=2)
```

---

## 3. Usage

1. **Generate scheduler report**
   ```bash
   python tools/scheduler_trace.py --src=src/ --out=reports/scheduler_jobs.json
   ```
2. **Detect dynamic imports**
   ```bash
   python tools/dynamic_imports.py --src=src/ --out=reports/dynamic_imports.json
   ```
3. **Collect runtime imports**
   - Ensure `runtime_import_logger` is enabled in your test suite or at server startup.
   - Run your test suite or start the app to produce `reports/runtime_imports.log`.
4. **Combine all traces**
   ```bash
   python tools/combined_trace.py
   ```
5. **Review results**
   - Open `reports/unused_candidates_combined.json`.
   - Compare against your critical registry and existing used lists.

---

## 4. Next Steps

- Commit and push these new tools.
- Update CI to run the combined trace and fail on unexpected unused files.
- Create a PR listing any high‚Äêconfidence unused modules for human review.

"""
Combine static AST imports, scheduler jobs, dynamic imports, and runtime logs into one report.
"""
import json

def load_json(path):
with open(path) as f:
return json.load(f)

if **name** == '**main**':
static = set(load_json('reports/used_files.json'))
scheduler = set(entry['file'] for entry in load_json('reports/scheduler_jobs.json'))
dynamic = set(entry['file'] for entry in load_json('reports/dynamic_imports.json'))
runtime = set(open('reports/runtime_imports.log').read().splitlines())

    combined = static.union(scheduler).union(dynamic).union(runtime)
    all_py = set()  # scan src/ for all .py files

    import os
    for root, _, files in os.walk('src/'):
        for f in files:
            if f.endswith('.py'):
                all_py.add(os.path.join(root, f))

    unused = sorted(all_py - combined)
    with open('reports/unused_candidates_combined.json', 'w') as out:
        json.dump(unused, out, indent=2)
