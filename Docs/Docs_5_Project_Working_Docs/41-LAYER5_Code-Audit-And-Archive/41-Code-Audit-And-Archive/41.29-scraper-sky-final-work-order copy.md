# ScraperSky Code Audit: Final Work Order

## 1. Background & Project History

This work order documents the final phase of the ScraperSky Backend code audit project, which has been ongoing for several weeks. The primary goal is to identify and safely archive unused Python code in the `src/` directory to improve maintainability.

### Previous Approaches

The project has gone through multiple methodological iterations:

1. **Initial Static Analysis** (41.0-41.5)
   - Attempted using `trace_imports.py` with `importlab` and other static analysis tools
   - Failed due to dependency issues and inability to handle dynamic imports

2. **Hybrid AST+grep Analysis** (41.6-41.12)
   - Developed custom AST parsing and grep-based solutions
   - Insufficient for capturing FastAPI's dynamic routing and dependencies

3. **Manual Dependency Tracing** (41.13-41.26)
   - Manually traced imports from `main.py`
   - Conducted targeted static analysis for specific components
   - Created a functional dependency map
   - Still missed dynamic relationships

4. **Initial Archiving Attempt** (41.25)
   - Identified 18 potentially unused files
   - Archiving them caused application failures
   - Had to restore 11-13 files to maintain functionality

### Current State

We have discovered that the most reliable approach is **runtime tracing** - directly monitoring which modules and files are loaded during application execution. The project currently has:

1. **Existing Runtime Tracer**: There's a functioning runtime tracer in `src/config/runtime_tracer.py`
2. **New Analysis Tools**: Scripts to exercise endpoints and analyze results
3. **Enhanced Tracer**: We've just enhanced the tracer to track both file paths and module names

## 2. Why This New Approach Will Work

Previous approaches failed because they all tried to approximate what happens at runtime through static analysis. The runtime tracing approach:

1. **Captures Ground Truth**: Records exactly which modules load during execution
2. **Handles Dynamic Patterns**: Catches all types of imports (standard, dynamic, lazy, reflection)
3. **Integrates with Existing Code**: Leverages the already-functioning tracer
4. **Full Coverage**: Exercises all endpoints and background tasks

This approach follows the principle: "The only way to know for sure what code runs is to run the code."

## 3. Final Work Plan

### Step 1: Update Main.py for Module Logging

```
Cursor Command: Update src/main.py to log loaded modules

1. First, update the import statement to include the new get_loaded_modules function:
   from src.config.runtime_tracer import start_tracing, stop_tracing, get_loaded_files, get_loaded_modules

2. In the lifespan function, where it currently logs loaded files at shutdown:
   - After the existing code that logs loaded files, add:

   # Log loaded modules
   modules = get_loaded_modules()
   logger.info(f"Total unique modules loaded: {len(modules)}")
   for module_path in sorted(list(modules)):
       logger.info(f"LOADED_MODULE: {module_path}")

3. Add a new debug endpoint to see modules:
   @app.get("/debug/loaded-modules", tags=["Debug"], response_model=list)
   async def get_loaded_modules_endpoint():
       """Returns the list of unique modules loaded during runtime."""
       modules = get_loaded_modules()
       return sorted(list(modules))
```

### Step 2: Update Analyzer Script

```
Cursor Command: Update tools/analyze_runtime_results.py to handle module tracking

1. Add a function to read loaded modules:
   def read_loaded_modules(file_path="reports/runtime_loaded_modules.txt"):
       """Read the list of loaded modules from the runtime trace"""
       if not os.path.exists(file_path):
           print(f"Error: {file_path} does not exist!")
           return []

       with open(file_path, 'r') as f:
           return [line.strip() for line in f if line.strip()]

2. Add a function to convert modules to file paths:
   def convert_modules_to_files(modules, base_dir="src"):
       """Convert module names to file paths"""
       files = []
       for module in modules:
           if module.startswith('src.'):
               # Convert module.name.submodule to src/module/name/submodule.py
               path = os.path.join(os.path.abspath(base_dir), *module.split('.')[1:]) + '.py'
               files.append(path)
       return files

3. Add module-based analysis to the analyze_results function:
   # After getting loaded_files
   loaded_modules = read_loaded_modules(module_results_file)
   print(f"Found {len(loaded_modules)} loaded modules in the runtime trace")

   # Convert modules to files
   module_files = convert_modules_to_files(loaded_modules)
   print(f"Converted to {len(module_files)} file paths")

   # Combine with direct file paths
   all_loaded_files = set(loaded_files + module_files)
   print(f"Total unique loaded files: {len(all_loaded_files)}")

   # Find unused files
   unused_files = [f for f in all_files if f not in all_loaded_files]

4. Update the main function to include the module results parameter:
   parser.add_argument("--module-results", default="reports/runtime_loaded_modules.txt",
                     help="Runtime module tracing results file")

   # And pass it to analyze_results:
   analyze_results(args.src, args.results, args.module_results, args.output)
```

### Step 3: Update Exercise Endpoints Script

```
Cursor Command: Update tools/exercise_endpoints.py to extract logs for analysis

1. Add a function to extract log files:
   def extract_log_files(docker_container="scraper-sky-api", output_dir="reports"):
       """Extract log files from Docker container"""
       try:
           import subprocess
           import re

           # Ensure output directory exists
           os.makedirs(output_dir, exist_ok=True)

           # Get logs
           result = subprocess.run(
               ["docker", "logs", docker_container],
               capture_output=True, text=True, check=True
           )

           # Extract loaded files
           files = []
           modules = []

           for line in result.stdout.splitlines():
               if "LOADED_SRC_FILE:" in line:
                   match = re.search(r'LOADED_SRC_FILE: (.+)$', line)
                   if match:
                       files.append(match.group(1))
               elif "LOADED_MODULE:" in line:
                   match = re.search(r'LOADED_MODULE: (.+)$', line)
                   if match:
                       modules.append(match.group(1))

           # Write files to output
           with open(f"{output_dir}/runtime_loaded_files.txt", 'w') as f:
               for file_path in sorted(files):
                   f.write(f"{file_path}\n")

           # Write modules to output
           with open(f"{output_dir}/runtime_loaded_modules.txt", 'w') as f:
               for module in sorted(modules):
                   f.write(f"{module}\n")

           print(f"Extracted {len(files)} files and {len(modules)} modules to {output_dir}/")

       except Exception as e:
           print(f"Error extracting logs: {str(e)}")

2. Call this function in main:
   # After waiting for scheduled tasks
   print("Extracting logs...")
   extract_log_files()
```

### Step 4: Update Master Script

```
Cursor Command: Update run_runtime_analysis.sh to handle new workflow

1. Modify the script to handle Docker:
   #!/bin/bash
   set -e

   echo "===== RUNTIME CODE USAGE ANALYSIS ====="

   # Set container name
   CONTAINER_NAME="scraper-sky-api"

   echo "Restarting application container..."
   docker restart $CONTAINER_NAME

   # Wait for the app to start
   echo "Waiting for app to start..."
   sleep 10

   # Run the test script
   echo "Exercising endpoints..."
   python tools/exercise_endpoints.py --wait 60

   # Stop and restart container to ensure logs are written
   echo "Restarting container to flush logs..."
   docker restart $CONTAINER_NAME
   sleep 5

   # Extract logs and analyze
   echo "Extracting logs and analyzing results..."
   python tools/analyze_runtime_results.py

   echo "===== ANALYSIS COMPLETE ====="
   echo "Check reports/unused_files.json for the list of unused files"
```

## 4. Execution Instructions

1. **Modify the Code**:
   - Use Cursor to make the changes specified in Steps 1-4
   - Save all modified files

2. **Run the Analysis**:
   ```bash
   chmod +x run_runtime_analysis.sh
   ./run_runtime_analysis.sh
   ```

3. **Review Results**:
   - Open `reports/unused_files.json` to see unused files
   - Compare with previous lists (in `reports/` or `project-docs/`)
   - Check log files in `reports/` for details

4. **Verify Findings**:
   - For a few unused files, move them to a temp directory
   - Restart the application to confirm it still works
   - If issues occur, immediately move them back

5. **Create Final Archive**:
   - Create directory: `mkdir -p _Archive_$(date +%Y.%m.%d)/src/`
   - Move verified unused files: `mv [file] _Archive_$(date +%Y.%m.%d)/src/`
   - Update the manifest: `project-docs/41-Code-Audit-And-Archive/final_archived_files.md`

## 5. Expected Outcome

Upon successful completion:

1. **Clear List of Unused Files**:
   - A definitive JSON file (`reports/unused_files.json`) listing all unused Python files
   - High confidence in results due to runtime verification

2. **Verified Archive**:
   - All identified unused files safely moved to archive directory
   - Application still functions properly
   - Manifest documenting what was archived and why

3. **Documentation**:
   - Updated project documentation capturing the process and findings
   - Clear process for future code audits

## 6. Troubleshooting

If any issues arise:

1. **Application Failure After Archiving**:
   - Immediately restore archived files: `mv _Archive_*/src/* src/`
   - Check logs for specific import errors

2. **Empty or Missing Results**:
   - Verify the application started properly
   - Check Docker logs to ensure tracer was activated
   - Inspect network calls to confirm endpoints were exercised

3. **Unexpected Results**:
   - Run a simple test: add `print("LOADED")` to a few files that should definitely be used
   - Verify these show up in the trace; if not, the tracer may not be working correctly

## 7. Final Notes

This runtime-based approach provides the highest confidence level for identifying truly unused code. It does have one limitation: code only executed under specific conditions (like error handlers) might be marked as unused if those conditions don't occur during testing. Use judgment when reviewing results.

Document all decisions and maintain the archive with clear records. This will make future maintenance much easier and provide an audit trail for the cleanup process.
