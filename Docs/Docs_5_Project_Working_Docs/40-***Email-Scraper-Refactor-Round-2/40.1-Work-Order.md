# Work Order: Refactor Email Scraping Initiation to Standard Scheduler Pattern

**Version:** 1.0
**Date:** 2025-04-19
**Status:** Open
**Assignee:** TBD
**Related Files:**

- `src/routers/email_scanner.py`
- `src/tasks/email_scraper.py`
- `src/models/job.py`
- `src/models/task_status.py`
- `src/schedulers/` (Potential new file: `email_scan_scheduler.py`)
- `docker-compose.yml`
- `.env.example`
- `project-docs/39-Background-Task-Architecture-Audit-&-Standardization-Plan/39.1-Audit-Report.md` (Expected Output)
- `Docs/Docs_1_AI_GUIDES/21-SCHEDULED_TASKS_APSCHEDULER_PATTERN.md`
- `Docs/Docs_1_AI_GUIDES/24-SHARED_SCHEDULER_INTEGRATION_GUIDE.md`

## 1. Objective

Refactor the email scanning initiation process (`ContactLaunchpad` workflow) to strictly adhere to the project's standard background task architecture. This involves modifying the API endpoint to only create a job record and implementing a dedicated scheduler component to poll for and execute pending email scan jobs.

## 2. Background & Context

The initial implementation of the email scan API endpoint (`POST /api/v3/email-scan/scan/website` in `src/routers/email_scanner.py`) directly queues the scraping task (`scan_website_for_emails` from `src/tasks/email_scraper.py`) using FastAPI's `BackgroundTasks`.

While functional, this deviates from the established project standard where API endpoints create/update database records (e.g., setting a `Job` status to `PENDING`), and separate APScheduler-based processes poll these records to trigger the actual background work. This standard pattern promotes better separation of concerns, robustness, and aligns with how other background tasks (like sitemap processing) are managed ([Guide 21](Docs/Docs_1_AI_GUIDES/21-SCHEDULED_TASKS_APSCHEDULER_PATTERN.md), [Guide 24](Docs/Docs_1_AI_GUIDES/24-SHARED_SCHEDULER_INTEGRATION_GUIDE.md)). The need for this alignment was identified during the background task audit (`project-docs/39-*`).

This work order details the steps to bring the email scanning workflow into compliance.

## 3. Requirements & Implementation Steps

1.  **Modify API Endpoint (`src/routers/email_scanner.py::scan_website_for_emails_api`):**

    - **Remove Direct Task Queuing:** Delete the line `background_tasks.add_task(scan_website_for_emails, job_id=new_job_id, user_id=user_id)`.
    - **Ensure Job Creation:** Verify that the endpoint correctly creates a `Job` record in the database with `job_type='email_scan'` and `status='PENDING'` (using the `TaskStatus` enum) upon successful request validation and prerequisite checks (like checking for existing jobs).
    - **Response:** The endpoint should still return the `JobSubmissionResponse` with the `job_id` and a `202 Accepted` status code.

2.  **Implement Email Scan Scheduler:**

    - **Create Scheduler File (Recommended):** Create a new file, e.g., `src/schedulers/email_scan_scheduler.py`.
    - **Define Scheduler Function:** Implement an `async` function (e.g., `process_pending_email_scans`) that:
      - Queries the `Job` table for records where `job_type == 'email_scan'` and `status == TaskStatus.PENDING.value`, limiting results by a configurable batch size (`EMAIL_SCAN_SCHEDULER_BATCH_SIZE`).
      - Iterates through the found pending jobs.
      - For each job:
        - **Update Status to Running:** Immediately update the job's status to `TaskStatus.RUNNING.value` in the database within its own transaction to prevent duplicate processing by concurrent scheduler runs.
        - **Execute Scraping Task:** Call the actual scraping function `src.tasks.email_scraper.scan_website_for_emails`, passing the necessary `job_id` and `user_id`.
        - **Error Handling:** Wrap the call to `scan_website_for_emails` in a try/except block. If the task function raises an exception, update the job status to `TaskStatus.FAILED.value` and log the error. _Note: The `scan_website_for_emails` task itself should handle its internal logic, session management, and final status updates (`complete`/`failed`) upon completion or failure._
    - **Register Scheduler Job:** In `src/main.py` (or wherever APScheduler is configured), register this new function to run periodically based on `EMAIL_SCAN_SCHEDULER_INTERVAL_MINUTES`, ensuring appropriate concurrency control (e.g., `max_instances=1`).

3.  **Verify Task Function (`src/tasks/email_scraper.py::scan_website_for_emails`):**

    - Ensure this function correctly handles its own database session acquisition (using `get_background_session`) and transaction management.
    - Confirm it updates the corresponding `Job` record's status to `TaskStatus.COMPLETE.value` or `TaskStatus.FAILED.value` upon completion or failure.

4.  **Update Configuration:**
    - Add `EMAIL_SCAN_SCHEDULER_INTERVAL_MINUTES` and `EMAIL_SCAN_SCHEDULER_BATCH_SIZE` environment variables to `.env.example` with reasonable defaults (e.g., 1 minute, 10 jobs).
    - Add these variables to the `environment` section of the `scrapersky` service in `docker-compose.yml`.

## 4. Acceptance Criteria

- Calling `POST /api/v3/email-scan/scan/website` successfully creates a `Job` record with `status='PENDING'` and returns a `202 Accepted` response quickly, without initiating the scan immediately.
- The new email scan scheduler runs periodically as configured.
- Logs show the scheduler identifying pending email scan jobs, updating their status to `RUNNING`, and invoking the `scan_website_for_emails` task.
- The `scan_website_for_emails` task executes successfully in the background, performs the scraping, and updates the final job status (`COMPLETE` or `FAILED`) in the `Job` table.
- The system remains stable, and email scans are processed reliably via the scheduler mechanism.

## 5. Effort Estimation (Preliminary)

- **Development:** ~2-4 hours (modifying API, creating scheduler, registering job, updating config)
- **Testing:** ~1-2 hours (unit tests for scheduler logic if applicable, integration testing of the end-to-end flow)
- **Total:** ~3-6 hours

_(Estimation assumes familiarity with the codebase structure, APScheduler, and SQLAlchemy patterns used in the project.)_
