# SITEMAP API ENDPOINT JOB CREATION FIX WORK ORDER

**Document ID:** 13.8-SITEMAP_API_JOB_CREATION_FIX
**Date:** 2025-04-01
**Status:** Open
**Priority:** Critical
**Related Documents:**

- 07-69-SITEMAP-SCANNER-BACKGROUND-SERVICE-STANDARDIZATION-COMPLETION
- 13.7-SITEMAP_SCHEDULER_INTEGRATION_WORKORDER

## 1. Executive Summary

This work order addresses a critical issue in the sitemap scanning API endpoint. While the background processing functionality was previously standardized and tested successfully (per 07-69-SITEMAP-SCANNER-BACKGROUND-SERVICE-STANDARDIZATION-COMPLETION), the API endpoint itself is not properly creating job records in the database. The result is that domains submitted through the API are not being processed by the scheduler.

The issue requires a targeted fix to ensure job records are properly created in the database when a domain is submitted through the `/api/v3/sitemap/scan` endpoint.

## 2. Current State

1. **What Works:**

   - The sitemap processing functionality itself (`process_domain_with_own_session`) works correctly
   - Test scripts (`add_test_job.py`) can create job records that appear in the database
   - The scheduler can process jobs that exist in the database
   - Direct HTTP access to sitemaps works (verified with `test_sitemap.py`)

2. **What's Broken:**

   - The API endpoint `/api/v3/sitemap/scan` returns a job ID but fails to create an actual database record
   - Jobs submitted through the API don't appear in `debug_pending_jobs.py` output
   - The scheduler cannot find or process jobs submitted through the API

3. **Root Cause:**
   - The `scan_domain` function in `modernized_sitemap.py` is missing the job creation code
   - It creates in-memory tracking only but doesn't call `job_service.create()`

## 3. Required Changes

### 3.1. Primary File to Modify

- **File:** `src/routers/modernized_sitemap.py`
- **Function:** `scan_domain()`

### 3.2. Changes Required

Add job database record creation code to the `scan_domain` function. This must be done **before** the background task is added, within the existing transaction boundary.

### 3.3. Implementation Pattern

```python
# In scan_domain() function, inside the transaction boundary:

# After generating the job_id but before adding the background task:
from ..services.job_service import job_service
job_data = {
    "job_id": job_id,
    "job_type": "sitemap",
    "status": "pending",
    "created_by": current_user.get("id"),
    "result_data": {"domain": request.base_url, "max_pages": request.max_pages}
}
await job_service.create(session, job_data)
logger.info(f"Created database job record for domain: {request.base_url}, job_id: {job_id}")

# Continue with existing code for in-memory status and background task
```

## 4. Implementation Steps

1. **Add Import:** Ensure `job_service` is imported at the top of the file if not already present
2. **Add Job Creation:** Insert the job creation code at the appropriate location in the transaction
3. **Add Logging:** Include detailed logging for debugging purposes
4. **Test:** Verify the job appears in the database after submission

## 5. Testing Requirements

### 5.1. Test Approach

1. **Direct API Testing:**

   ```bash
   curl -X POST "http://localhost:8000/api/v3/sitemap/scan" \
     -H "Content-Type: application/json" \
     -d '{"base_url": "thecrackedbeanroastery.com", "max_pages": 100}'
   ```

2. **Database Verification:**

   ```bash
   cd scripts/sitemap_scheduler
   python debug_pending_jobs.py
   ```

3. **End-to-End Processing Testing:**

   ```bash
   # Trigger the scheduler to process the job
   python trigger_scheduler.py

   # Check job status after processing
   python check_job.py --latest
   ```

### 5.2. Verification Criteria

1. The job ID returned from the API must exist in the database
2. The job must have the correct status ("pending")
3. The result_data must contain the domain information
4. The scheduler must be able to find and process the job
5. The job status must update correctly during processing

## 6. Risks and Mitigation

1. **Risk: Duplicate Code Paths**

   - **Description:** The fix introduces code similar to what exists in test scripts
   - **Mitigation:** Carefully follow existing job creation patterns to maintain consistency

2. **Risk: Transaction Management**

   - **Description:** Adding database operations within existing transaction
   - **Mitigation:** Ensure code is added within existing transaction boundary, not creating a new one

3. **Risk: Regression**
   - **Description:** Breaking other job processing functionality
   - **Mitigation:** Targeted change to specific function, minimal impact on other components

## 7. Verification Checklist

- [ ] Job ID returned by API is found in database using `debug_pending_jobs.py`
- [ ] Job record contains correct domain information in result_data
- [ ] Job has "pending" status when created
- [ ] Scheduler can find and process the job
- [ ] Job updates to "complete" status after processing
- [ ] Sitemap data is stored in sitemap_files table

## 8. Implementation Notes

1. **DO NOT modify `process_domain_with_own_session`** - this function has been previously standardized and tested
2. **DO NOT change the existing transaction boundaries** - add code within the existing transaction
3. **FOLLOW the job creation pattern** from `add_test_job.py` - it's proven to work

## 9. Code Comparison

### Before (current state):

```python
async def scan_domain(
    request: SitemapScrapingRequest,
    background_tasks: BackgroundTasks,
    session: AsyncSession = Depends(get_session_dependency),
    current_user: dict = Depends(user_dependency)
):
    """Initiate a sitemap scan for a domain."""
    try:
        # Router owns the transaction boundary
        async with session.begin():
            # Generate a unique job ID using standard UUID format
            job_id = str(uuid.uuid4())

            logger.info(f"Initiating sitemap scan for domain: {request.base_url}, job_id: {job_id}")

            # Initialize the job in memory
            from ..services.sitemap.processing_service import _job_statuses
            _job_statuses[job_id] = {
                'status': 'pending',
                'created_at': datetime.utcnow().isoformat(),
                'domain': request.base_url,
                'progress': 0.0,
                'metadata': {'sitemaps': []}
            }

            # Add background task to process the domain
            from ..services.sitemap.processing_service import process_domain_with_own_session
            background_tasks.add_task(
                process_domain_with_own_session,
                job_id=job_id,
                domain=request.base_url,
                user_id=current_user.get("id"),
                max_urls=request.max_pages
            )

            # Return response with job details
            return SitemapScrapingResponse(
                job_id=job_id,
                status_url=f"/api/v3/sitemap/status/{job_id}"
            )
```

### After (with fix):

```python
async def scan_domain(
    request: SitemapScrapingRequest,
    background_tasks: BackgroundTasks,
    session: AsyncSession = Depends(get_session_dependency),
    current_user: dict = Depends(user_dependency)
):
    """Initiate a sitemap scan for a domain."""
    try:
        # Router owns the transaction boundary
        async with session.begin():
            # Generate a unique job ID using standard UUID format
            job_id = str(uuid.uuid4())

            logger.info(f"Initiating sitemap scan for domain: {request.base_url}, job_id: {job_id}")

            # Create database record for the job
            from ..services.job_service import job_service
            job_data = {
                "job_id": job_id,
                "job_type": "sitemap",
                "status": "pending",
                "created_by": current_user.get("id"),
                "result_data": {"domain": request.base_url, "max_pages": request.max_pages}
            }
            await job_service.create(session, job_data)
            logger.info(f"Created database job record for domain: {request.base_url}, job_id: {job_id}")

            # Initialize the job in memory
            from ..services.sitemap.processing_service import _job_statuses
            _job_statuses[job_id] = {
                'status': 'pending',
                'created_at': datetime.utcnow().isoformat(),
                'domain': request.base_url,
                'progress': 0.0,
                'metadata': {'sitemaps': []}
            }

            # Add background task to process the domain
            from ..services.sitemap.processing_service import process_domain_with_own_session
            background_tasks.add_task(
                process_domain_with_own_session,
                job_id=job_id,
                domain=request.base_url,
                user_id=current_user.get("id"),
                max_urls=request.max_pages
            )

            # Return response with job details
            return SitemapScrapingResponse(
                job_id=job_id,
                status_url=f"/api/v3/sitemap/status/{job_id}"
            )
```

## 10. Conclusion

This targeted fix addresses the specific issue preventing the sitemap scanning API endpoint from creating proper job records. By adding the job creation code at the appropriate location within the existing transaction boundary, we ensure that jobs are properly registered in the database and can be discovered and processed by the scheduler.

The fix maintains consistency with the previously standardized background processing implementation and follows established patterns for job creation. Once implemented, the system should function as documented in the 07-69-SITEMAP-SCANNER-BACKGROUND-SERVICE-STANDARDIZATION-COMPLETION report.
