# Sitemap Processing System Overview

## System Components

### Complete File Structure

```
src/
├── routers/
│   └── modernized_sitemap.py
├── services/
│   ├── sitemap/
│   │   ├── processing_service.py
│   │   ├── background_service.py
│   │   └── analyzer_service.py
│   └── sitemap_scheduler.py
├── models/
│   └── sitemap.py
└── scripts/
    └── sitemap_scheduler/
        ├── add_test_sitemap.py
        ├── check_sitemap.py
        ├── process_sitemap.py
        ├── reset_sitemap_status.py
        └── monitor_scheduler.py
```

## Process Flow Diagrams

### 1. Single Domain Processing Flow

```
[HTTP Request]
      ↓
[modernized_sitemap.py]
      ↓
[scan_domain() → background_tasks.add_task()]
      ↓
[processing_service.py]
      ↓
[process_domain_with_own_session()]
      ↓
[background_service.py]
      ↓
[process_domain_background()]
      ↓
[store_domain_data()]
      ↓
[Database Tables]
      ↓
[Jobs] → [Domains] → [Sitemaps] → [SitemapUrls]
```

### 2. Batch Processing Flow

```
[HTTP Request]
      ↓
[modernized_sitemap.py]
      ↓
[scan_domain() → background_tasks.add_task()]
      ↓
[processing_service.py]
      ↓
[process_batch_background()]
      ↓
[Loop for each domain]
      ↓
[process_domain_background()]
      ↓
[store_domain_data()]
      ↓
[Database Tables]
      ↓
[Single Job] → [Multiple Domains] → [Multiple Sitemaps] → [Multiple URLs]
```

### 3. Scheduler Processing Flow

```
[FastAPI Startup]
      ↓
[setup_sitemap_scheduler()]
      ↓
[Interval Trigger (5 min)]
      ↓
[process_pending_sitemaps()]
      ↓
[Query Pending Sitemaps]
      ↓
[Process Each Sitemap]
      ↓
[Update Status]
      ↓
[Database Tables]
      ↓
[Status: pending → processing → completed/error]
```

### 1. API Layer

- **File**: `src/routers/modernized_sitemap.py`
- **Purpose**: Handles HTTP requests for sitemap processing
- **Key Endpoints**:
  - `POST /api/v3/sitemap/scan`: Initiates single domain processing
  - `POST /api/v3/sitemap/batch`: Initiates batch domain processing

### 2. Processing Service

- **File**: `src/services/sitemap/processing_service.py`
- **Purpose**: Core processing logic for sitemap discovery and analysis
- **Key Functions**:
  - `process_domain_with_own_session`: Processes a single domain
  - `process_batch_background`: Processes multiple domains
  - `store_domain_data`: Stores results in database

### 3. Background Service

- **File**: `src/services/sitemap/background_service.py`
- **Purpose**: Handles asynchronous processing of sitemap scanning operations
- **Key Functions**:
  - `process_domain_background`: Background task for single domain
  - `process_batch_background`: Background task for multiple domains

### 4. Scheduler Service

- **File**: `src/services/sitemap_scheduler.py`
- **Purpose**: Automatically processes pending sitemaps
- **Key Functions**:
  - `process_pending_sitemaps`: Processes sitemaps with 'pending' status
  - `setup_sitemap_scheduler`: Configures and starts the scheduler
  - `shutdown_sitemap_scheduler`: Gracefully stops the scheduler

### 5. Database Models

- **File**: `src/models/sitemap.py`
- **Purpose**: Defines database schema for sitemap data
- **Key Tables**:
  - `SitemapFile`: Stores sitemap information
  - `SitemapUrl`: Stores individual URLs from sitemaps

## Process Flow

### 1. Initial Request

1. User submits domain(s) through API endpoint
2. Request is handled by `modernized_sitemap.py` router
3. Job ID is generated and stored in memory
4. Background task is initiated

### 2. Background Processing

1. `process_domain_with_own_session` or `process_batch_background` is called
2. Domain is analyzed using `SitemapAnalyzer`
3. Results are stored in database tables
4. Job status is updated throughout process

### 3. Scheduler Processing

1. Scheduler runs at configured intervals (default: 5 minutes)
2. Queries database for sitemaps with 'pending' status
3. Processes each pending sitemap in batches (default: 20)
4. Updates sitemap status to 'completed' or 'error'

## Database Schema

### SitemapFile Table

```sql
CREATE TABLE sitemap_files (
    id UUID PRIMARY KEY,
    domain_id UUID REFERENCES domains(id),
    url VARCHAR(2000),
    sitemap_type VARCHAR(50),
    status VARCHAR(20),
    url_count INTEGER,
    has_lastmod BOOLEAN,
    has_priority BOOLEAN,
    has_changefreq BOOLEAN,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

### SitemapUrl Table

```sql
CREATE TABLE sitemap_urls (
    id UUID PRIMARY KEY,
    sitemap_id UUID REFERENCES sitemap_files(id),
    url VARCHAR(2000),
    lastmod TIMESTAMP,
    changefreq VARCHAR(50),
    priority FLOAT,
    created_at TIMESTAMP
);
```

## Configuration

### Environment Variables

```bash
# Scheduler Configuration
SITEMAP_SCHEDULER_INTERVAL_MINUTES=5
SITEMAP_SCHEDULER_BATCH_SIZE=20
SITEMAP_SCHEDULER_MAX_INSTANCES=1

# Processing Configuration
MAX_URLS_PER_SITEMAP=1000
```

## Testing Tools

Located in `scripts/sitemap_scheduler/`:

1. **Add Test Data**

```bash
python scripts/sitemap_scheduler/add_test_sitemap.py --domain example.com
```

2. **Check Status**

```bash
python scripts/sitemap_scheduler/check_sitemap.py --domain example.com
```

3. **Process Specific Sitemap**

```bash
python scripts/sitemap_scheduler/process_sitemap.py --sitemap-id <id>
```

4. **Monitor Scheduler**

```bash
python scripts/sitemap_scheduler/monitor_scheduler.py --interval 30
```

## Integration with FastAPI

The scheduler is integrated into the FastAPI application lifecycle:

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Start up
    await start_sitemap_scheduler()

    yield

    # Shutdown
    await shutdown_sitemap_scheduler()
```

## Error Handling

1. **Network Errors**

   - Failed HTTP requests are caught and recorded
   - Sitemap status updated to 'error'
   - Error message stored in database

2. **Processing Errors**

   - Invalid sitemap format errors are captured
   - Processing continues with next sitemap
   - Error details logged for debugging

3. **Database Errors**
   - Transaction management ensures data consistency
   - Failed operations are rolled back
   - Error status is properly recorded

## Status Flow

1. **Initial State**

   - Sitemap created with 'pending' status
   - Job created with 'running' status

2. **Processing State**

   - Sitemap status updated to 'processing'
   - Job status remains 'running'
   - Progress tracked in job metadata

3. **Completion State**

   - Sitemap status updated to 'completed'
   - Job status updated to 'completed'
   - Results stored in database

4. **Error State**
   - Sitemap status updated to 'error'
   - Job status updated to 'failed'
   - Error message stored in database

## Recent Changes

1. **Standardization Work Order**

   - Improved database connection handling
   - Enhanced transaction boundaries
   - Better error handling and logging

2. **Scheduler Implementation**
   - Added background processing service
   - Implemented batch processing
   - Added monitoring and testing tools

## Next Steps

1. Complete scheduler implementation
2. Add comprehensive testing
3. Implement monitoring and alerting
4. Add performance optimization
5. Enhance error recovery mechanisms

## Entry Points and Processing Flow

### 1. Entry Points

There are two ways to submit domains for processing:

1. **Single Entry**

   - Endpoint: `POST /api/v3/sitemap/scan`
   - Takes one domain
   - Creates one job entry in job table
   - Processes immediately through background task

2. **Multiple Entry**
   - Endpoint: `POST /api/v3/sitemap/batch`
   - Takes multiple domains
   - Creates one job entry in job table (for entire batch)
   - Processes sequentially through same background task

### 2. Job Table Integration

```
[Entry Point]
      ↓
[Create Job Entry]
      ↓
[Store Domains]
      ↓
[Background Task]
      ↓
[Process Domains]
      ↓
[Update Job Status]
```

### 3. Required Integration Steps

1. **Job Table Setup**

   ```sql
   CREATE TABLE jobs (
       id UUID PRIMARY KEY,
       job_type VARCHAR(50),
       status VARCHAR(20),
       progress FLOAT,
       result_data JSONB,
       error_message TEXT,
       created_at TIMESTAMP,
       updated_at TIMESTAMP
   );
   ```

2. **Job Service Integration**

   - Ensure `job_service.py` has proper methods:
     ```python
     async def create_job(session, job_type, status="pending")
     async def update_status(session, job_id, status, progress, result_data=None)
     async def get_job(session, job_id)
     ```

3. **Background Task Integration**

   - Update `process_domain_with_own_session` to:
     - Use proper job tracking
     - Handle both single and multiple domains
     - Update progress for each domain
     - Store results in job table

4. **Scheduler Integration**
   - Ensure scheduler can:
     - Find pending jobs
     - Process domains from jobs
     - Update job status
     - Handle errors properly

### 4. Current Integration Status

1. **Working**

   - Single domain entry
   - Multiple domain entry
   - Basic job tracking
   - Database storage

2. **Needs Implementation**

   - Proper job status updates
   - Progress tracking for batches
   - Error handling in scheduler
   - Result storage in job table

3. **Required Changes**

   ```python
   # In background_service.py
   async def process_batch_background(args: Dict[str, Any]):
       job_id = args.get("job_id")
       domains = args.get("domains", [])

       # Update job status for batch start
       await job_service.update_status(
           session=session,
           job_id=job_id,
           status="processing",
           progress=0.0
       )

       # Process each domain
       for domain in domains:
           # Process domain
           result = await process_domain_background(...)

           # Update job progress
           progress = (processed_count / total_domains) * 100
           await job_service.update_status(
               session=session,
               job_id=job_id,
               status="processing",
               progress=progress
           )
   ```
