# 14.2 DEEP SCRAPE IMPLEMENTATION PROGRESS LOG

**Work Order:** `14.1-GOOGLE MAPS DEEP SCRAPE IMPLEMENTATION PLAN.md`

This document tracks the progress of implementing the Curation-Driven Deep Scrape feature according to the phases and tasks outlined in the implementation plan (14.1).

## Phase 1: Core Deep Scan & Persistence Logic

- **Goal:** Implement and thoroughly test the logic for retrieving, mapping, and saving detailed information for a _single_ place.

### Task 1.1: Create `LocalBusiness` Model & Migration

- **Status:** ✅ COMPLETED
- **Action:** Created the `src/models/local_business.py` file with the SQLAlchemy model definition based on the `local_businesses` table schema from section 4.5 of the plan (14.1).
- **Date:** 2023-04-02
- **Notes:**
  - Used Python to create the model file with all fields from the inspected schema
  - Added proper typing, indexes, and helper methods (`to_dict()`, `__repr__()`)
  - Added appropriate nullability constraints matching the database schema
  - Made `place_id` unique and indexed for efficient lookups
  - Added comments and organized fields into logical sections

### Task 1.2: Align Database Schema

- **Status:** ✅ ADDRESSED MANUALLY (Pending Alembic Sync)
- **Action:** Added missing `place_id` column (with unique constraint and index) directly to the `local_businesses` table via SQL script.
- **Date:** 2023-04-02
- **Notes:**
  - Initial testing revealed a schema mismatch: `place_id` column defined in the `LocalBusiness` model was missing from the database table.
  - This prevented the service from saving data using the `place_id` for identification/upserting.
  - Provided SQL script (`ALTER TABLE ... ADD COLUMN ...`, `ADD CONSTRAINT ...`, `CREATE INDEX ...`) to user for manual execution in Supabase to unblock testing.
  - A full Alembic migration generation (`alembic revision --autogenerate`) and application (`alembic upgrade head`) should be performed later to ensure complete schema synchronization and proper versioning.

### Task 1.3: Implement Deep Scan API Call

- **Status:** ✅ COMPLETED
- **Action:** Implemented the call to the Google Maps Place Details API within `PlacesDeepService.process_single_deep_scan` using the `googlemaps` client.
- **Date:** 2023-04-02
- **Notes:**
  - Defined `required_fields` based on `LocalBusiness` model.
  - Corrected field name from `utc_offset_minutes` to `utc_offset` based on API error during testing.
  - Handled API key initialization and basic error checking (`status != 'OK')`.
  - Added logging for API call and response status.

### Task 1.4: Implement API Response Mapping

- **Status:** ✅ COMPLETED
- **Action:** Implemented and refined the `_map_details_to_model` helper function in `PlacesDeepService`.
- **Date:** 2023-04-02
- **Notes:**
  - Initially mapped basic fields.
  - Corrected dictionary keys during testing to match `LocalBusiness` model column names based on `CompileError: Unconsumed column names` (e.g., `phone_number`->`phone`, `website`->`website_url`, `user_ratings_total`->`reviews_count`).
  - Added basic logic to handle mapping/transformation for `price_level` -> `price_text`, `types` -> `main_category`/`extra_categories`, `utc_offset` -> `timezone`.
  - Implemented storing unmapped API fields (`business_status`, `vicinity`, raw `opening_hours`) into the `additional_json` column.
  - Used `.get()` for safe dictionary access.

### Task 1.5: Implement Database Upsert Logic

- **Status:** ✅ COMPLETED
- **Action:** Implemented ORM-based upsert logic in `PlacesDeepService.process_single_deep_scan` using SQLAlchemy's `insert().on_conflict_do_update()`.
- **Date:** 2023-04-02
- **Notes:**
  - Used `await get_session()` and `async with session.begin()` for correct async session handling and transaction management (fixed initial `TypeError` related to awaiting coroutine).
  - Added requirement and handling for `tenant_id` parameter, passing it into the data dictionary for insertion (fixed `NotNullViolationError`).
  - Targeted `place_id` for conflict resolution (`index_elements`).
  - Dynamically built the `set_` dictionary to update all relevant columns on conflict.
  - Used `.returning(LocalBusiness)` to get the affected object.
  - Added basic DB error handling and session closing in `finally` block.

### Task 1.6: Implement Orchestration Logic (`process_places_deep_scan_job`)

- **Status:** ✅ COMPLETED
- **Action:** Implemented the core orchestration method in `PlacesDeepService` to handle batch processing based on a discovery job ID.
- **Date:** 2023-04-03
- **Notes:**
  - Fetches the `Job` record to get the `discovery_job_id` from `params`.
  - Queries `places_staging` table for relevant places linked to the discovery job.
  - Iterates through staging places, calling `process_single_deep_scan` for each.
  - Includes logic for direct job status updates (`processing`, `completed`, `failed`) and progress calculation/updates.
  - Handles cases with no places found and includes basic error handling for the orchestration process itself.

### Task 1.7: Resolve `JobStatus` Import Error

- **Status:** ✅ COMPLETED
- **Action:** Investigated and resolved the persistent linter error related to `JobStatus` import in `PlacesDeepService`.
- **Date:** 2023-04-03
- **Notes:**
  - Confirmed by reading `src/models/job.py` that the `Job.status` field is a `String`, not an `Enum`.
  - Removed the incorrect `JobStatus` import.
  - Updated all status assignments within `process_places_deep_scan_job` to use string literals (e.g., `"processing"`, `"completed"`).

### Task 1.8: Resolve `processed` Field Assignment Error

- **Status:** ✅ COMPLETED
- **Action:** Investigated and addressed the linter error related to assigning the `processed` flag on `Place` model instances.
- **Date:** 2023-04-03
- **Notes:**
  - Confirmed by reading `src/models/place.py` that `processed` is a standard `Boolean` column.
  - Re-enabled the assignment `place_staging_record.processed = True` and the subsequent `session.merge()` call.
  - Added `# type: ignore` to suppress the persistent (likely false positive) linter warning on the assignment line.

### Task 1.9: Debug and Resolve Place Storage Enum Mismatch

- **Status:** ✅ COMPLETED
- **Action:** Diagnosed persistent database errors during place storage using `scripts/testing/test_google_maps_api.py`. Identified a mismatch between the `PlaceStatusEnum` values defined in the Python model (`src/models/place.py`, using lowercase like 'new') and the actual enum values in the Supabase database (`place_status_enum`, requiring capitalized like 'New'). Corrected the Python enum definition to match the database schema.
- **Date:** 2023-04-03
- **Notes:**
  - This mismatch prevented new places fetched from the Google Maps API from being saved, causing the `store_places` function in `PlacesStorageService` to fail during `session.flush()`.
  - Root cause was an incorrect assumption about the database enum values within the Python model definition.
  - Fix involved updating the string values in `PlaceStatusEnum` in `src/models/place.py`.

## Phase 2: Queueing Mechanism

- **Goal:** Implement the mechanism for users to mark places for deep scanning and trigger the queue.

### Task 2.1: Update `Place` Model & Enums

- **Status:** ✅ COMPLETED (Manual SQL + Model Code)
- **Action:** Updated `src/models/place.py` to include `deep_scan_status` (Enum) and `deep_scan_error` (Text) columns. Corresponding database changes were applied manually via SQL.
- **Date:** 2023-04-03
- **Notes:** Required separating deep scan lifecycle from main user-facing status.

### Task 2.2: Implement Direct Queueing Endpoint (`/queue-deep-scan`)

- **Status:** ✅ COMPLETED
- **Action:** Implemented and refined the `PUT /api/v3/places-staging/queue-deep-scan` endpoint in `src/routers/places_staging.py`.
- **Date:** 2023-04-03
- **Notes:**
  - This endpoint accepts a list of `place_ids`.
  - It ONLY sets `deep_scan_status = queued` and clears `deep_scan_error` for eligible places.
  - It does NOT modify the main `status` field.
  - Serves as a secondary mechanism for programmatic/admin queueing.

### Task 2.3: Implement Unified Status Update Endpoint (`/status`)

- **Status:** ✅ COMPLETED & VERIFIED
- **Action:** Implemented the `PUT /api/v3/places/staging/status` endpoint in `src/routers/places_staging.py` and removed the old single-item endpoint (`PUT /{place_id}/status`).
- **Date:** 2023-04-03
- **Notes:**
  - This is the **primary** endpoint for UI-driven status updates.
  - Accepts a list of one or more `place_ids` and a target main `status`.
  - Updates the main `status` field for all specified places.
  - **Crucially**, if the target main `status` is `SELECTED_FOR_DEEP_SCAN`, it also **automatically** sets `deep_scan_status = queued` (and clears `deep_scan_error`) for eligible places within the same transaction.
  - Handles both single and batch updates.
- **Verification (2023-04-03):**
  - **Goal:** Confirm endpoint updates both `status` and `deep_scan_status` correctly when trigger status is used (Verification Point #1 from `14.9`).
  - **Debugging Steps:**
    - Resolved initial `404 Not Found` errors by correcting router prefix inclusion in `src/main.py` (removed duplicate prefix application).
    - Resolved subsequent `422 Unprocessable Entity` errors by identifying mismatch between API enum value (`"Selected for Deep Scan"` from `PlaceStagingStatusEnum`) and database enum label (`"Selected"` from `place_status_enum`).
    - Encountered persistent `500 Internal Server Error` ('Database error') with initial implementation attempts, including `update().values().case()` and early ORM fetch-update variations.
  - **Resolution:**
    - Refactored the endpoint logic to strictly adhere to the ORM-first principle (`01-ABSOLUTE_ORM_REQUIREMENT.md`) using a fetch-then-update pattern:
      1. Fetch `Place` objects based on `place_ids`.
      2. Loop through objects in Python.
      3. Explicitly map the incoming API trigger status (`Selected for Deep Scan`) to the correct database `PlaceStatusEnum.Selected` member.
      4. Assign the correct `PlaceStatusEnum` member directly to the `place.status` attribute.
      5. Conditionally assign `DeepScanStatusEnum.queued` to `place.deep_scan_status` based on the original API trigger status and eligibility.
      6. Let the SQLAlchemy session handle the UPDATE generation on commit.
  - **Outcome:** Successful `curl` test confirmed the endpoint now correctly updates `status` to `'Selected'` and `deep_scan_status` to `'queued'` for the target `place_id`. **Verification Point #1 is confirmed.**

### Task 2.4: Update Implementation Plan (Section 4.6)

- **Status:** ✅ COMPLETED
- **Action:** Updated `14.1-GOOGLE MAPS DEEP SCRAPE IMPLEMENTATION PLAN.md` (Section 4.6) multiple times to reflect the finalized approach:
  - Automatic queueing trigger via main status update.
  - Consolidation to a single primary batch/single status update endpoint (`/status`).
  - Specification of the secondary direct queueing endpoint (`/queue-deep-scan`).
  - Recommendation to deprecate/remove the old single-item endpoint.
- **Date:** 2023-04-03

## Phase 3: Background Automation & Final Integration

- **Goal:** Automate the process of picking up queued items (`deep_scan_status == 'queued'`) and triggering the deep scan logic.

### Task 3.1: Implement Background Monitoring Logic

- **Status:** ✅ COMPLETED (Integrated into existing scheduler)
- **Action:** Modified the **existing `src/services/sitemap_scheduler.py`** service. Specifically, the `process_pending_jobs` function was updated to query `places_staging` for `deep_scan_status == 'queued'` and trigger the single-place deep scan service (`PlacesDeepService.process_single_deep_scan`) for each found item, updating the status to `processing` beforehand.
- **Date:** [Date Implementation Completed - Needs Verification]
- **Notes:** The deep scan monitoring and triggering logic was integrated into the existing scheduler service rather than creating a new, dedicated service file. This handles Phase 3.1.

### Task 3.2: Implement Final Status Updates in `places_staging`

- **Status:** ✅ COMPLETED (Integrated into scheduler logic)
- **Action:** The logic within `process_pending_jobs` in `src/services/sitemap_scheduler.py` was implemented to handle the outcome of the `PlacesDeepService.process_single_deep_scan` call. It updates the `deep_scan_status` to `completed` on success or `failed` (and sets `deep_scan_error`) on failure for the corresponding record in the `places_staging` table.
- **Date:** [Date Implementation Completed - Needs Verification]
- **Notes:** This handles Phase 3.2 as part of the integrated scheduler modification.

### Task 3.3: Testing

- **Status:** ⏳ PENDING
- **Action:** Develop unit and integration tests covering the full background processing flow (queueing -> scheduler pickup -> processing -> final status/data verification).

### Task 3.4: Debugging & Lessons Learned (Phase 3)

- **Status:** ✅ COMPLETED
- **Date:** 2023-04-03
- **Summary:** Significant debugging was required to make the background scheduler (`sitemap_scheduler.py`) reliably process queued deep scans. Several key issues and architectural points were clarified:
  - **Scheduler Workflow:** The `process_pending_jobs` function in `sitemap_scheduler.py` implements a _synchronous execution model_ for each deep scan within its loop. It finds `queued` items, updates status to `processing`, **awaits** the full execution of `PlacesDeepService.process_single_deep_scan`, and _then_ updates the status to `complete` or `failed`. It does **not** launch `process_single_deep_scan` as a separate, independent background task.
  - **Nested Transaction Error:** The primary bug preventing scheduler processing was `sqlalchemy.exc.InvalidRequestError: A transaction is already begun on this Session`. This occurred because the status updates to `processing` and `complete` were incorrectly wrapped in nested `async with session.begin():` blocks _within_ the main loop's session context. **Lesson:** Avoid nesting `session.begin()` if a transaction is already active on the session. Rely on the outer transaction context.
  - **Fix Implementation:** Removed the nested `session.begin()` blocks for status updates inside the `process_pending_jobs` loop. Added `await session.flush()` after the `session.execute(stmt_update_...)` calls for `processing` and `complete` statuses to ensure changes are sent to the DB promptly within the ongoing transaction.
  - **`places_staging` Status Fields:**
    - `status` (Enum `PlaceStatusEnum`): Represents the main user-facing status (e.g., `New`, `Selected`). Updated via the `/status` API.
    - `deep_scan_status` (Enum `DeepScanStatusEnum`): Tracks the background processing lifecycle (`queued`, `processing`, `complete`, `failed`). Managed primarily by the `/status` API (sets to `queued` conditionally) and the background scheduler.
    - `deep_scan_error` (Text): Stores error messages if the background scheduler fails to process a deep scan.
  - **`skip_locked` Debugging:** The `.with_for_update(skip_locked=True)` clause was temporarily removed from the scheduler's query for `queued` items to help diagnose if row locking was preventing items from being picked up. While the root cause was the nested transaction, this highlighted `skip_locked` as a factor to consider in scheduler behavior. **Recommendation:** Add `skip_locked=True` back to the query for production robustness against multiple scheduler instances, now that the primary bug is fixed.
  - **ORM Best Practices:** This debugging exercise reinforced the critical importance of adhering to correct SQLAlchemy ORM session and transaction management patterns, especially in asynchronous contexts.

## Phase 4: Documentation & Cleanup

- **Goal:** Finalize documentation and remove any temporary code.

### Task 4.1: Update Documentation

- **Status:** ⏳ PENDING
- **Action:** Update relevant project documentation (README, architecture docs) to reflect the final implementation, including the lessons learned above.

### Task 4.2: Remove Debug Code

- **Status:** ⏳ PENDING
- **Action:** Remove the temporary debug endpoint (`/debug/trigger-scan/{place_id}`) added to `src/routers/places_staging.py`.

### Task 4.3: Reinstate `skip_locked=True`

- **Status:** ⏳ PENDING
- **Action:** Add `.with_for_update(skip_locked=True)` back to the scheduler query in `src/services/sitemap_scheduler.py` as recommended in Task 3.4.

## Phase 5: UI Integration & Final Debugging (Added 2023-04-03)

- **Goal:** Integrate the backend API with the "Staging Editor" UI (`static/google-maps.html`) and resolve any remaining issues to achieve a functional end-to-end Curation-Driven Deep Scan workflow initiated from the UI.

### Task 5.1: Initial UI Implementation & Data Fetching Debug

- **Status:** ✅ COMPLETED
- **Action:** Implemented the data grid UI according to `14.11-Staging-Editor-UI-Data-Grid.md`. Addressed initial JavaScript errors preventing data display (incorrect property names: `item.name` -> `item.business_name`, `item.formatted_address` -> `item.address` in `renderStagingTable`).
- **Date:** 2023-04-03
- **Notes:** Frontend JS was initially accessing incorrect keys from the API response.

### Task 5.2: Resolve Backend 404 on Batch Status Update (`PUT /status`)

- **Status:** ⏳ FAILED / BLOCKED
- **Action:** Investigated and attempted to resolve the `404 Not Found` error encountered when the UI's "Update Selected" button sends a `PUT` request to `/api/v3/places/staging/status`.
- **Debugging Steps & History:**
  1.  **Initial Hypothesis:** Router (`places_staging.router`) not included in `main.py`. **Result:** Incorrect. Verified `app.include_router(places_staging_router)` was present.
  2.  **Second Hypothesis:** Missing `prefix` in `app.include_router` call in `main.py`. **Action:** Added `prefix="/api/v3/places/staging"`. **Result:** Fixed the `PUT /status` 404, BUT introduced a _new_ 404 error on the _data fetching_ endpoint (`GET /api/v3/places/staging/`).
  3.  **Third Hypothesis:** Conflict between `@router.get("/")` (or `@router.get("")`) and `@router.get("/{discovery_job_id}")` due to route matching order in FastAPI when combined with the prefix in `main.py`. **Action:** Reordered routes in `places_staging.py` to define the general `@router.get("")` _before_ the specific `@router.get("/{discovery_job_id}")`. **Result:** Still resulted in 404 on the `GET` request.
  4.  **Fourth Hypothesis:** Ambiguity in prefix merging. **Action:** Refactored routing:
      - Removed `prefix` from `APIRouter` definition in `places_staging.py`.
      - Added full explicit paths (e.g., `/places/staging`, `/places/staging/status`) to each `@router` decorator within `places_staging.py`.
      - Updated `app.include_router` in `main.py` to use only `prefix="/api/v3"`.
  5.  **Current State:** This refactor (Step 4) successfully **fixed the 404 error on the `GET /api/v3/places/staging/`** request, allowing the data grid to load. HOWEVER, the **`PUT /api/v3/places/staging/status`** request now results in a **`404 Not Found`** again, as shown in the UI error message ("Error updating status: HTTP error! Status: 404 - Not Found").
- **Date:** 2023-04-03
- **Conclusion:** Despite the backend code appearing correct (router included with `/api/v3` prefix, endpoint defined with `/places/staging/status` path), FastAPI is not routing the `PUT` request correctly. The exact cause remains elusive. Key takeaways include the sensitivity of FastAPI routing to definition order and prefix interactions. Potential next steps for debugging could involve inspecting the exact URL (including trailing slashes) being sent by the frontend JavaScript for the PUT request and comparing it meticulously against the backend route definition. This is the **critical blocker** preventing the completion of the Curation-Driven Deep Scan UI workflow.

### Task 5.4: Deep Reflection & Guidance for Next Steps (Added 2023-04-03)

- **Status:** ✅ COMPLETED
- **Action:** Synthesized lessons learned from the extended debugging session to provide clear guidance for resolving the final `PUT /status` 404 error.
- **Key Lessons & Critical Considerations:**
  1.  **FastAPI Routing Complexity:** Interactions between router prefixes, path parameters, route definition order, and trailing slashes require meticulous attention. Explicit paths within the router seem safer than complex prefixing.
  2.  **Verify All Related Endpoints:** Changes to routing (especially prefixes) demand re-testing _all_ endpoints on that router.
  3.  **Trailing Slashes:** Check meticulously for mismatches between frontend calls (`/path` vs `/path/`) and backend definitions.
  4.  **Definition Order:** More general routes (`/`) should typically precede more specific routes (`/{param}`) within the same router file.
  5.  **ORM Requirements:** Adherence to the project's ORM-first principle (fetch-then-update, transaction/session management, enum mapping) is mandatory for writes and was a source of earlier bugs.
  6.  **Enum Alignment:** Persistent issues arose from mismatches between Python Enums and DB Enum definitions. Constant verification is needed.
  7.  **Tooling Limitations:** AI code generation/application requires critical review and verification.
- **Concrete Debugging Steps for `PUT /api/v3/places/staging/status` 404:**
  1.  **Focus:** Target only this specific 404 error.
  2.  **Frontend Call Verification:** Use browser dev tools (Network Tab) to capture the **exact** Request URL (check trailing slash!), Method (`PUT`), and Headers (`Authorization`, `Content-Type`).
  3.  **Backend Definition Verification:** Double-check `@router.put("/places/staging/status", ...)` in `places_staging.py` and `app.include_router(..., prefix="/api/v3", ...)` in `main.py`.
  4.  **Server Log Forensics:** Restart server, trigger the error, check `docker-compose logs -f scrapersky` _immediately_ for _any_ related log entry (even if just the incoming request line).
  5.  **Inspect Runtime Routes (Optional):** Add a temporary `/debug/routes` endpoint to `main.py` to list all routes FastAPI has registered at runtime. Verify the target `PUT` route exists.
  6.  **Simplify/Isolate:** Temporarily comment out potentially conflicting routes (like `/{discovery_job_id}`) in `places_staging.py` to eliminate them as the cause.

### Task 5.5: Final End-to-End Testing (Renumbered from 5.3)

- **Status:** ⏳ PENDING (Blocked by Task 5.2)
- **Action:** Perform full workflow testing: Select items -> Update status to trigger deep scan -> Verify scheduler picks up items -> Verify `local_businesses` data -> (Optional) Verify results in new viewer UI.

## Phase 6: Future Enhancements (Optional)

### Task 6.1: Implement Results Viewer UI

- **Status:** ⏳ PENDING
- **Action:** Create a new UI tab/section to view data from the `local_businesses` table, similar to the Staging Editor. See Work Order `14.12-Results-Viewer-UI-Data-Grid.md` (to be created).
