# Work Order: Implement Synchronous Secondary State Update for Email Scraping

**Version:** 1.0
**Date:** 2025-04-22
**Related Files:** `src/routers/domains.py`, `project-docs/10-architectural-patterns/5-SYNCHRONOUS-SECONDARY-STATE-UPDATE-PATTERN.md`

## 1. Objective

Implement a new API endpoint that follows the [Synchronous Secondary State Update Pattern](project-docs/10-architectural-patterns/5-SYNCHRONOUS-SECONDARY-STATE-UPDATE-PATTERN.md) to trigger email scraping for domains. This endpoint will allow batch updating of domains' curation status while simultaneously setting their `page_scrape_status` to "Queued" when appropriate, similar to how `sitemap_analysis_status` is updated when domains are set to "Selected".

## 2. Background & Context

The system currently has a pattern where setting `sitemap_curation_status` to "Selected" also sets `sitemap_analysis_status` to "Queued" (implemented in `src/routers/domains.py`). This triggers the sitemap analysis background job.

Similarly, we need to implement this pattern for email scraping, where updating a primary status also updates the `page_scrape_status` field to "Queued" which will trigger the email scraping background job.

## 3. Requirements

### 3.1 API Endpoint

Create a new endpoint in the `domains.py` router:

```
PUT /api/v3/domains/email-scan/status
```

This endpoint should:

1. Accept a batch of domain IDs and a target status value
2. Update all specified domains with the new status
3. Conditionally set `page_scrape_status = "Queued"` (using `TaskStatus.PENDING.value`) for domains that meet specific criteria
4. Return counts of the updated domains and those queued for email scraping

### 3.2 Request Schema

Use a Pydantic model similar to `DomainBatchCurationStatusUpdateRequest`:

```python
class DomainBatchEmailScanStatusUpdateRequest(BaseModel):
    domain_ids: List[UUID]
    email_scan_status: EmailScanStatusEnum  # Define this enum based on UI requirements
```

### 3.3 Response Schema

The endpoint should return:

```json
{
  "updated_count": 10,
  "queued_for_scan_count": 8
}
```

### 3.4 Conditional Logic

The exact condition for when to set `page_scrape_status` to "Queued" should be defined, but typically:

1. If the user sets the primary status to a specific value (e.g., "Selected" or equivalent)
2. AND the current `page_scrape_status` is not already in an active state ("Queued"/"InProgress")
3. THEN set `page_scrape_status = "Queued"` (TaskStatus.PENDING.value)

### 3.5 Database Transaction

Ensure both the primary status update and the `page_scrape_status` update happen within the same atomic database transaction.

## 4. Implementation Guidelines

### 4.1 Follow Existing Pattern

Use the implementation in `update_domain_sitemap_curation_status_batch` function in `src/routers/domains.py` as a reference:

```python
# Conditional logic: If status is 'Selected', queue for analysis
if db_curation_status == SitemapCurationStatusEnum.Selected:
    domain.sitemap_analysis_status = SitemapAnalysisStatusEnum.Queued
    domain.sitemap_analysis_error = None
    queued_count += 1
    logger.debug(f"Setting domain {domain.id} sitemap_analysis_status to Queued")
```

### 4.2 Use TaskStatus Enum

For the `page_scrape_status` field, use the `TaskStatus` enum from `src/models/__init__.py`:

```python
from src.models import TaskStatus

# When setting queued status:
domain.page_scrape_status = TaskStatus.PENDING.value  # "Queued"
```

### 4.3 Error Handling

Include proper error handling with:

- Validation of input values
- Appropriate HTTP status codes for errors
- Detailed error messages in responses
- Transaction rollback on errors

### 4.4 Logging

Add comprehensive logging:

- Log the start of the batch update operation with user ID and count
- Log individual domain updates and status changes
- Log final counts of updated and queued domains
- Log any errors with appropriate severity

## 5. Expected Behavior

1. Client sends batch update request with domain IDs and target status
2. Server updates the specified domains' primary status
3. For domains meeting the criteria, `page_scrape_status` is set to "Queued"
4. The existing email scraper background job processor will pick up domains with `page_scrape_status = "Queued"` and process them
5. Client receives counts of updated and queued domains

## 6. Testing Instructions

1. **Basic Operation:**

   ```bash
   curl -X PUT \
        -H "Authorization: Bearer scraper_sky_2024" \
        -H "Content-Type: application/json" \
        -d '{"domain_ids": ["uuid1", "uuid2"], "email_scan_status": "Selected"}' \
        "http://localhost:8000/api/v3/domains/email-scan/status" | cat
   ```

2. **Database Verification:**

   - Verify updated domains have the requested primary status
   - Verify appropriate domains have `page_scrape_status = "Queued"`

3. **Background Processing:**
   - Verify the email scraper background job picks up the queued domains
   - Verify emails are extracted and stored in the contacts table

## 7. Dependencies

This implementation depends on:

- The Domain model with `page_scrape_status` field
- The TaskStatus enum in `src/models/__init__.py`
- Authentication middleware
- The existing email scraper background job

## 8. Definition of Done

This work is considered complete when:

1. The API endpoint is implemented according to the requirements
2. The conditional logic correctly sets `page_scrape_status = "Queued"`
3. The endpoint returns accurate counts
4. Testing confirms the email scraper background job successfully processes the queued domains
5. Code passes linting and any automated tests
6. Documentation is updated to reflect the new endpoint
