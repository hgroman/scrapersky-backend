# Work Order: Investigate Stuck Sitemap Analysis Job

**Version:** 1.0
**Date:** 2025-04-16
**Status:** Completed (Pending Follow-up)
**Assignee:** AI Assistant / User

## 1. Objective

Diagnose and resolve the issue preventing domains with `sitemap_analysis_status = 'Queued'` from being processed by the background job system. This investigation stems from observing that domain `863925f7-d2bd-4e5a-ae38-8af1ae870793` (ohsu.edu) remained in the 'Queued' state after being updated via the Domain Curation UI.

## 2. Background & Context

The system uses a synchronous secondary state update pattern where setting `sitemap_curation_status` to 'Selected' also sets `sitemap_analysis_status` to 'Queued'. A background scheduler (`domain_sitemap_submission_scheduler.py`) is expected to detect this 'Queued' status and trigger the actual sitemap analysis, likely via an adapter service (`domain_to_sitemap_adapter_service.py`) making an internal API call. This process appears to be stuck.

This investigation also serves as a case study for the broader goal of auditing and potentially standardizing all background job processing workflows within the application.

## 3. Scope

- Trace the execution path starting from the `domain_sitemap_submission_scheduler.py`.
- Analyze logs for the scheduler, adapter service, and target processing endpoint.
- Identify the point of failure or bottleneck preventing the job from running or completing.
- Propose and implement a fix.

## 4. Investigation Steps

1.  Verify current database status for the target domain.
2.  Examine logs for `domain_sitemap_submission_scheduler.py`.
3.  Examine logs for `domain_to_sitemap_adapter_service.py`.
4.  Examine logs for the target sitemap processing service/endpoint (e.g., `/api/v3/sitemap/scan`).
5.  Analyze findings and identify root cause.

## 5. Acceptance Criteria

- Domains with `sitemap_analysis_status = 'Queued'` are reliably picked up and processed by the background system.
- The specific domain `863925f7-d2bd-4e5a-ae38-8af1ae870793` transitions out of the 'Queued' state (either to 'Processing', 'Complete', or 'Error').
- Logs clearly show the successful execution flow or provide specific errors if processing fails for valid reasons.

## 6. Notes & Findings

_(Updated 2025-04-18 by AI Assistant)_

This investigation aimed to diagnose why domain `ohsu.edu` (ID `863925f7-d2bd-4e5a-ae38-8af1ae870793`) appeared stuck with `sitemap_analysis_status = 'Queued'`.

**Investigation Steps & Observations:**

1.  **Log Analysis (Scheduler & Processing Service):**

    - Reviewed application logs corresponding to the time the domain was likely queued.
    - Logs confirmed the `domain_sitemap_submission_scheduler` ran successfully around `2025-04-18 04:53:44 UTC`.
    - The scheduler correctly identified `ohsu.edu` as needing processing based on its 'Queued' status.
    - The scheduler invoked the `domain_to_sitemap_adapter_service`.
    - The adapter service successfully made an internal API call to `POST /api/v3/sitemap/scan` with the `ohsu.edu` domain, receiving a `202 Accepted` response. This initiated job ID `61c893f2-82b9-47d3-b24e-66ea2c7f2a67`.
    - The background task (`process_domain_with_own_session` within `src/services/sitemap/processing_service.py`) associated with this job ID started execution.
    - The background task successfully discovered the sitemap `https://ohsu.edu/sitemap.xml`.
    - Crucially, the background task **successfully created a record** in the `sitemap_files` table for this sitemap (record ID `791d3d3d-...`, `created_at` `2025-04-18 04:59:33 UTC`). This indicates a previous database insertion error (related to Enum handling, identified earlier) was resolved.
    - The background task completed by updating the domain's sitemap counts and marking the associated job (`61c893f2-...`) as `complete`.

2.  **Database Verification Attempts & Docker Issues:**

    - Initial attempts to directly query the `sitemap_files` table using a `run_psql.sh` script failed.
    - Investigation revealed the `postgres` service was not running because it was not defined in the primary `docker-compose.yml` file.
    - Analysis of `docker-compose.yml` and environment variables indicated the application connects to an **external Supabase database**, not a local Docker-managed one. This was a misunderstanding in the initial debugging approach.
    - Confirmed the main `scrapersky` service was running correctly via `docker-compose ps`, implying the connection to the external database was functional from within the container.

3.  **Direct API Endpoint Testing:**

    - To isolate the processing logic, the `POST /api/v3/sitemap/scan` endpoint was tested directly using `curl` for the `ohsu.edu` domain (`2025-04-18 05:00:26 UTC`).
    - The API call was successful, returning `200 OK` and initiating a new job ID `d2b7c977-2cbb-4a3e-8208-4f07145dfd44`.
    - A subsequent `GET` request to the status URL (`/api/v3/sitemap/status/d2b7c977-...`) confirmed this job also ran and reached `status: complete`.

4.  **Successful Database Verification:**

    - Identified and used the `scripts/db/inspect_table.py` script (after resolving `ModuleNotFoundError` by running as a module and removing an unsupported `--order_by` argument) to query the external database.
    - The query **confirmed the existence of two `sitemap_files` records** for `ohsu.edu`:
      - One created by the original scheduler run (`job_id: 61c893f2-...`, `created_at: 04:59:33 UTC`).
      - One created by the direct API test (`job_id: d2b7c977-...`, `created_at: 05:00:29 UTC`).

5.  **Final Domain Status Verification:**
    - Queried the `domains` table using `scripts/db/inspect_table.py` for `ohsu.edu` (ID `863925f7-...`).
    - Confirmed the `sitemap_analysis_status` is now `Completed`, indicating the status was updated correctly after the background job ran.
    - Noted the `last_scan` timestamp matches the time of the most recent job completion.
    - **Discrepancy Found:** The query showed `total_sitemaps: 0` and `sitemap_urls: 3`, while the `sitemap_files` table contains records for this domain. This count mismatch needs further investigation.

**Conclusions:**

- The core sitemap analysis pipeline (`scheduler -> adapter -> API endpoint -> background processing -> database save -> domain status update`) **is functional**. The initial run triggered by the scheduler _did_ successfully process the domain `ohsu.edu`, save the sitemap record, and update the domain's `sitemap_analysis_status` to `Completed`.
- The initial observation of the domain being "stuck" in 'Queued' is resolved and likely attributable to timing delays or observation before the full process completed.
- The background processing logic in `src/services/sitemap/processing_service.py` correctly handles sitemap discovery and database interaction.
- The database connection to the external Supabase instance is working correctly.

**Next Steps:**

- Verify the current `sitemap_analysis_status` of the `ohsu.edu` domain in the `domains` table (Completed).
- Review the scheduler's frequency (`SITEMAP_SCHEDULER_INTERVAL_MINUTES`) and logic (`src/schedulers/domain_sitemap_submission_scheduler.py`) if processing delays are still suspected for other domains.
- Monitor future scheduler runs for consistent processing of queued domains.

## 7. Follow-up Tasks (TODO)

- **Investigate `total_sitemaps` Count Discrepancy:** The `domains` table record for `ohsu.edu` shows `total_sitemaps: 0` despite `sitemap_files` containing records and `sitemap_urls` being > 0. Review the domain update logic in `src/services/sitemap/processing_service.py` (around line 640) to understand why this count isn't being updated correctly, especially when multiple scans for the same domain occur.
