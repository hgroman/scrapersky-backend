# ScraperSky Workflow Builder Cheat Sheet

This is a practical checklist for creating a new standardized workflow by cloning and adapting an existing one. All ScraperSky workflows follow the identical pattern - just fill in the blanks and follow these cookie-cutter steps.

## Step 1: Define Your Workflow

| Question                                    | Example Answer       | Your Answer |
| ------------------------------------------- | -------------------- | ----------- |
| **What is the workflow name?** (snake_case) | `contact_extraction` |             |
| **What is the source table?**               | `pages`              |             |
| **What is the destination table?**          | `contacts`           |             |

> **NOTE**: The workflow name determines all naming patterns. Use snake_case format.

## Step 2: Add Database Columns (Run These SQL Commands)

```sql
-- Add to source table (e.g., pages)
ALTER TABLE {source_table} ADD COLUMN {workflow_name}_curation_status {source_table}curationstatus NOT NULL DEFAULT 'New';
ALTER TABLE {source_table} ADD COLUMN {destination_table}_extraction_status {destination_table}extractionstatusenum NULL;

-- Create required enum types
CREATE TYPE {source_table}curationstatus AS ENUM ('New', 'Queued', 'Processing', 'Complete', 'Error', 'Skipped');
CREATE TYPE {destination_table}extractionstatusenum AS ENUM ('New', 'Queued', 'Processing', 'Complete', 'Error');
```

## Step 3: Add Python Enums (src/models/enums.py)

```python
class {SourceTable}CurationStatus(str, Enum):
    New = "New"
    Queued = "Queued"
    Processing = "Processing"
    Complete = "Complete"
    Error = "Error"
    Skipped = "Skipped"

class {DestinationTable}ExtractionStatusEnum(str, Enum):
    New = "New"
    Queued = "Queued"
    Processing = "Processing"
    Complete = "Complete"
    Error = "Error"
```

## Step 4: Clone & Adapt API Router

1. **Copy an existing router** (e.g., `src/routers/pages.py` → `src/routers/{source_table}s.py`)
2. **Find & replace** throughout the file:
   - Original source table name → Your source table name
   - Original workflow name → Your workflow name
   - Original destination table name → Your destination table name
   - Enum references to match your new enums

## Step 5: Register Router in main.py

```python
from src.routers.{source_table}s import router as {source_table}_router
# ...
app.include_router({source_table}_router)
```

## Step 6: Clone & Adapt Background Scheduler

1. **Copy an existing scheduler** from `src/schedulers.py`
2. **Find & replace** workflow-specific names
3. **Register your scheduler** in the same file:

```python
@scheduler.scheduled_job("interval", seconds=30, id="{workflow_name}_processor")
async def {workflow_name}_processor_job():
    await {workflow_name}_processor()
```

## Step 7: Implement Processing Service

This is the only part requiring custom logic. Create `src/services/{workflow_name}/{workflow_name}_service.py`:

```python
async def process_{source_table}_for_{workflow_name}({source_table}_id: UUID) -> None:
    try:
        # Step 1: Retrieve the source record
        async with session.begin():
            stmt = select({SourceTable}).where({SourceTable}.id == {source_table}_id)
            result = await session.execute(stmt)
            source_record = result.scalar_one_or_none()

            if not source_record:
                logger.warning(f"{SourceTable} {source_table}_id not found")
                return

        # Step 2: Extract & process data - THIS PART IS CUSTOM
        # Implement your specific extraction logic here
        extracted_data = extract_data_from_source_record(source_record)

        # Step 3: Update status after processing
        async with session.begin():
            # Create new destination record with extracted data
            new_record = {DestinationTable}(**extracted_data)
            session.add(new_record)

            # Update source record status
            source_record.{workflow_name}_curation_status = {SourceTable}CurationStatus.Complete
            await session.commit()
    except Exception as e:
        logger.error(f"Error in {workflow_name} processing: {e}")
        # Update error status
        async with session.begin():
            source_record.{workflow_name}_curation_status = {SourceTable}CurationStatus.Error
            await session.commit()
```

## Done!

That's it! The workflow is now implemented and will follow the standard pattern used throughout ScraperSky.
