# Work Order: WF7-PageCuration Workflow Implementation

_Created: 2025-05-05T08:30:00-07:00_
_Author: Cascade AI_

## Executive Summary

This work order outlines the implementation of WF7-PageCuration, a producer-consumer workflow that consumes SitemapUrl records produced by WF6-SitemapImport and processes them to extract webpage content. This workflow follows the established producer-consumer pattern used throughout the ScraperSky backend, ensuring architectural consistency and maintainability.

## 1. Workflow Definition

### 1.1 Producer-Consumer Relationship

```yaml
workflow_name: WF7-PageCuration
workflow_display_name: WF7 - Page Content Curation

workflow_connections:
  as_consumer:
    - producer_workflow: WF6-SitemapImport
      interface_table: sitemap_urls
      handoff_field: page_curation_status
      consumed_value: PageCurationStatusEnum.New
      description: |
        WF7 consumes SitemapUrl records with page_curation_status='New' produced by WF6.
        These URLs appear in the Page Curation UI for human review and selection.

  as_producer:
    - consumer_workflow: WF8-ContactCuration  # Future workflow
      interface_table: page_contents
      handoff_field: status
      produced_value: PageContentStatusEnum.New
      description: |
        WF7 produces PageContent records that can be later processed by contact extraction.
```

### 1.2 Workflow Phases

1. **UI Interaction**: User selects SitemapUrl records in Page Curation UI and updates status
2. **API Routing**: Router authenticates and validates batch status update request
3. **Database Interaction**: Status update transaction executed with ORM
4. **Background Task Triggering**: Status change acts as trigger for background processing
5. **Background Task Execution**: Scheduler polls and processes queued items
6. **Page Content Storage**: Scraped content is stored in page_contents table

## 2. Implementation Tasks

### 2.1 Database Schema Updates

**Actionable TODO: DB-Schema-Update**
- **Ticket**: SCRSKY-301
- **Target Date**: 2025-05-12
- **Priority**: HIGH
- **Description**: Add required fields and tables for page curation workflow

1. Add enum to `src/models/sitemap.py`:
   ```python
   class PageCurationStatusEnum(str, Enum):
       New = "new"             # Initial state when SitemapUrl is created
       Selected = "selected"   # Manually selected for processing
       Queued = "queued"      # Queued for background processing
       Processing = "processing" # Currently being processed
       Complete = "complete"   # Successfully processed
       Error = "error"        # Error during processing
       Skipped = "skipped"    # Manually skipped/ignored
   ```

2. Add fields to `sitemap_urls` table in `src/models/sitemap.py`:
   ```python
   # In SitemapUrl class:
   page_curation_status = Column(String, default=PageCurationStatusEnum.New)
   page_curation_updated_at = Column(DateTime)
   page_scrape_error = Column(String)
   ```

3. Create new table in `src/models/page.py`:
   ```python
   class PageContentStatusEnum(str, Enum):
       New = "new"
       Processing = "processing"
       Complete = "complete"
       Error = "error"
   
   class PageContent(Base):
       __tablename__ = "page_contents"
       
       id = Column(Integer, primary_key=True, index=True)
       sitemap_url_id = Column(Integer, ForeignKey("sitemap_urls.id"))
       url = Column(String, index=True)
       status = Column(String, default=PageContentStatusEnum.New)
       html_content = Column(Text)  # Raw HTML
       processed_content = Column(JSONB)  # Structured data
       error_message = Column(String)
       created_at = Column(DateTime, default=func.now())
       updated_at = Column(DateTime, onupdate=func.now())
       tenant_id = Column(UUID(as_uuid=True), ForeignKey("tenants.id"))
       
       # Relationships
       sitemap_url = relationship("SitemapUrl", back_populates="page_content")
   ```

4. Add relationship to `SitemapUrl` class:
   ```python
   # In SitemapUrl class:
   page_content = relationship("PageContent", back_populates="sitemap_url", uselist=False)
   ```

### 2.2 API Endpoints Implementation

**Actionable TODO: API-Endpoints-Creation**
- **Ticket**: SCRSKY-302
- **Target Date**: 2025-05-14
- **Priority**: HIGH
- **Description**: Create API endpoints for page curation workflow

1. Create router file `src/routers/page_curation_api.py`:
   ```python
   # Standard imports (FastAPI, SQLAlchemy, etc.)
   
   router = APIRouter(
       tags=["Page Curation"],
       responses={404: {"description": "Not found"}},
   )
   
   # API Models
   class PageCurationRecord(BaseModel):
       id: int
       url: str
       discovered_at: datetime
       page_curation_status: str
       domain: str
       # Additional fields as needed
   
   class PaginatedPageCurationResponse(BaseModel):
       items: List[PageCurationRecord]
       total: int
       page: int
       size: int
       pages: int
   
   class PageCurationBatchStatusUpdateRequest(BaseModel):
       sitemap_url_ids: List[int]
       status: str  # Validate against PageCurationStatusEnum
       error_message: Optional[str] = None
   ```

2. Implement listing endpoint:
   ```python
   @router.get(
       "/page-curation/list",
       response_model=PaginatedPageCurationResponse,
       summary="List All Page Curation URLs (Paginated)",
   )
   async def list_all_page_curation_urls(
       status_filter: Optional[str] = Query(None),
       page: int = Query(1, ge=1),
       size: int = Query(50, ge=1, le=100),
       session: AsyncSession = Depends(get_db_session),
       current_user: Dict[str, Any] = Depends(get_current_user),
   ) -> PaginatedPageCurationResponse:
       # Implementation follows pattern from places_staging.py
       # 1. Build query with proper filters
       # 2. Get total count for pagination
       # 3. Execute query with pagination
       # 4. Return formatted response
   ```

3. Implement batch status update endpoint:
   ```python
   @router.put(
       "/page-curation/status",
       status_code=status.HTTP_200_OK,
   )
   async def update_page_curation_status_batch(
       request_body: PageCurationBatchStatusUpdateRequest = Body(...),
       session: AsyncSession = Depends(get_db_session),
       current_user: Dict[str, Any] = Depends(get_current_user),
   ) -> Dict[str, Any]:
       # Implementation follows pattern from places_staging.py
       # 1. Extract sitemap_url_ids and status from request
       # 2. Start transaction with session.begin()
       # 3. Fetch relevant SitemapUrl objects
       # 4. Update status and timestamps using ORM (not raw SQL)
       # 5. Use dual-update pattern: when status=Selected, also set page_curation_status=Queued
       # 6. Return success response with counts
   ```

4. Register router in `src/main.py`:
   ```python
   # In main.py
   from .routers import page_curation_api
   # ...
   app.include_router(page_curation_api.router, prefix="/api/v3")
   ```

### 2.3 Background Service Implementation

**Actionable TODO: Background-Service-Creation**
- **Ticket**: SCRSKY-303
- **Target Date**: 2025-05-16
- **Priority**: HIGH
- **Description**: Create background service for processing page content

1. Create scheduler file `src/services/page_scrape_scheduler.py`:
   ```python
   # Standard imports
   
   async def process_pending_page_scrapes(limit: int = 10):
       """Process pending page scrapes fetched from the database."""
       # Implementation follows pattern from process_pending_jobs in sitemap_scheduler.py
       # 1. Set up logging and tracking variables
       # 2. Query for SitemapUrls with page_curation_status=Queued using ORM
       # 3. For each URL, update status to Processing and process with error handling
       # 4. Use PageScrapeService to do the actual scraping
       # 5. Update status to Complete or Error based on result
       
   def setup_page_scrape_scheduler():
       """Sets up the page scrape processing scheduler job."""
       # Implementation follows pattern from setup_sitemap_scheduler
       # 1. Get polling interval from settings
       # 2. Add job to scheduler with proper interval
       # 3. Log configuration
   ```

2. Create service file `src/services/page/page_scrape_service.py`:
   ```python
   # Standard imports
   
   class PageScrapeService:
       """Service for scraping and processing page content."""
       
       async def scrape_and_store_page(self, sitemap_url_id: int, tenant_id: UUID) -> Dict[str, Any]:
           """Scrapes a page based on sitemap_url_id and stores the content."""
           # 1. Retrieve SitemapUrl by ID
           # 2. Make HTTP request to URL with proper headers and timeout
           # 3. Process HTML content based on Appendix implementation details
           # 4. Create PageContent record with results
           # 5. Return success/error response
   ```

3. Register scheduler in `src/main.py`:
   ```python
   # In main.py startup event
   from .services.page_scrape_scheduler import setup_page_scrape_scheduler
   # ...
   setup_page_scrape_scheduler()
   ```

### 2.4 UI Implementation

**Actionable TODO: UI-Tab-Creation**
- **Ticket**: SCRSKY-304
- **Target Date**: 2025-05-18
- **Priority**: MEDIUM
- **Description**: Create Page Curation UI tab

1. Create static JS file `static/js/page-curation-tab.js`:
   ```javascript
   // Implementation follows pattern from staging-editor-tab.js
   // 1. Functions for loading page curation data
   // 2. Functions for batch selection/update
   // 3. Status display and filtering
   // 4. API interaction with proper authentication
   ```

2. Update HTML to include new tab in `static/scraper-sky-mvp.html`:
   ```html
   <!-- Add Page Curation tab to navbar -->
   <li class="nav-item">
     <a class="nav-link" id="page-curation-tab" data-toggle="tab" href="#page-curation" role="tab">Page Curation</a>
   </li>
   
   <!-- Add Page Curation tab content -->
   <div class="tab-pane fade" id="page-curation" role="tabpanel">
     <!-- Implementation follows pattern from staging-editor tab -->
     <!-- 1. Search/filter controls -->
     <!-- 2. Data table with pagination -->
     <!-- 3. Batch action buttons -->
   </div>
   ```

### 2.5 Testing

**Actionable TODO: Test-Implementation**
- **Ticket**: SCRSKY-305
- **Target Date**: 2025-05-20
- **Priority**: MEDIUM
- **Description**: Create comprehensive tests for page curation workflow

1. Create router tests in `tests/routers/test_page_curation_api.py`:
   ```python
   # Implementation follows pattern from test_places_staging.py
   # 1. Test listing endpoint with various filters
   # 2. Test batch status update with various scenarios
   # 3. Test error handling and validation
   ```

2. Create service tests in `tests/services/test_page_scrape_service.py`:
   ```python
   # Implementation follows pattern from other service tests
   # 1. Test scrape_and_store_page with mocked HTTP responses
   # 2. Test error handling for various scenarios
   # 3. Test proper database updates
   ```

3. Create background service tests in `tests/services/test_page_scrape_scheduler.py`:
   ```python
   # Implementation follows pattern from other scheduler tests
   # 1. Test process_pending_page_scrapes with various scenarios
   # 2. Test proper status transitions
   # 3. Test error handling
   ```

### 2.6 Documentation

**Actionable TODO: Documentation-Creation**
- **Ticket**: SCRSKY-306
- **Target Date**: 2025-05-22
- **Priority**: MEDIUM
- **Description**: Create comprehensive documentation for page curation workflow

1. Create dependency trace document:
   ```markdown
   # Workflow Trace: Page Curation

   **Version:** 1.0
   **Date:** 2025-05-22

   This document traces the full dependency chain for the user workflow where
   a user selects URLs in the Page Curation UI tab, resulting in background 
   processing to scrape and store page content.

   ## 1. Detailed Dependency Chain

   ### 1.1. Frontend (UI & JS)
   1. **File:** `static/scraper-sky-mvp.html` [SHARED]
   2. **File:** `static/js/page-curation-tab.js` [NOVEL]

   ### 1.2. Backend (API Router)
   1. **File:** `src/routers/page_curation_api.py` [NOVEL]

   ### 1.3. Backend (Services)
   1. **File:** `src/services/page_scrape_scheduler.py` [NOVEL]
   2. **File:** `src/services/page/page_scrape_service.py` [NOVEL]

   ### 1.4. Database (Models & Enums)
   1. **File:** `src/models/sitemap.py` [SHARED]
   2. **File:** `src/models/page.py` [NOVEL]

   ### 1.5. Configuration
   1. **File:** `docker-compose.yml` or `.env` [SHARED]
   
   ### 1.6. Testing
   1. **File:** `tests/routers/test_page_curation_api.py` [NOVEL]
   2. **File:** `tests/services/test_page_scrape_service.py` [NOVEL]
   3. **File:** `tests/services/test_page_scrape_scheduler.py` [NOVEL]
   ```

2. Create linear steps document:
   ```markdown
   # WF7-PageCuration Linear Steps

   This document provides a detailed step-by-step breakdown of the Page Curation workflow,
   chronicling how user input leads to page content storage.

   ## Step 1: User Interaction in UI
   The user navigates to the "Page Curation" tab and views a list of URLs discovered from sitemap imports.
   The user selects one or more URLs and clicks "Process Selected".

   ## Step 2: API Request to Update Status
   The frontend Javascript makes a PUT request to `/api/v3/page-curation/status` with a list of selected IDs
   and the target status "Selected".

   ## Step 3: Database Transaction in API Router
   The router handles the request by:
   1. Starting a transaction with `async with session.begin()`
   2. Fetching the SitemapUrl records by ID
   3. Updating the status to "Selected" and also setting page_curation_status to "Queued"
   4. Committing the transaction
   5. Returning a response with counts of updated records

   ## Step 4: Background Scheduler Polling
   The page_scrape_scheduler periodically polls the database for SitemapUrl records with page_curation_status = "Queued"
   and processes them in batches.

   ## Step 5: Page Content Scraping
   For each queued URL:
   1. The scheduler updates status to "Processing"
   2. The PageScrapeService makes an HTTP request to the URL
   3. The service processes the HTML content
   4. The service creates a PageContent record with the results
   5. The scheduler updates status to "Complete" or "Error"

   ## Step 6: Results Storage
   The results are stored in the page_contents table, ready for further processing by downstream workflows.
   ```

3. Create canonical YAML file:
   ```yaml
   # Create WF7-PageCuration_CANONICAL.yaml following the pattern from other workflows
   # Include all phases, steps, principles, and architectural requirements
   ```

4. Update python_file_status_map.md with new files.

## 3. Implementation Schedule

| Phase | Description | Start Date | End Date | Assignee |
|-------|-------------|------------|----------|----------|
| 1 | Database Schema Updates | 2025-05-10 | 2025-05-12 | TBD |
| 2 | API Endpoints Implementation | 2025-05-12 | 2025-05-14 | TBD |
| 3 | Background Service Implementation | 2025-05-14 | 2025-05-16 | TBD |
| 4 | UI Implementation | 2025-05-16 | 2025-05-18 | TBD |
| 5 | Testing | 2025-05-18 | 2025-05-20 | TBD |
| 6 | Documentation | 2025-05-20 | 2025-05-22 | TBD |

## 4. Architectural Compliance Checklist

- [ ] API Versioning: All endpoints use `/api/v3/` prefix
- [ ] ORM Usage: All database operations use SQLAlchemy ORM (no raw SQL)
- [ ] Transaction Management: Routers own transaction boundaries
- [ ] JWT Authentication: Authentication happens at API gateway level only
- [ ] Error Handling: Proper error handling and status updates
- [ ] Logging: Comprehensive logging throughout the workflow
- [ ] Testing: Comprehensive test coverage for all components

## Appendix A: Page Scraping Implementation Details

### A.1 HTML Processing Strategy

The page scraping implementation will need to handle various aspects of HTML processing:

1. **HTTP Request Handling**:
   - Proper user agent and headers
   - Timeout and retry configuration
   - Error handling for various HTTP status codes

2. **Content Extraction**:
   - Extract page title, description, and metadata
   - Extract main content using content extraction algorithms
   - Handle different page layouts and structures

3. **Data Storage**:
   - Store raw HTML for reference
   - Store processed structured data in JSONB format
   - Include metadata about processing (timestamp, version, etc.)

### A.2 Integration with Existing Page Scraping Code

The existing page scraping code will need to be integrated into the new service:

1. **Code Review**: Review existing code for quality and compatibility
2. **Adaptation**: Adapt code to fit into the service architecture
3. **Enhancement**: Add missing features (error handling, logging, etc.)

### A.3 Field Structure for page_contents Table

The exact field structure for the page_contents table will depend on the specific data being extracted, but should include:

1. **Basic Metadata**:
   - Page title
   - Description
   - Publication date
   - Author

2. **Content Structure**:
   - Main content text
   - Sections/headings
   - Lists
   - Tables

3. **Media**:
   - Images (URLs and alt text)
   - Videos
   - Documents

4. **Contact Information**:
   - Phone numbers
   - Email addresses
   - Physical addresses
   - Social media links

This structure will serve as the foundation for downstream workflows like Contact Curation.

---

**Author**: Cascade AI  
**Creation Date**: 2025-05-05T08:30:00-07:00  
**Status**: DRAFT