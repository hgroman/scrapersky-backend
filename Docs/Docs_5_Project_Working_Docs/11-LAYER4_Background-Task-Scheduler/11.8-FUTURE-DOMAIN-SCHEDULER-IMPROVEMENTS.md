# Future Domain Scheduler Improvements

## Overview

This document outlines future improvements needed for the domain scheduler component. While the critical database conformance issues have been addressed in `11.7-DOMAIN-SCHEDULER-BUGFIX-IMPLEMENTATION.md`, these additional enhancements should be implemented in a future sprint to fully modernize the scheduler component.

## 1. FastAPI Lifespan Migration

### Current Implementation

The domain scheduler is currently initialized and shut down using the deprecated FastAPI event handlers:

```python
# in src/main.py

@app.on_event("startup")
async def startup_event():
    """Initialize resources on application startup."""
    logger.info("Starting up the ScraperSky API")

    # Start the domain processing scheduler
    setup_domain_scheduler()
    logger.info("Domain processing scheduler started")

@app.on_event("shutdown")
async def shutdown_event():
    """Clean up resources on application shutdown."""
    # Shutdown the metadata extractor session
    await session_manager.close()

    # Shutdown the domain scheduler
    shutdown_domain_scheduler()
    logger.info("Domain processing scheduler shut down")
```

### Proposed Change

The startup and shutdown events should be migrated to the modern FastAPI lifespan context manager pattern:

```python
# in src/main.py

from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Lifespan context manager for FastAPI application.
    This replaces the deprecated @app.on_event handlers.
    """
    # Startup logic
    logger.info("Starting up the ScraperSky API")

    # Start the domain processing scheduler
    scheduler = setup_domain_scheduler()
    logger.info("Domain processing scheduler started")

    yield  # This is where FastAPI runs and serves requests

    # Shutdown logic
    await session_manager.close()
    shutdown_domain_scheduler()
    logger.info("Domain processing scheduler shut down")

# Create FastAPI app with lifespan manager
app = FastAPI(
    title="ScraperSky API",
    description="API for ScraperSky web scraping and data management",
    version="3.0.0",
    debug=True,
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
    lifespan=lifespan  # Use the lifespan context manager
)
```

### Benefits

1. **Modern Pattern**: Uses the recommended approach in FastAPI for resource management
2. **Clearer Resource Lifecycle**: Explicitly shows the start and end of resources within a single context
3. **Better Error Handling**: Context managers provide better error handling for startup/shutdown operations
4. **Future Compatibility**: Ensures compatibility with future FastAPI versions

## 2. Configurable Scheduler Parameters

### Current Implementation

The domain scheduler has hardcoded values for several critical parameters:

```python
# Add job to process pending domains every 1 minute
job = scheduler.add_job(
    process_pending_domains,
    IntervalTrigger(minutes=1),
    id="process_pending_domains",
    replace_existing=True,
    kwargs={"limit": 10}  # Process up to 10 domains each time
)
```

### Proposed Change

These parameters should be made configurable through the Settings class:

```python
# in src/config/settings.py
class Settings(BaseSettings):
    # ... existing settings ...

    # Scheduler settings
    SCHEDULER_INTERVAL_MINUTES: int = 1
    SCHEDULER_BATCH_SIZE: int = 10
    SCHEDULER_MAX_INSTANCES: int = 1

# in src/services/domain_scheduler.py
def setup_domain_scheduler():
    """Set up the scheduler with the domain processing job"""
    logger.info("Setting up domain processing scheduler")

    # Use settings for scheduler configuration
    job = scheduler.add_job(
        process_pending_domains,
        IntervalTrigger(minutes=settings.SCHEDULER_INTERVAL_MINUTES),
        id="process_pending_domains",
        replace_existing=True,
        kwargs={"limit": settings.SCHEDULER_BATCH_SIZE}
    )

    # ... rest of the function ...
```

### Benefits

1. **Environment-specific Configuration**: Allows different settings for development, testing, and production
2. **Tunable Performance**: Enables performance tuning without code changes
3. **Testing Flexibility**: Makes it easier to test with different scheduling parameters

## 3. Enhanced Telemetry and Monitoring

### Current Implementation

The current implementation has basic logging but lacks structured telemetry data about scheduler performance:

```python
# Log completion statistics
logger.debug("--------------------------------------------------")
logger.debug(f"DOMAIN PROCESSING JOB {job_id} COMPLETE")
logger.debug(f"Processed: {domains_processed} domains, Successful: {domains_successful}")
logger.debug("--------------------------------------------------")
```

### Proposed Change

Implement structured telemetry with timing information and standardized metrics:

```python
import time
from dataclasses import dataclass
from typing import Dict, Any

@dataclass
class SchedulerMetrics:
    job_id: str
    start_time: float
    end_time: float
    domains_processed: int
    domains_successful: int
    domains_failed: int
    domains_skipped: int

    @property
    def duration_seconds(self) -> float:
        return self.end_time - self.start_time

    @property
    def success_rate(self) -> float:
        return self.domains_successful / self.domains_processed if self.domains_processed > 0 else 0.0

    def to_dict(self) -> Dict[str, Any]:
        return {
            "job_id": self.job_id,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(self.end_time)),
            "duration_seconds": self.duration_seconds,
            "domains_processed": self.domains_processed,
            "domains_successful": self.domains_successful,
            "domains_failed": self.domains_failed,
            "domains_skipped": self.domains_skipped,
            "success_rate": self.success_rate
        }

async def process_pending_domains(limit: int = 10):
    """Process pending domains with enhanced telemetry."""
    start_time = time.time()
    job_id = f"domain_batch_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"

    # Initialize metrics
    metrics = SchedulerMetrics(
        job_id=job_id,
        start_time=start_time,
        end_time=0,
        domains_processed=0,
        domains_successful=0,
        domains_failed=0,
        domains_skipped=0
    )

    # ... existing processing code ...

    # Update metrics at the end
    metrics.end_time = time.time()
    metrics.domains_processed = domains_processed
    metrics.domains_successful = domains_successful
    metrics.domains_failed = domains_processed - domains_successful

    # Log structured metrics
    logger.info(f"DOMAIN PROCESSING JOB METRICS: {json.dumps(metrics.to_dict())}")

    # Store metrics for monitoring (could be in database, file, or monitoring service)
    await store_scheduler_metrics(metrics)
```

### Benefits

1. **Performance Monitoring**: Enables tracking scheduler performance over time
2. **Better Debugging**: More detailed metrics help identify issues
3. **Operational Insights**: Provides data for capacity planning and optimization
4. **Monitoring Integration**: Structured data can be easily integrated with monitoring systems

## Implementation Priority

1. **High**: FastAPI Lifespan Migration (1-2 days)
2. **Medium**: Configurable Scheduler Parameters (1 day)
3. **Medium**: Enhanced Telemetry and Monitoring (2-3 days)

## References

- [FastAPI Lifespan Documentation](https://fastapi.tiangolo.com/advanced/events/#lifespan-events)
- [APScheduler Documentation](https://apscheduler.readthedocs.io/en/stable/)
- `Docs/Docs_1_AI_GUIDES/21-SCHEDULED_TASKS_APSCHEDULER_PATTERN.md`
- `project-docs/11-Background-Task-Scheduler/11.7-DOMAIN-SCHEDULER-BUGFIX-IMPLEMENTATION.md`
