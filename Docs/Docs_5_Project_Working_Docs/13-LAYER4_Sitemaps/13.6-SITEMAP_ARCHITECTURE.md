# Sitemap Processing Architecture

## System Overview

The sitemap processing system follows a **Producer-Consumer Pattern** with clear separation of concerns. The system is designed to handle both single and batch domain processing with optimal resource utilization and resilience.

## Architecture Patterns

### Producer-Consumer Pattern

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   Producer  │────▶│    Queue    │────▶│  Consumer   │
│  (API/HTTP) │     │ (Job Table) │     │ (Scheduler) │
└─────────────┘     └─────────────┘     └─────────────┘
                                               │
                                               ▼
                                        ┌─────────────┐
                                        │  Processor  │
                                        │ (Analyzer)  │
                                        └─────────────┘
                                               │
                                               ▼
                                        ┌─────────────┐
                                        │  Database   │
                                        │   Storage   │
                                        └─────────────┘
```

## Component Responsibilities

### 1. Producer (API Layer)

- **Purpose**: Accept API requests and queue work
- **Primary File**: `src/routers/modernized_sitemap.py`
- **Responsibilities**:
  - Validates requests
  - Creates job entries in job table
  - Sets initial status as "pending"
  - Returns job ID to client immediately
  - Does not perform actual processing

### 2. Queue (Job Table)

- **Purpose**: Store work to be done
- **Primary Files**:
  - `src/models/job.py`
  - `src/services/job_service.py`
- **Responsibilities**:
  - Maintains job status (pending, processing, completed, error)
  - Tracks progress
  - Stores result data
  - Acts as the communication medium between components

### 3. Consumer (Scheduler)

- **Purpose**: Detect work and trigger processing
- **Primary File**: `src/services/sitemap_scheduler.py`
- **Responsibilities**:
  - Polls job table for pending entries
  - Triggers the processor for each job
  - Simple "detect and delegate" role
  - Runs on a defined interval (default: 5 minutes)

### 4. Processor (Analyzer)

- **Purpose**: Perform the actual work
- **Primary Files**:
  - `src/services/sitemap/processing_service.py`
  - `src/services/sitemap/background_service.py`
  - `src/services/sitemap/analyzer_service.py`
- **Responsibilities**:
  - Executes sitemap discovery and processing
  - Performs all database operations
  - Updates job status
  - Handles errors
  - Operates independently once triggered

### 5. Storage (Database)

- **Purpose**: Persistent storage of results
- **Primary Files**:
  - `src/models/domain.py`
  - `src/models/sitemap.py`
- **Responsibilities**:
  - Stores domains
  - Stores sitemaps
  - Stores URLs
  - Maintains relationships between entities

## Complete Dependency Tree

```
modernized_sitemap.py
├── processing_service.py
│   ├── background_service.py
│   │   ├── analyzer_service.py
│   │   │   └── sitemap_analyzer.py
│   │   ├── job_service.py
│   │   │   └── models/job.py
│   │   └── models/domain.py
│   │       └── models/sitemap.py
│   └── session/async_session.py
└── job_service.py

sitemap_scheduler.py
├── processing_service.py
│   ├── (same dependencies as above)
└── session/async_session.py
```

## Complete File Inventory

1. **API Layer**

   - `src/routers/modernized_sitemap.py` - HTTP endpoints for sitemap processing

2. **Service Layer**

   - `src/services/sitemap/processing_service.py` - Core processing logic
   - `src/services/sitemap/background_service.py` - Background processing tasks
   - `src/services/sitemap/analyzer_service.py` - Sitemap analysis service
   - `src/services/job_service.py` - Job tracking service
   - `src/services/sitemap_scheduler.py` - Scheduler service

3. **Model Layer**

   - `src/models/job.py` - Job model
   - `src/models/domain.py` - Domain model
   - `src/models/sitemap.py` - Sitemap and SitemapUrl models

4. **Utility Layer**

   - `src/session/async_session.py` - Database session management
   - `src/scraper/sitemap_analyzer.py` - Sitemap analysis utilities
   - `src/scraper/domain_utils.py` - Domain utilities

5. **Scripts**
   - `scripts/sitemap_scheduler/add_test_sitemap.py` - Add test sitemaps
   - `scripts/sitemap_scheduler/check_sitemap.py` - Check processing status
   - `scripts/sitemap_scheduler/process_sitemap.py` - Process specific sitemap
   - `scripts/sitemap_scheduler/monitor_scheduler.py` - Monitor scheduler

## Data Flow

### 1. API Request Flow

```
[Client] ───▶ [modernized_sitemap.py] ───▶ [Create Job Entry]
                                                   │
                                                   ▼
[API Response] ◀─── [Return Job ID] ◀─── [Set Status="pending"]
```

### 2. Scheduler Flow

```
[Scheduler Timer] ───▶ [sitemap_scheduler.py] ───▶ [Query Pending Jobs]
                                                            │
                                                            ▼
                                                   [For Each Pending Job]
                                                            │
                                                            ▼
                                              [Set Status="processing"]
                                                            │
                                                            ▼
                                               [Call process_domain_with_own_session]
```

### 3. Processing Flow

```
[process_domain_with_own_session] ───▶ [analyzer_service.py] ───▶ [Analyze Sitemaps]
                                                                          │
                                                                          ▼
                                                                  [Process Results]
                                                                          │
                                                                          ▼
                                                                [Store in Database]
                                                                          │
                                                                          ▼
                                                              [Set Status="completed"]
```

## Key Implementation Details

### 1. Job Table Schema

```sql
CREATE TABLE jobs (
    id UUID PRIMARY KEY,
    job_type VARCHAR(50),
    status VARCHAR(20),
    progress FLOAT,
    result_data JSONB,
    error_message TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

### 2. Process Domain Function

```python
async def process_domain_with_own_session(job_id: str, domain: str, user_id: Optional[str] = None, max_urls: int = 100):
    """Process domain with its own dedicated session for background task reliability."""
    # Create session
    async with get_background_session() as session:
        # Update job status to processing
        async with session.begin():
            await job_service.update_status(
                session=session,
                job_id=job_id,
                status="processing",
                progress=0.1
            )

        # Initialize analyzer and process domain
        analyzer = SitemapAnalyzer()
        result = await analyzer.analyze_domain_sitemaps(...)

        # Store results and update job status
        await store_domain_data(...)

        # Update job as completed
        async with session.begin():
            await job_service.update_status(
                session=session,
                job_id=job_id,
                status="completed",
                progress=1.0,
                result_data=result
            )
```

### 3. Scheduler Implementation

```python
async def process_pending_sitemaps(limit: int = 10):
    """Process pending sitemaps that have been queued for processing."""
    # Query for pending jobs
    async with get_background_session() as session:
        async with session.begin():
            # Get pending jobs
            pending_jobs = await job_service.get_pending_jobs(session, limit=limit)

    # Process each job
    for job in pending_jobs:
        job_id = job.id
        domain = job.domain  # Assuming domain is stored in job data

        # Process domain using existing function
        await process_domain_with_own_session(
            job_id=job_id,
            domain=domain,
            user_id="scheduler",
            max_urls=1000
        )
```

## Integration Requirements

For the scheduler to properly integrate with the existing system:

1. **Session Management**:

   - Use `get_background_session()` for all database operations
   - Follow transaction boundary best practices

2. **Job Management**:

   - Query for pending jobs
   - Pass job_id to processor
   - Let processor update all status information

3. **Error Handling**:
   - Catch and log exceptions
   - Don't update job status directly - let processor handle it
   - Use separate sessions for each operation

## Conclusion

The sitemap processing system follows a clean producer-consumer architecture with clear separation of concerns. This pattern ensures:

1. **Scalability**: The system can handle increasing load by queuing work
2. **Resilience**: Failures in processing don't affect the API or other jobs
3. **Maintainability**: Each component has a single responsibility
4. **Efficiency**: Resources are used optimally with background processing

The implementation is complete except for the final integration of the scheduler component, which needs to be connected to process the pending jobs in the queue.
