# Work Order: Debugging and Testing Domain Sitemap Submission Scheduler

**Document ID:** 18.3-DEBUGGING-TESTING
**Status:** In Progress
**Created:** April 2025
**Author:** Gemini Assistant
**Related Work Orders:** `18.1-Domain-to-Sitemap.md`, `18.2-Implementation-Details.md`
**Related Standards:**

- `README.md` - Project setup and standards
- `Docs/Docs_1_AI_GUIDES/01-ABSOLUTE_ORM_REQUIREMENT.md` - Mandatory ORM usage
- `docker-compose.yml` - Service configuration
- `.env` - Environment variables

## 1. Goal

Debug and verify the functionality of the Domain Sitemap Submission scheduler, ensuring it correctly processes domains marked for sitemap analysis while adhering to project standards for ORM usage and transaction management.

## 2. Debugging Steps Taken

1.  **Initial Hypothesis (Incorrect):** Suspected missing environment variables (`DOMAIN_SITEMAP_SCHEDULER_INTERVAL_MINUTES`, `DOMAIN_SITEMAP_SCHEDULER_BATCH_SIZE`) were causing the scheduler setup to fail silently or immediately error out.
    - **Verification:** Checked `src/config/settings.py`. Confirmed Pydantic model defines defaults (1 min, 10 batch size).
    - **Conclusion:** Missing environment variables would not cause immediate failure; defaults would be used. The issue lies elsewhere.
2.  **Code Review:** Examined `src/services/domain_sitemap_submission_scheduler.py`.
    - **Verification:** Confirmed logger (`logging.getLogger(__name__)`) is initialized correctly. Reviewed the `process_pending_domain_sitemap_submissions` function logic, including database queries, status updates, adapter service calls, and error handling within the `async with get_background_session()` block.
    - **Observation:** Found `DEBUG` level logging statements intended to track job execution.
3.  **Hypothesis: Stale Code/Container Desync:** The presence of debug code not reflected in runtime logs suggested the running `scrapersky` container might be using an older image without the latest changes.
    - **Action:** Forced a container rebuild and restart.
    - **Command:** `docker-compose down && docker-compose up --build -d scrapersky` (Uses project's `docker-compose.yml` configuration)
    - **Result:** Container rebuilt and restarted successfully. Logs showed application startup, including the `Domain Sitemap Submission` scheduler initializing and scheduling its first run.
4.  **Log Monitoring & Timing Issues:** Repeatedly checked logs after the rebuild.
    - **Commands:** `docker-compose logs --tail=150 scrapersky | cat`, `docker-compose logs --tail=300 scrapersky | cat` (Uses project's `docker-compose.yml` service definitions)
    - **Observation:** Logs confirmed scheduler setup and the timestamp for the _next scheduled run_ (e.g., `Next run: 2025-04-08 12:06:49.397406+00:00`). However, the log output consistently cut off _before_ this timestamp, meaning the actual job execution logs were still not visible. This suggests a potential delay in log propagation or insufficient log tail size initially requested.

## 3. Current Status

- The `scrapersky` service is running with the latest code after a forced rebuild (managed via `docker-compose.yml`)
- Logs confirm the `Domain Sitemap Submission` scheduler (`process_pending_domain_sitemap_submissions` job) is configured with:
  - Interval: 1 minute (from `DOMAIN_SITEMAP_SCHEDULER_INTERVAL_MINUTES` in `.env`/`docker-compose.yml`)
  - Batch Size: 10 (from `DOMAIN_SITEMAP_SCHEDULER_BATCH_SIZE` in `.env`/`docker-compose.yml`)
  - These settings follow the environment variable conventions documented in `README.md`
- The scheduler is running and has scheduled its next execution
- The primary remaining task is to verify that the job runs as expected, processes domains marked with `sitemap_curation_status='Selected'`, calls the adapter service, and updates the `sitemap_analysis_status` in the database correctly. This requires capturing logs from the actual job execution timeframe.

## 3.5 Code Changes Implemented (for `src/services/domain_sitemap_submission_scheduler.py`)

Based on review, the `process_pending_domain_sitemap_submissions` function was modified to align with the transaction handling strategy outlined in `18.2-Implementation-Details.md`. The key changes implemented are:

1.  **Single Transaction Context:** The entire batch processing logic is wrapped in a single `async with get_background_session() as session:` block. This ensures that all database operations within a single scheduler run are part of one atomic transaction.
2.  **ORM Query with Locking:** The query to fetch domains uses `select(Domain)...with_for_update(skip_locked=True)` to ensure that domains being processed are locked and skipped by concurrent scheduler runs (if any, although `max_instances=1` is standard).
3.  **In-Memory Status Updates:**
    - Before calling the adapter service for a specific domain, its status is updated _in memory_ to `processing` (`domain.sitemap_analysis_status = SitemapAnalysisStatusEnum.processing`).
    - The `Adapter Service` (`DomainToSitemapAdapterService.submit_domain_to_legacy_sitemap`) is responsible for updating the status _in memory_ to `submitted` or `failed` based on the outcome of its internal API call. The adapter **does not commit** the session.
4.  **No Commits within Loop:** There are no explicit `session.commit()` or `session.rollback()` calls inside the loop that iterates through the fetched domains.
5.  **Implicit Batch Commit/Rollback:**
    - If the `for` loop completes without any exceptions being raised, the `async with` context manager automatically commits the session, persisting all the in-memory status changes for the entire batch.
    - If any exception occurs during the loop (either directly in the loop's logic or within the adapter service call), the exception is caught, the specific domain is marked `failed` _in memory_, the exception is **re-raised**, and this causes the `async with` context manager to automatically roll back the entire transaction, discarding all changes made during that batch run.

This approach ensures batch atomicity and strictly adheres to the project's mandatory ORM and transaction management standards as defined in `Docs/Docs_1_AI_GUIDES/01-ABSOLUTE_ORM_REQUIREMENT.md`. The implementation follows the project's established patterns for:

- Using SQLAlchemy ORM exclusively (no raw SQL)
- Managing transactions through context managers
- Handling database sessions via `get_background_session()`
- Implementing proper error handling and rollbacks

## 3.6 Investigation and Resolution: SQLAlchemy Pooling Strategy Conflict (Supplemental - 2025-04-08)

**Problem:** Steps 2 and 3 of the Test Plan were blocked by a persistent `sqlalchemy.exc.NoReferencedTableError: Foreign key associated with column 'domains.local_business_id' could not find table 'local_businesses'`. This error occurred during the `session.commit()` phase when using ORM scripts (`temp_update_domain.py`) relying on `get_background_session`.

**Investigation:**

1.  **Table Existence Confirmed:** Initial hypothesis was a missing `local_businesses` table. However, using the direct `psycopg2` connection script (`scripts/db/simple_inspect.py`) confirmed the `public.local_businesses` table _does_ exist in the target database with the correct `id` column.
2.  **Connection Strategy Discrepancy:** Analysis revealed a key difference in database connection methods (as documented in `scripts/db/CONNECTION_STRATEGY_GUIDE.md`):
    - `simple_inspect.py`: Uses a direct, synchronous `psycopg2` connection, bypassing application-level SQLAlchemy engine configuration.
    - ORM Scripts (`temp_*.py`) / Runtime App: Use the shared asynchronous engine configured in `src/db/engine.py` via `get_background_session`. This engine connects through the Supavisor pooler (Transaction Mode, port 6543) and uses specific compatibility flags.
3.  **Root Cause Identified:** Inspection of `src/db/engine.py` showed the async engine is configured with `raw_sql=True`, `no_prepare=True`, `statement_cache_size=0` (required for Supavisor Transaction Mode) but uses SQLAlchemy's default `QueuePool` (implicitly or explicitly configured with `pool_size`, `max_overflow`).
4.  **Documentation Conflict:** Supabase documentation ("Using SQLAlchemy with Supabase" [[Source]](https://supabase.com/docs/guides/troubleshooting/using-sqlalchemy-with-supabase-FUqebT)) explicitly recommends using `poolclass=NullPool` in SQLAlchemy when connecting via the **Transaction Mode pooler (port 6543)**. Using `QueuePool` likely conflicts with Supavisor's transaction pooling, potentially causing issues with ORM metadata reflection or foreign key validation during `flush`, leading to the observed `NoReferencedTableError`. The necessity of using the pooler for deployment environments like Render.com [[Source]](https://supabase.com/docs/guides/database/connecting-to-postgres) was also noted, making the correct pooler configuration critical. Context from developer discussions (e.g., [[Source]](https://www.reddit.com/r/Supabase/comments/1fan2ka/supabase_rejecting_conenctions_from_rendercom/)) also highlights complexities at this interface.

**Proposed Solution:**

Align SQLAlchemy configuration with Supabase best practices for Transaction Mode pooling by switching to `NullPool`. This involves modifying `src/db/engine.py`.

**Implementation Steps:**

1.  **Backup:** Create a backup copy of the existing `src/db/engine.py` file before modification.
2.  **Modify `src/db/engine.py`:**
    - Import `NullPool` from `sqlalchemy.pool`.
    - Add `poolclass=NullPool` to the `create_async_engine` parameters.
    - Remove `pool_size`, `max_overflow`, and `pool_recycle` parameters, as they are inapplicable to `NullPool`.
3.  **Document:** Create/Update an AI Guide (e.g., `Docs/Docs_1_AI_GUIDES/24-SQLALCHEMY_POOLING_AND_SUPAVISOR.md` or update `07-DATABASE_CONNECTION_STANDARDS.md`) detailing the mandatory use of `NullPool` with Supavisor Transaction Mode, referencing documentation and explaining the rationale.

**Immediate Testing Plan (Post-Modification):**

Before resuming the main test plan (Section 4), perform these checks:

1.  **Application Startup:** Verify the FastAPI application starts without errors using `docker-compose up scrapersky` (or similar). Check startup logs.
2.  **Health Check:** Confirm the `/health` endpoint responds successfully.
3.  **Error Resolution:** Re-run the previously failing ORM script: `python temp_update_domain.py e508f8de-16d0-4759-ae2d-8c2a91b6d742`. Verify that the `NoReferencedTableError` is resolved and the script completes successfully (updating the domain status).

**Next Steps (Post-Testing):**

If the immediate tests pass, proceed with executing the original Test Plan detailed in Section 4. If new errors arise, further investigation will be required.

## 4. Test Plan: Domain Sitemap Submission Scheduler

**Objective:** Verify that the `Domain Sitemap Submission` scheduler correctly identifies domains ready for sitemap submission (`sitemap_curation_status='Selected'`, `sitemap_analysis_status='queued'`), processes them by calling the internal legacy API endpoint via the adapter service, and updates their `sitemap_analysis_status` to `submitted` or `failed` in the database, **adhering to all project standards.**

**Prerequisites:**

1.  Docker environment is up: `docker-compose ps` shows `scrapersky` service running (defined in `docker-compose.yml`)
2.  Database contains domains: At least one record exists in the `domains` table
3.  Development Token configured: The `DEV_TOKEN` environment variable must be correctly set in the `.env` file for the adapter service's internal API call to succeed
4.  **Project Standards Awareness:**
    - Understand the strict requirement for ORM-only database interaction (`Docs/Docs_1_AI_GUIDES/01-ABSOLUTE_ORM_REQUIREMENT.md`)
    - Review database helper scripts in `scripts/db/` as documented in `README.md`
    - Follow environment variable conventions from `README.md`

**Test Steps:**

| Step | Action                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Expected Result                                                                                                                                                                                                                                                                 | Actual Result                                                                                                                                                                                                                                                                                                                                                                                                                                        | Status    |
| :--- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------- | --------- |
| 1    | **Query Initial State (ORM ONLY):** <br> **DO NOT USE `psql`** <br> Use an appropriate method adhering to the ORM requirement: <br> a) **Preferred:** Create & run a temporary Python script using `get_background_session` and the `Domain` model to execute: `select(Domain.id, Domain.domain, Domain.sitemap_curation_status, Domain.sitemap_analysis_status).order_by(Domain.created_at.desc()).limit(1)` <br> b) **Alternative:** Use `scripts/db/simple_inspect.py domains` and manually identify the latest relevant domain if possible (less precise) <br> **Reference:** `README.md`, `01-ABSOLUTE_ORM_REQUIREMENT.md` | Output shows the `id`, `domain`, `sitemap_curation_status`, and `sitemap_analysis_status` for the most recently created domain. Note down the `id`.                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                      | ☐ Pending |
| 2    | **Mark Domain for Processing (ORM ONLY):** <br> **DO NOT USE `psql`** <br> Use the noted `id` from Step 1. <br> Create & run a temporary Python script using `get_background_session` and the `Domain` model to execute an ORM update equivalent to: `UPDATE domains SET sitemap_curation_status = 'Selected', sitemap_analysis_status = 'queued', updated_at = NOW() WHERE id = <domain_id_from_step_1>;`                                                                                                                                                                                                                      | Script executes successfully and confirms 1 row updated.                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                      | ☐ Pending |
| 3    | **Verify Queued State (ORM ONLY):** <br> **DO NOT USE `psql`** <br> Use the noted `id` from Step 1. <br> Create & run a temporary Python script using `get_background_session` and `Domain` model to execute: `select(Domain.sitemap_curation_status, Domain.sitemap_analysis_status).where(Domain.id == <domain_id_from_step_1>)`                                                                                                                                                                                                                                                                                              | Script executes and output shows `sitemap_curation_status='Selected'` and `sitemap_analysis_status='queued'` for the target domain.                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                      | ☐ Pending |
| 4    | **Monitor Logs:** <br> Wait ~70-90 seconds (adjust based on `DOMAIN_SITEMAP_SCHEDULER_INTERVAL_MINUTES`). <br> Run `docker-compose logs --tail=500 scrapersky                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | cat`                                                                                                                                                                                                                                                                            | Logs from `src.services.domain_sitemap_submission_scheduler` appear, showing: <br> - Job start message. <br> - Found 1 domain(s) with status 'queued'. <br> - "Processing domain <id>..." log. <br> - "Updating status to 'processing' in memory..." log. <br> - Logs from `DomainToSitemapAdapterService` attempting `POST /api/v3/sitemap/scan`. <br> - Adapter success/failure log. <br> - Scheduler batch commit log. <br> - Job completion log. |           | ☐ Pending |
| 5    | **Verify Final Domain Status (ORM ONLY):** <br> **DO NOT USE `psql`** <br> Use the noted `id` from Step 1. <br> Create & run a temporary Python script using `get_background_session` and `Domain` model to execute: `select(Domain.sitemap_curation_status, Domain.sitemap_analysis_status, Domain.sitemap_analysis_error).where(Domain.id == <domain_id_from_step_1>)`                                                                                                                                                                                                                                                        | Script executes and output shows: <br> **If API call succeeded:** `sitemap_analysis_status='submitted'`, `sitemap_analysis_error=NULL`. <br> **If API call failed (e.g., bad DEV_TOKEN):** `sitemap_analysis_status='failed'`, `sitemap_analysis_error` contains error message. |                                                                                                                                                                                                                                                                                                                                                                                                                                                      | ☐ Pending |
| 6    | **Verify Legacy Job Creation (ORM ONLY - Optional):** <br> **DO NOT USE `psql`** <br> Create & run a temporary Python script using `get_background_session` and the `Job` model (assuming it exists in `src/models/job.py` or similar) to execute: `select(Job).where(Job.job_type == 'sitemap_scan').order_by(Job.created_at.desc()).limit(1)`                                                                                                                                                                                                                                                                                 | Script executes and output shows a `Job` record with `job_type='sitemap_scan'`. The `parameters['base_url']` should match the domain from Step 1.                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                      | ☐ Pending |

## 5. Test Execution

_(This section will be filled in as the test plan steps are executed)_

---

**(Work Order Updated)**
