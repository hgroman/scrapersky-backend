**Summary of Current Issue & Immediate Plan:**

**Core Problem:**
The term "deep_scrape" remains in database columns, model attributes, and code variables despite renaming the workflow to "Sitemap Import". This creates confusion and inconsistency.

**Work Order to Fix Naming (Linear Execution):**

| **Step** | **Action**                  | **Files/Components** | **Details** |
| -------- | --------------------------- | -------------------- | ----------- |
| **1**    | **Rename Database Columns** | PostgreSQL Database  |
|          |                             |                      | **SQL:**    |

```sql
ALTER TABLE sitemap_files RENAME COLUMN deep_scrape_process_status TO sitemap_import_status;
ALTER TABLE sitemap_files RENAME COLUMN deep_scrape_error TO sitemap_import_error;
```

**Note:** Execute this directly via `psql` or GUI tool.

| **2** | **Rename Enum Type (Optional)** | PostgreSQL Database |
| | | | **SQL (If Renaming Enum):**

```sql
ALTER TYPE deep_scan_status_enum RENAME TO sitemap_import_status_enum;
```

| **3** | **Update Model Attributes** | `src/models/sitemap.py` |
| | | | Change:

```python
# BEFORE:
deep_scrape_process_status = Column(...)
deep_scrape_error = Column(...)

# AFTER:
sitemap_import_status = Column(SQLAlchemyEnum(SitemapDeepProcessStatusEnum), name="sitemap_import_status")
sitemap_import_error = Column(Text, name="sitemap_import_error")
```

| **4** | **Update Service Code** | `src/services/sitemap_import_service.py` |
| | | | Replace all instances of `deep_scrape_process_status` with `sitemap_import_status` and `deep_scrape_error` with `sitemap_import_error`.

| **5** | **Update Scheduler Code** | `src/services/sitemap_import_scheduler.py` |
| | | | Replace references to old status/error fields. Example:

```python
# BEFORE:
status_field_name="deep_scrape_status"

# AFTER:
status_field_name="sitemap_import_status"
```

| **6** | **Update Logs/Comments** | All affected files |
| | | | Search for "deep_scrape" in codebase and replace with "sitemap_import" in log messages, comments, or variables.

| **7** | **Restart Services** | Docker |
| | | | Run:

```bash
docker compose restart app scrapersky
```

| **8** | **Verify Fix** | Terminal & Database |
| | | | **1.** Re-run CURL test.
**2.** Check `sitemap_files.sitemap_import_status` in DB.
**3.** Confirm logs use "sitemap_import" terminology.

**Critical Files to Review:**

- `src/models/sitemap.py`
- `src/services/sitemap_import_service.py`
- `src/services/sitemap_import_scheduler.py`
- Database schema (`sitemap_files` table).

**Time Estimate:** 30 minutes (if no conflicts).

**Urgent Next Step:**
Execute Step 1 (SQL) immediately, followed by Steps 3-6. Iâ€™ll guide you through each line of code or SQL command if needed.
