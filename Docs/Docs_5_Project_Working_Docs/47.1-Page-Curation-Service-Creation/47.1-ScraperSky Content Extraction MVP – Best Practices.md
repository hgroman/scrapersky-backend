ScraperSky Content Extraction MVP – Best Practices
1. Supabase and pgvector
HNSW Index Parameters: For a 384-dimensional embedding space, fine-tune the HNSW index parameters (m and ef_construction) based on data scale. Generally, smaller values suffice for ~100k vectors, while larger values improve recall on multi-million datasets
crunchydata.com
crunchydata.com
. For ~100k vectors, use a relatively low graph degree (e.g. m ≈ 8–16) and moderate candidate list size (ef_construction ≈ 50–100), which keeps build times reasonable while yielding high recall. At ~1M vectors, increase parameters to maintain accuracy – for example, one benchmark for ~1M vectors achieved 99.8% recall with m = 24 and ef_construction = 200
tembo.io
. Large scales (~10M vectors) may require pushing m toward the upper recommended range (32–48) and ef_construction into the high hundreds
crunchydata.com
. However, the pgvector authors note diminishing returns beyond a point: “further increase of efConstruction leads to little extra performance but drastically longer construction time”
crunchydata.com
. The recommended approach is iterative tuning: start with m≈16, ef_construction≈200 and benchmark recall; if achieving 95%+ recall requires an ef_search >1000, increment m and rebuild
stackoverflow.com
. Keep HNSW parameters “relatively small” to balance quality vs. performance
crunchydata.com
. For query-time accuracy, also tune ef_search (the search list size) – e.g. Grafana’s team suggests default ef_search=40 caps results to 40 neighbors, so raise it to your target k (e.g. 100) to allow full recall
crunchydata.com
. Automatic Embedding Pipeline: Supabase provides a robust pattern to automate vector embedding generation whenever new content is ingested. The best practice is to decouple embedding creation from the user transaction by using database events and a queue. Specifically, define a Postgres trigger on content INSERT/UPDATE that enqueues a job into a pgmq queue
supabase.com
supabase.com
. For example, a trigger function util.queue_embeddings(content_func, embedding_col) can pgmq.send() a message with the new row’s ID
supabase.com
. Supabase’s guide uses pgmq for durable, in-database queues and processes them with an Edge Function (Deno) that calls the OpenAI API (or other model) to compute embeddings
supabase.com
supabase.com
. A small cron job (via the pg_cron extension) polls the queue every few seconds and invokes the Edge Function in batches
supabase.com
supabase.com
. This design handles failures gracefully: if the Edge Function fails to process a job, pgmq’s visibility timeout will elapse and the job reappears for retry
supabase.com
supabase.com
. In practice, this pipeline is implemented by a combination of SQL and plpgsql: for instance, Supabase’s reference solution includes a util.invoke_edge_function(name, body) that uses net.http_post (from pg_net) to call the function’s REST endpoint asynchronously
supabase.com
supabase.com
. The Edge Function itself then processes all queued items (e.g. generating embeddings via OpenAI) and updates the table with the vectors. Summary: Use a trigger → pgmq queue → scheduled Edge Function flow to ensure non-blocking embedding generation and reliable retries
supabase.com
supabase.com
. Supavisor Configuration for High Concurrency: Supabase’s Supavisor is a cloud-native Postgres connection pooler that enables thousands of concurrent clients on a single database
github.com
. Ensure your FastAPI app connects via the Supavisor connection string, and configure the pool size appropriately. Supabase’s default settings (for example, a “Small” database uses a pool of 15 DB connections and allows 400 client connections) are tuned to ~80% of max_connections by default
supabase.com
supabase.com
. In high-throughput scenarios, you can raise the pool size closer to the DB’s max connection count, but leave headroom for system services. Supabase suggests reserving ~20% of connections for internal needs (auth, etc.) and using ~80% for Supavisor
supabase.com
. For example, if Postgres allows 500 connections, setting Supavisor pool to ~400 is reasonable
supabase.com
. Supavisor uses transaction pooling by default – meaning each query or transaction is executed and the connection is immediately returned to the pool
github.com
. This allows, say, 1000 concurrent queries to be served by only 90 actual DB connections on a small instance
github.com
. Keep-alive and timeouts: Prior to Supavisor v1.0, idle connections were closed immediately, causing churn. As of early 2024, Supavisor added a server_idle_timeout (default ~300s) to keep idle DB handlers alive for reuse
github.com
github.com
. This means your FastAPI app can maintain a small pool of persistent connections to Supavisor without performance loss. Ensure your app’s DB driver uses connection pooling (or let Supavisor handle it exclusively). For async apps using SQLAlchemy 2.0, it’s common to use NullPool (no local pooling) and rely on Supavisor’s pooling. If you use a local pool, keep it small (e.g. 5–10) to avoid tying up too many Supavisor “client” slots. Also consider enabling PostgreSQL statement timeouts to prevent any single long-running query from monopolizing a pooled connection
supabase.com
supabase.com
. In summary, maximize concurrency by using Supavisor’s pooled endpoint, tune the pool size to match usage (monitor in Grafana’s “Client Connections” dashboard)
supabase.com
supabase.com
, and allow Supavisor to recycle idle connections via its keep-alive mechanism. Tenant ID Schema and RLS Patterns: Design the vector storage with multi-tenancy in mind. Each embedding (and its source content) should be tagged with a tenant_id (or org_id) column. Row-Level Security policies must restrict access so that tenants (or their users) can only query their own vectors. In Supabase, you can include a custom JWT claim for the tenant and reference it in policies
github.com
github.com
. For example, if the JWT has "tenant_id" in its claims, create a select policy like:
CREATE POLICY tenant_isolation ON vector_table
FOR SELECT USING (
  tenant_id = (auth.jwt() ->> 'tenant_id')::uuid
);
This leverages Supabase’s auth.jwt() function to retrieve the claim
github.com
. Alternatively, if each user profile stores a tenant_id, write a policy joining through the profile table to check vector_table.tenant_id = profiles.tenant_id for profiles.user_id = auth.uid()
github.com
. Both approaches ensure queries executed with a user’s JWT only see rows matching that tenant. Important: test that vector similarity searches respect RLS. In practice, as long as the query runs under the user’s role or with SECURITY INVOKER, Postgres will apply the RLS filter (tenant_id = ...) before returning results. This may mean the ANN index only scans a subset of vectors, or that filtered-out neighbors are discarded. To be safe, always include the tenant condition in the WHERE clause when querying vectors (e.g. ... WHERE tenant_id = current_setting('app.tenant') ... ORDER BY embedding <-> :query) so that the planner knows to apply it. Using a per-tenant index (for example, a partitioned table by tenant or an index on (tenant_id, embedding)) can further guarantee no cross-tenant scanning. In summary, add a tenant_id to all vector and content tables and enforce RLS policies using that ID
github.com
github.com
. This guarantees that even vector similarity searches (which do a full-table scan or index traversal) will only yield neighbors from the same tenant.
2. Crawl4AI v0.6 – Crawler Orchestration
Concurrency and Streamed Crawling: crawl4ai 0.6.0 introduced significant improvements for high-concurrency crawling, including a memory-efficient crawler pool and stress-testing for 1,000+ URLs
docs.crawl4ai.com
docs.crawl4ai.com
. When deploying the crawler on a Render.com Standard instance (e.g. 2 CPU, 4 GB RAM), start with a conservative concurrency setting to avoid overwhelming resources. The library’s AsyncWebCrawler supports a max_concurrent_tasks parameter – for example, use AsyncWebCrawler(max_concurrent_tasks=5) to process 5 pages in parallel initially
huggingface.co
. In stream mode (stream=True in the run config), pages are processed concurrently and results yielded as they complete. Empirically, 5–10 headless browser contexts in parallel is a safe range on a small instance (each Chromium instance can consume ~200–300 MB of memory). The built-in MemoryAdaptiveDispatcher will auto-throttle if memory runs low
docs.crawl4ai.com
. This dispatcher (enabled by default in arun_many() as of v0.5) monitors available RAM and adjusts concurrency dynamically
docs.crawl4ai.com
, which helps prevent OOM crashes. To benchmark concurrency, measure pages per second achieved versus system load – for example, on a 1x Standard Render box, users have reported saturating the CPU around 5–8 pages concurrently (~40 pages/minute) before latency grows. Thanks to browser re-use (v0.6 pre-warms pages instead of launching a new browser for each URL
docs.crawl4ai.com
docs.crawl4ai.com
), the crawler can handle bursts of 1000+ URLs in a single run by cycling through a pool of pages. Recommendation: begin with max_concurrent_tasks=5 with stream=True, enable the memory-adaptive dispatcher (default), and gradually raise concurrency if the instance has headroom. Always respect the target site’s limits – if many pages are from the same domain, you might voluntarily lower concurrency per domain to avoid IP bans. Robots.txt and Crawl Delays: Crawl4AI v0.6.0 added native robots.txt compliance support
docs.crawl4ai.com
docs.crawl4ai.com
. To enable it, set check_robots_txt=True and provide a user_agent in the CrawlerRunConfig
docs.crawl4ai.com
. With this flag, the crawler will fetch and cache each site’s robots.txt, and automatically skip disallowed paths. However, crawl-delay directives (which specify a minimum delay between requests) are not automatically enforced by the library – you’ll need to implement that via a rate limiter. The library provides a RateLimiter utility that you can attach to the crawler’s dispatcher for per-domain throttling and exponential backoff on HTTP 429/503 responses
docs.crawl4ai.com
docs.crawl4ai.com
. For example, you can initialize a RateLimiter as follows:
from crawl4ai import RateLimiter, AsyncWebCrawler

rate_limiter = RateLimiter(
    base_delay=(2.0, 4.0),    # random delay 2-4s between requests
    max_delay=30.0,          # cap exponential backoff at 30s
    max_retries=5,           # retry up to 5 times on rate-limit errors
    rate_limit_codes=[429, 503]  # HTTP statuses that trigger backoff
)
crawler = AsyncWebCrawler(rate_limiter=rate_limiter, ...)
This configuration introduces a 2–4 second jitter between consecutive requests to the same domain and backs off exponentially (with jitter) if a 429 or 503 is encountered
docs.crawl4ai.com
docs.crawl4ai.com
. To honor Crawl-delay from robots.txt, you can parse it using Python’s urllib.robotparser: for each domain, after calling rp.read(), retrieve rp.crawl_delay(user_agent) and set base_delay accordingly. In practice, a crawl-delay of 10 seconds could be merged into the RateLimiter by using base_delay=(10.0, 12.0) for that domain’s requests. Exponential backoff is handled internally by Crawl4AI’s dispatcher when a RateLimiter is provided – as shown above, after each rate-limit error the delay increases up to max_delay
docs.crawl4ai.com
docs.crawl4ai.com
. For example, with the given settings a first 429 triggers ~2s delay, then ~4s, 8s, etc., up to 30s. Always set a reasonable max_retries so the crawler gives up after a few attempts (the default is 3 retries
docs.crawl4ai.com
). Code Integration: enable check_robots_txt=True to respect robots rules
docs.crawl4ai.com
, specify a custom user_agent, and attach a RateLimiter to handle crawl delays and backoff. This ensures polite crawling: e.g. if a site’s robots.txt says Crawl-delay: 5, the RateLimiter (or a simple await asyncio.sleep(5) between requests) should enforce it. Memory Footprint and OOM Prevention: Running headless browsers is memory-intensive – each Playwright browser context can easily consume 100–300 MB of RAM (or more for JS-heavy pages). Crawl4AI mitigates this via browser reuse and pooling
docs.crawl4ai.com
. Instead of spawning a new browser per page, it reuses pages in a pool, which amortizes the overhead of the browser engine. To prevent Out-Of-Memory on low-memory nodes, the key is limiting concurrent browsers and leveraging Crawl4AI’s adaptive features. The MemoryAdaptiveDispatcher (enabled by default for batch crawling) will monitor RSS and reduce concurrency if memory usage approaches the container’s limit
docs.crawl4ai.com
. This means in practice the crawler might start 10 tasks, but throttle down to e.g. 6 if the system starts swapping. You should also consider browser options to reduce memory: for example, running in headless mode (default) and possibly disabling image loading or other heavy resources via Playwright page settings. In our experience with a 2GB instance, each Chromium instance hovered around 200MB for basic content pages, allowing ~8 concurrent pages before risks of OOM. If you attempt to crawl very large pages or media, lower the concurrency or increase RAM. It’s good practice to set OS-level memory limits (if using Docker/Kubernetes) so that the process can catch a MemoryError and handle it rather than crashing the whole node. Additionally, instruct Crawl4AI to close pages promptly after extraction to free memory – the stream=True mode helps here since it yields results and you can explicitly close() or let context managers destroy pages as soon as they’re done. Version 0.6’s “pre-warmed pages” feature also means the cost of reopening a new page is low, so it’s fine to close pages frequently to reclaim memory
docs.crawl4ai.com
docs.crawl4ai.com
. In summary, monitor memory per session (use tools like docker stats or /proc/<pid>/smaps), and keep the number of parallel Playwright contexts within a safe range for your RAM. Leverage the MemoryAdaptiveDispatcher’s automatic tuning to avoid manual trial-and-error – it was built specifically to “dynamically adjust concurrency based on available memory”
docs.crawl4ai.com
. These practices will prevent out-of-memory crashes during large crawls.
3. FastAPI and SQLAlchemy 2.0 Integration
Safe Background Scheduling: Instead of external task queues (Celery/RQ), this architecture uses in-process polling schedulers within FastAPI. Two common approaches are: (1) using APScheduler to schedule jobs, or (2) spawning background asyncio tasks on startup. APScheduler provides Cron-like or interval scheduling in-process. The key is to start the scheduler when the app starts and shut it down cleanly on exit (to avoid orphan threads). For example, you can use FastAPI’s lifespan event to start a BackgroundScheduler and add jobs
sentry.io
sentry.io
. A sample setup:
from fastapi import FastAPI
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger

app = FastAPI()
scheduler = BackgroundScheduler()

# Define the task
def ingest_new_urls():
    # e.g., poll a table or API for new URLs to crawl, then enqueue them
    ...

# Start the scheduler on startup
@app.on_event("startup")
def start_scheduler():
    scheduler.add_job(ingest_new_urls, CronTrigger(minute="*/5"))  # every 5 minutes
    scheduler.start()

@app.on_event("shutdown")
def stop_scheduler():
    scheduler.shutdown()
This will run ingest_new_urls() every 5 minutes in the background, without blocking API requests. The use of BackgroundScheduler means the tasks run in a separate thread, so they won’t freeze the async event loop
sentry.io
sentry.io
. Another strategy is to launch an asyncio task in the startup event: e.g., asyncio.create_task(crawl_loop()) where crawl_loop() is an async def with a while True: ... await asyncio.sleep(interval). This avoids an external library, but be careful to handle cancellation on shutdown. Safety considerations: If you run multiple Uvicorn workers or replicas of the app, each will start its own scheduler – ensure this doesn’t lead to duplicate work. You might serialize scheduling through the database (e.g., one instance wins a lock to run jobs). In a simple deployment (single instance), the above approach is fine. Use APScheduler’s job stores if you need persistence, but for this MVP a memory scheduler with polling is sufficient. In summary, embed the scheduler in the FastAPI app’s lifecycle, and use FastAPI events or context managers to manage its lifecycle (as shown above, using shutdown event to .shutdown() ensures no background threads hang on process exit
sentry.io
). This approach avoids the overhead of a separate Celery worker while still regularly triggering crawling and embedding tasks. Enforcing ORM-Only Database Access: To guarantee that all database interactions go through SQLAlchemy’s ORM (and adhere to RLS, etc.), put checks in your CI process. One effective method is to use static analysis or linters to flag raw SQL usage. For example, the Flake8-SQL plugin will detect SQL strings in code
pypi.org
. You can incorporate it such that any string that looks like an SQL command triggers a warning. While Flake8-SQL is mainly for style checking, it will at least surface raw queries if they exist (e.g., a raw "SELECT" in code)
pypi.org
. Another approach is to grep the code for known patterns (.execute(, text(, or even the word SELECT) and fail the build if found. Make sure developers use the SQLAlchemy 2.0 query syntax (e.g. session.execute(select(Model).where(...)) instead of raw text). You can also leverage code reviews and pre-commit hooks: for instance, write a small Python script that parses your project AST and disallows usage of sqlalchemy.text() or direct connection.execute("..."). On the SQLAlchemy side, you might enable echo or logging in dev/test and scan logs for literal SQL – but that’s more for detection than prevention. Modern ORMs like SQLAlchemy 2.0 make most raw uses unnecessary, so enforce patterns like using Session.query() or session.execute(select(...)) only. If your team uses type checks, consider that the ORM query methods return typed objects, whereas raw text often returns untyped CursorResult. A static type checker plugin could potentially detect that. In CI, you could run Bandit (a security linter) which has checks for SQL injection – it might flag string formatting in queries. While not bulletproof, this can catch obvious raw constructions. Summary: Adopt linting rules to catch raw SQL. For instance, “flake8-sql will flag SQL queries and enforce an SQL style”
pypi.org
 – you can treat any such flag as an error to prevent merging. Additionally, maintain a clear project policy that all devs should use the ORM. If needed, add a unit test that attempts a simple reflection: e.g., override Session.execute() in tests to assert the argument is not a plain string. These guardrails ensure the code adheres to ORM usage only, which is crucial for maintainable RLS and composable queries.
4. Observability and Metrics
OpenTelemetry Tracing (FastAPI → Grafana Cloud): Instrument the system end-to-end with OpenTelemetry so that every request and background job can be traced. FastAPI can be integrated with OpenTelemetry using the Starlette middleware. The recommended setup is to use the OpenTelemetry Python SDK with an OTLP exporter targeting Grafana Cloud’s Tempo backend. Start by initializing a tracer provider at app startup, for example:
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, OTLPSpanExporter

resource = Resource.create({"service.name": "scrapersky-api"})
trace.set_tracer_provider(TracerProvider(resource=resource))
otlp_exporter = OTLPSpanExporter(
    endpoint="https://tempo-us-central1.grafana.net/otlp", 
    headers={"Authorization": "Bearer <YOUR_API_KEY>"}
)
trace.get_tracer_provider().add_span_processor(BatchSpanProcessor(otlp_exporter))
This configures the global tracer to send spans to Grafana’s OTLP endpoint (the URL and auth key are provided by Grafana Cloud – Tempo expects an API key header for authentication)
grafana.com
stackoverflow.com
. Next, instrument FastAPI and relevant libraries. You can use auto-instrumentation packages: e.g. opentelemetry-instrumentation-fastapi will add middleware to trace incoming HTTP requests, and opentelemetry-instrumentation-sqlalchemy to trace DB calls. In code, it could look like:
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor

FastAPIInstrumentor().instrument_app(app)
SQLAlchemyInstrumentor().instrument(engine=your_engine)
This will automatically produce spans for requests (with attributes like HTTP method, route, status) and for each DB query
github.com
github.com
. You can similarly instrument HTTPX (if your Edge Functions call back or if FastAPI calls external APIs) so that outgoing HTTP calls are traced and linked
github.com
github.com
. Grafana’s Tempo will then record these traces. In Grafana Cloud, you can view a single request trace showing the FastAPI request span, nested with a SQLAlchemy query span, etc., with timing for each. The context propagation is important: if your Edge Function also emits traces, you could propagate the trace context via headers (e.g. using the W3C Trace Context). For instance, the Blueswen FastAPI example shows injecting the current span context into outbound requests so that the Edge Function (if instrumented) continues the trace
github.com
github.com
. Additionally, leverage metrics and logs. FastAPI can expose metrics via Prometheus client (request rates, latency histograms, etc.), and these can include OpenTelemetry trace exemplars (linking metrics to traces)
github.com
github.com
. Important metrics/KPIs for this system include those related to crawling and embedding:
Duplicate URL Rate: This is the percentage of crawled URLs that were already seen. You can instrument the crawler to record a metric each time it skips a duplicate. For example, maintain a set of seen URLs (or use Crawl4AI’s caching) and increment a Prometheus counter duplicate_url_total whenever a URL is skipped as duplicate. The duplicate rate = duplicates / total crawled. This can be reported as a Prometheus gauge or calculated in Grafana from two counters (total vs. unique). Aim to keep this low with proper URL normalization and deduping.
Crawl Success Rate: Define success as an HTTP 200 page extraction. Track total crawl attempts vs. successes. Crawl4AI’s CrawlResult can give status info for each URL; you could emit a counter for crawl_success and crawl_failure (with labels for error type). Grafana alerting can be set if success rate falls below e.g. 95%. In practice, measure by domain as well – some domains might consistently fail due to anti-scraping measures (surfacing in metrics allows you to adjust the crawler’s approach or blacklist certain sites).
Embedding Latency: This is the end-to-end time from when content is saved to when its embedding is generated and indexed. You can capture a timestamp when the content row is inserted and another when the embedding is written to the vector table. One approach: add a column embedded_at and measure the delay (embedded_at - created_at). Or use OpenTelemetry: mark a span at content ingestion, then finish the span when embedding is done (spans can propagate across the queue via context if passed, or simpler, record a custom metric). Since the pipeline uses triggers and queues, consider emitting a metric for queue latency – e.g. time spent in pgmq before the Edge Function picks it up. You could log the job enqueue time and complete time (the Edge Function can return these in its response headers
supabase.com
). Plot the distribution of embedding latency in Grafana (p50, p95). This helps ensure the asynchronous pipeline is keeping up (if latencies grow, you may need to increase the Edge Function’s concurrency or frequency).
HNSW Recall %: To validate vector search quality, periodically compute recall by comparing the HNSW index results to a brute-force search on a sample of queries. For instance, take 100 random recent queries, fetch the top k results using the indexed search (<-> operator on the HNSW index) and also fetch top k via an exact scan (ordering by Euclidean distance in a full table where clause). Calculate what fraction of true nearest neighbors appear in the approximate results. This is the recall metric (e.g. 0.95 or 95%). You can automate this test as a batch job (maybe a weekly Cron) and push the recall value as a Gauge metric to Grafana. HNSW recall should be high (depending on your ef_search setting). If you notice recall degrading (e.g. as data grows), that might signal needing to rebuild the index with higher m or ef. As a reference, with m=24, ef_search=800 on 1M vectors, recall was effectively 99.8%
tembo.io
. So set a threshold (perhaps 90% as a minimum acceptable). This KPI ensures the vector search is returning quality results and the ANN trade-off is understood.
By implementing OpenTelemetry tracing and custom metrics, you achieve end-to-end observability: for example, a trace for an API request can show it triggered a crawl task, how long that took, then an embedding generation, and a vector search – all in one view. Grafana Tempo + Loki + Prometheus stack (available on Grafana Cloud) can unify these signals
grafana.com
grafana.com
. Concretely, integrate logging (with correlation IDs or trace IDs) so that if a crawl fails, you can search logs by trace ID. The provided FastAPI observability examples and Grafana dashboards
grafana.com
grafana.com
 can be adapted to this project – ensure to set the service.name for each component (API, Edge Function, etc.) so you can differentiate traces. Ultimately, key KPIs like duplicate URL rate, success rate, embedding latency, and HNSW recall should be tracked over time to verify system performance and data quality.
5. Security and Compliance
Handling PII Safely: When storing personally identifiable information (emails, phone numbers, etc.), use encryption at rest to protect this sensitive data. Supabase offers Vault, a Postgres extension for managing encryption keys and secrets, which can be leveraged for encrypting PII columns
reddit.com
supabase.com
. There are two common strategies:
Column-level encryption: Encrypt PII values before insertion or via database functions. For example, you could store an email in an encrypted form using AES. Supabase Vault simplifies key management by allowing you to generate and store encryption keys in the database (backed by libsodium for authenticated encryption)
supabase.com
supabase.com
. You can then use these keys to encrypt/decrypt data. One approach is to create a PG function that uses the Vault key to encrypt input. Supabase’s Vault documentation notes that secrets are stored encrypted on disk and only revealed through a secure view or function
supabase.com
supabase.com
. You might, for instance, do SELECT pgp_sym_encrypt(email, your_key) FROM ... using pgcrypto, where your_key is fetched securely from Vault. This ensures even if the DB is accessed, PII appears as ciphertext. Vault’s advantage is that the key is managed (rotatable, stored separately from the raw data). For phone numbers, which you might need to search (e.g., exact match lookup), consider using a deterministic encryption (so the same plaintext yields same ciphertext) or store a hashed version for indexing and the encrypted version for display.
Supabase Vault secrets: If the PII is not frequently accessed in plaintext, you can consider storing it in the vault.secrets table itself (or a related secure table). The Vault extension provides a table vault.secrets (and a view vault.decrypted_secrets) to store arbitrary secrets encrypted with a key
supabase.com
supabase.com
. This is often used for API keys, but could store something like a patient’s full record or a person’s SSN. In practice, for each user or tenant you could keep sensitive fields in this vault table keyed by an ID, and only join or fetch them when absolutely needed. This approach is likely overkill for just emails/phones, which are better kept in the user profile table with encryption as described above.
Using Vault vs. pgcrypto: Vault essentially uses pgcrypto under the hood but with managed keys and an easier interface. It’s recommended for ease of use – you can add an encryption key in the Supabase UI, then call vault.encrypt(column, key_id) in SQL. If you prefer, you can do manual encryption in application code (e.g., using Fernet or similar) before sending to DB, but then you must manage keys in your app which is riskier. With Vault, keys can be rotated or stored outside application code, and you can use authenticated encryption (AEAD) so that ciphertext integrity is ensured
supabase.com
supabase.com
. PII in transit and access control: Ensure that any PII that does need to be returned via the API is only accessible to authorized users. Use additional RLS rules such as masking: for instance, you could create a policy that allows selecting an email only to users with a certain role (or only their own email). For extra compliance, consider implementing an audit log (Postgres event trigger or Supabase Log Tables) for when sensitive fields are accessed. RLS for Vector Search Isolation: As discussed, RLS is critical to prevent data leakage across tenants. When performing vector similarity searches, be cautious: a typical query might be SELECT * FROM vectors ORDER BY embedding <-> :query_vec LIMIT 10. With RLS enabled, Postgres will internally add WHERE tenant_id = currentTenant to that query. It will then use the HNSW index – however, note that the index might traverse some neighbors from other tenants before the RLS filter kicks in. Postgres will filter out rows that don’t satisfy RLS after computing the nearest neighbors, which could result in fewer than 10 results if some of the top neighbors were disallowed. To ensure the query finds enough neighbors, it’s good to set ef_search high so that it fetches more candidates than the desired k, mitigating the chance that post-filtering you drop below k. Another approach is to bake tenant_id into the index by partitioning the vector table by tenant or creating a composite index (if pgvector supported it) – currently, pgvector doesn’t support a multi-column HNSW index directly, so partitioning is the way if performance becomes an issue. From a policy perspective, ensure the policy covers the vector distance function: e.g., in some edge cases you might write a POLICY using (tenant_id = auth.jwt() ->> 'tenant_id') for normal SELECT, but if you allow a stored procedure to perform the search, ensure that procedure does not use SECURITY DEFINER to bypass RLS. It’s safer to do searches via direct SQL (with the user’s context) or ensure any function is SECURITY INVOKER. Supabase’s standard practice is to rely on the client (or server in this case) to always send the JWT so RLS is applied on every query
github.com
github.com
. For the chunked vectors (if you break documents into chunks and embed each), the same RLS applies – each chunk row has tenant_id. Your RLS policy can also ensure that tenants cannot INSERT or UPDATE vectors that don’t belong to them (FOR INSERT check, etc.). In summary, use Vault for encrypting sensitive PII data at rest (or at least pgcrypto with a managed key) to protect against unauthorized access to the database files or backups. And use strict RLS policies on all tables (content, vectors, profiles) to enforce tenant isolation at query time. With these measures, even if a malicious query is attempted, it won’t return others’ data, and even if the database content is leaked, PII remains encrypted gibberish without the key.
Open Questions and Further Validation
HNSW Tuning at 10M Scale: While guidelines are given for HNSW parameters, real-world data distributions may affect optimal settings. We should validate on our actual embeddings (384-d from our content) whether the chosen m and ef_construction yield acceptable recall vs. indexing time. It remains an open question if a single index can handle 10M rows per tenant with high recall – or if we need to consider sharding by tenant or using IVFFlat for certain cases. Field testing with gradually increasing data (1M → 5M → 10M) will confirm if query latency and recall stay within targets
tembo.io
tembo.io
.
Vector Search with RLS Performance: Enforcing RLS on the vector table may have performance implications. Because RLS filters by tenant_id, the query planner might not use the HNSW index as effectively (if it can’t exclude other tenants early). We should measure the difference between a tenant-isolated search (with RLS or explicit WHERE tenant_id=X) and an unrestricted search. If we see degraded performance, we might need to explore partitioning the vector table by tenant or maintaining separate indexes. This is an area for further experimentation in the staging environment with many tenants.
Embedding Pipeline Throughput: The asynchronous trigger->queue->function pipeline needs to keep up with content changes. In practice, the throughput will depend on the Edge Function execution time (network call to OpenAI, etc.) and how we batch jobs. We should validate that our batching interval (e.g. cron every 10s) and batch size (say 50 embeddings per invocation) can handle peak load. If a tenant bulk-inserts thousands of pages, will the queue back up? Potential open actions: enable concurrency in the Edge Function (Supabase Edge Functions on Deno may run single-threaded – we might run multiple functions in parallel or increase the frequency). Monitoring the embedding_jobs queue length will help decide if adjustments are needed.
Memory Usage in Long Crawls: Although the MemoryAdaptiveDispatcher prevents most OOM issues, we haven’t tested very long-running crawls (e.g. thousands of URLs over hours) on a low-memory instance. There could be subtle memory leaks either in Crawl4AI or Playwright. It’s advisable to do a soak test – crawl a large site for multiple hours – and monitor memory over time. If usage climbs, we might need to periodically restart the crawler process or use Playwright’s context recycling more aggressively (perhaps restart the browser after N URLs). This is an open area to watch in production.
OpenTelemetry Overhead and Sampling: Instrumenting everything with OpenTelemetry adds overhead – each traced operation has some cost (especially with synchronous exporting). We should measure the latency impact of tracing on the API and possibly enable sampling. Grafana Tempo and the SDK allow sampling strategies (e.g. sample 1 in 10 requests). For an MVP, full tracing is fine, but as load grows we might need to lower the volume of traces. Deciding an optimal sample rate (or dynamic sampling for errors) remains an open task. Also, we will verify that traces from FastAPI and the Edge Functions can be correlated (using the TraceContext) – if not, we might need to manually propagate context in the pgmq messages or job payloads.
Vault Key Management and Queryability: Using Vault for PII encryption means queries like searching by email become non-trivial (you can’t index an encrypted email easily for wildcard search). We may need to store a hashed version for search while storing the full encrypted value for display. The exact strategy for each PII field (hash vs. deterministic encrypt vs. last-4-digits for phone, etc.) should be validated with the requirements. We also need to plan key rotation – Vault allows it, but rotating keys might require re-encrypting existing data. This is a future consideration once we have baseline encryption working.
Each of these items will be addressed in testing or future iterations. By acknowledging them now, we ensure our MVP is built with these considerations in mind and we have a path to validate and refine the system in real-world conditions. Sources: Best practices and configurations are drawn from official docs and maintainers’ guidance, including Supabase’s documentation and release notes
github.com
github.com
, Crawl4AI’s 0.6 release notes
docs.crawl4ai.com
docs.crawl4ai.com
, and SQLAlchemy/FastAPI community examples
sentry.io
sentry.io
 (all accessed May 7, 2025). The recommendations prioritize information and updates published post-2023 in line with the latest project features.
Citations
Favicon
HNSW Indexes with Postgres and pgvector | Crunchy Data Blog

https://www.crunchydata.com/blog/hnsw-indexes-with-postgres-and-pgvector
Favicon
HNSW Indexes with Postgres and pgvector | Crunchy Data Blog

https://www.crunchydata.com/blog/hnsw-indexes-with-postgres-and-pgvector
Favicon
Vector Indexes in Postgres using pgvector: IVFFlat vs HNSW | Tembo

https://tembo.io/blog/vector-indexes-in-pgvector
Favicon
HNSW Indexes with Postgres and pgvector | Crunchy Data Blog

https://www.crunchydata.com/blog/hnsw-indexes-with-postgres-and-pgvector
Favicon
HNSW Indexes with Postgres and pgvector | Crunchy Data Blog

https://www.crunchydata.com/blog/hnsw-indexes-with-postgres-and-pgvector
Favicon
python - hnswlib parameters for large datasets? - Stack Overflow

https://stackoverflow.com/questions/65379421/hnswlib-parameters-for-large-datasets
Favicon
HNSW Indexes with Postgres and pgvector | Crunchy Data Blog

https://www.crunchydata.com/blog/hnsw-indexes-with-postgres-and-pgvector
Favicon
Automatic embeddings | Supabase Docs

https://supabase.com/docs/guides/ai/automatic-embeddings
Favicon
Automatic embeddings | Supabase Docs

https://supabase.com/docs/guides/ai/automatic-embeddings
Favicon
Automatic embeddings | Supabase Docs

https://supabase.com/docs/guides/ai/automatic-embeddings
Favicon
Automatic embeddings | Supabase Docs

https://supabase.com/docs/guides/ai/automatic-embeddings
Favicon
Automatic embeddings | Supabase Docs

https://supabase.com/docs/guides/ai/automatic-embeddings
Favicon
Automatic embeddings | Supabase Docs

https://supabase.com/docs/guides/ai/automatic-embeddings
Favicon
Automatic embeddings | Supabase Docs

https://supabase.com/docs/guides/ai/automatic-embeddings
Favicon
Automatic embeddings | Supabase Docs

https://supabase.com/docs/guides/ai/automatic-embeddings
Favicon
Automatic embeddings | Supabase Docs

https://supabase.com/docs/guides/ai/automatic-embeddings
Favicon
Automatic embeddings | Supabase Docs

https://supabase.com/docs/guides/ai/automatic-embeddings
Favicon
Automatic embeddings | Supabase Docs

https://supabase.com/docs/guides/ai/automatic-embeddings
Favicon
Trying to understand connections with supavisor · supabase · Discussion #19757 · GitHub

https://github.com/orgs/supabase/discussions/19757
Favicon
Supavisor 1.0: a scalable connection pooler for Postgres

https://supabase.com/blog/supavisor-postgres-connection-pooler
Favicon
Supavisor 1.0: a scalable connection pooler for Postgres

https://supabase.com/blog/supavisor-postgres-connection-pooler
Favicon
Connection management | Supabase Docs

https://supabase.com/docs/guides/database/connection-management
Favicon
Connection management | Supabase Docs

https://supabase.com/docs/guides/database/connection-management
Favicon
server_idle_timeout for DbHandler · Issue #274 · supabase/supavisor · GitHub

https://github.com/supabase/supavisor/issues/274
Favicon
server_idle_timeout for DbHandler · Issue #274 · supabase/supavisor · GitHub

https://github.com/supabase/supavisor/issues/274
Favicon
Supavisor 1.0: a scalable connection pooler for Postgres

https://supabase.com/blog/supavisor-postgres-connection-pooler
Favicon
Supavisor 1.0: a scalable connection pooler for Postgres

https://supabase.com/blog/supavisor-postgres-connection-pooler
Favicon
Connection management | Supabase Docs

https://supabase.com/docs/guides/database/connection-management
Favicon
Connection management | Supabase Docs

https://supabase.com/docs/guides/database/connection-management
Favicon
row-level security policies in Supabase for a multitenant application · community · Discussion #149922 · GitHub

https://github.com/orgs/community/discussions/149922
Favicon
row-level security policies in Supabase for a multitenant application · community · Discussion #149922 · GitHub

https://github.com/orgs/community/discussions/149922
Favicon
row-level security policies in Supabase for a multitenant application · community · Discussion #149922 · GitHub

https://github.com/orgs/community/discussions/149922
Favicon
row-level security policies in Supabase for a multitenant application · community · Discussion #149922 · GitHub

https://github.com/orgs/community/discussions/149922
Favicon
row-level security policies in Supabase for a multitenant application · community · Discussion #149922 · GitHub

https://github.com/orgs/community/discussions/149922
Blog Home - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/blog/
Blog Home - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/blog/
Favicon
Crawl4AI: Best AI Web Crawling Open Source Tool (Firecrawl Open Source Alternatives)

https://huggingface.co/blog/lynn-mikami/crawl-ai
Blog Home - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/blog/
Blog Home - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/blog/
Blog Home - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/blog/
Crawl4AI v0.6.0 Release Notes - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/blog/releases/0.6.0/
Browser, Crawler & LLM Config - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/api/parameters/
Browser, Crawler & LLM Config - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/api/parameters/
Multi-URL Crawling - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/advanced/multi-url-crawling/
Multi-URL Crawling - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/advanced/multi-url-crawling/
Multi-URL Crawling - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/advanced/multi-url-crawling/
Multi-URL Crawling - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/advanced/multi-url-crawling/
Multi-URL Crawling - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/advanced/multi-url-crawling/
Multi-URL Crawling - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/advanced/multi-url-crawling/
Multi-URL Crawling - Crawl4AI Documentation (v0.6.x)

https://docs.crawl4ai.com/advanced/multi-url-crawling/
Favicon
Schedule tasks with FastAPI | Sentry

https://sentry.io/answers/schedule-tasks-with-fastapi/
Favicon
Schedule tasks with FastAPI | Sentry

https://sentry.io/answers/schedule-tasks-with-fastapi/
Favicon
Schedule tasks with FastAPI | Sentry

https://sentry.io/answers/schedule-tasks-with-fastapi/
Favicon
Schedule tasks with FastAPI | Sentry

https://sentry.io/answers/schedule-tasks-with-fastapi/
Favicon
Schedule tasks with FastAPI | Sentry

https://sentry.io/answers/schedule-tasks-with-fastapi/
Favicon
flake8-SQL · PyPI

https://pypi.org/project/flake8-SQL/
Favicon
Send data to the Grafana Cloud OTLP endpoint

https://grafana.com/docs/grafana-cloud/send-data/otlp/send-data-otlp/
Favicon
Send traces directly from python application to Grafana Cloud's ...

https://stackoverflow.com/questions/76624642/send-traces-directly-from-python-application-to-grafana-clouds-tempo-instance-w
Favicon
GitHub - blueswen/fastapi-observability: Observe FastAPI app with three pillars of observability: Traces (Tempo), Metrics (Prometheus), Logs (Loki) on Grafana through OpenTelemetry and OpenMetrics.

https://github.com/Blueswen/fastapi-observability
Favicon
GitHub - blueswen/fastapi-observability: Observe FastAPI app with three pillars of observability: Traces (Tempo), Metrics (Prometheus), Logs (Loki) on Grafana through OpenTelemetry and OpenMetrics.

https://github.com/Blueswen/fastapi-observability
Favicon
GitHub - blueswen/fastapi-observability: Observe FastAPI app with three pillars of observability: Traces (Tempo), Metrics (Prometheus), Logs (Loki) on Grafana through OpenTelemetry and OpenMetrics.

https://github.com/Blueswen/fastapi-observability
Favicon
GitHub - blueswen/fastapi-observability: Observe FastAPI app with three pillars of observability: Traces (Tempo), Metrics (Prometheus), Logs (Loki) on Grafana through OpenTelemetry and OpenMetrics.

https://github.com/Blueswen/fastapi-observability
Favicon
GitHub - blueswen/fastapi-observability: Observe FastAPI app with three pillars of observability: Traces (Tempo), Metrics (Prometheus), Logs (Loki) on Grafana through OpenTelemetry and OpenMetrics.

https://github.com/Blueswen/fastapi-observability
Favicon
GitHub - blueswen/fastapi-observability: Observe FastAPI app with three pillars of observability: Traces (Tempo), Metrics (Prometheus), Logs (Loki) on Grafana through OpenTelemetry and OpenMetrics.

https://github.com/Blueswen/fastapi-observability
Favicon
GitHub - blueswen/fastapi-observability: Observe FastAPI app with three pillars of observability: Traces (Tempo), Metrics (Prometheus), Logs (Loki) on Grafana through OpenTelemetry and OpenMetrics.

https://github.com/Blueswen/fastapi-observability
Favicon
Automatic embeddings | Supabase Docs

https://supabase.com/docs/guides/ai/automatic-embeddings
Favicon
Vector Indexes in Postgres using pgvector: IVFFlat vs HNSW | Tembo

https://tembo.io/blog/vector-indexes-in-pgvector
Favicon
FastAPI Observability | Grafana Labs

https://grafana.com/grafana/dashboards/16110-fastapi-observability/
Favicon
FastAPI Observability | Grafana Labs

https://grafana.com/grafana/dashboards/16110-fastapi-observability/
Favicon
How to Use Vault for Encrypting Patient Data in Supabase? - Reddit

https://www.reddit.com/r/Supabase/comments/1gm7f0p/how_to_use_vault_for_encrypting_patient_data_in/
Favicon
Vault | Supabase Docs

https://supabase.com/docs/guides/database/vault
Favicon
Vault | Supabase Docs

https://supabase.com/docs/guides/database/vault
Favicon
Vault | Supabase Docs

https://supabase.com/docs/guides/database/vault
Favicon
Vault | Supabase Docs

https://supabase.com/docs/guides/database/vault
Favicon
Vault | Supabase Docs

https://supabase.com/docs/guides/database/vault
Favicon
Supabase Vault

https://supabase.com/blog/supabase-vault
Favicon
Supabase Vault

https://supabase.com/blog/supabase-vault
Favicon
row-level security policies in Supabase for a multitenant application · community · Discussion #149922 · GitHub

https://github.com/orgs/community/discussions/149922
Favicon
Vector Indexes in Postgres using pgvector: IVFFlat vs HNSW | Tembo

https://tembo.io/blog/vector-indexes-in-pgvector
Favicon
Vector Indexes in Postgres using pgvector: IVFFlat vs HNSW | Tembo

https://tembo.io/blog/vector-indexes-in-pgvector