# COMPREHENSIVE STRATEGIC PLAN: ScraperSky Page Content Extraction Service

## EXECUTIVE SUMMARY

The ScraperSky backend requires a robust page content extraction service that efficiently discovers, processes, and stores webpage content while embracing modern vector search capabilities. This plan combines two critical infrastructure improvements:

1. **Database Migration**: Transitioning from Alembic to Supabase MCP for schema management
2. **Page Content Extraction Framework**: Implementing a production-ready, scalable system for content discovery, extraction, and semantic search

This plan strictly adheres to ScraperSky's architectural principles including FastAPI, SQLAlchemy ORM, proper transaction boundaries, Supabase integration with Supavisor connection pooling, and comprehensive monitoring. It eliminates tenant isolation dependencies while preserving RLS security and implements best practices for vector embeddings, deduplication, and politeness management.

## 1. RESOURCE INVENTORY

### 1.1 TECHNOLOGY STACK
- **Backend Framework**: FastAPI with async endpoints
- **ORM**: SQLAlchemy 2.0 (type-safe, async support)
- **Database**: PostgreSQL via Supabase
- **Vector Store**: pgvector with HNSW indexing
- **Crawler**: crawl4ai v0.6.* (leveraging stream mode)
- **Connection Pool**: Supavisor (Supabase's connection pooler)
- **Scheduler**: APScheduler (integrated with FastAPI lifespan)
- **Monitoring**: OpenTelemetry + Prometheus/Grafana
- **Deployment**: Render.com (configured via render.yaml)
- **Migration Tooling**: Supabase MCP (replacing Alembic)
- **Schema Management**: Supabase CLI
- **Vector Embeddings**: Supabase Edge Functions with gte-small model
- **Queue System**: pgmq (via Supabase Automatic Embeddings pattern)
- **Security**: RLS policies, Supabase Vault for PII

### 1.2 EXISTING CODEBASE STRUCTURE
- **Architecture Pattern**: Router → Queue → Scheduler → Service
- **Transaction Pattern**:
  - Routers own transaction boundaries (`async with session.begin()`)
  - Services accept sessions but don't manage transactions
  - Background tasks manage their own sessions
- **Database Connection**: One standardized method via FastAPI dependency injection
- **API Versioning**: All endpoints use v3 prefix (/api/v3/*)
- **Error Handling**: Native FastAPI error handling (custom ErrorService removed)

### 1.3 CRITICAL FILES REQUIRING UPDATES
- **High Priority**:
  - `/src/routers/page_processor.py`: API endpoints for content extraction
  - `/src/services/page_content_extractor_service.py`: Core extraction logic
  - `/src/schedulers/page_content_scheduler.py`: Background processing
  - `/src/utils/vector_utils.py`: Vector embedding utilities
- **Integration Points**:
  - `/src/db/session.py`: Database connection configuration
  - `/src/main.py`: Router registration
  - `/src/services/core/db_service.py`: ORM-only database operations

### 1.4 BOTTLENECKS TO ADDRESS
- URL deduplication and normalization
- Rate limiting and politeness management
- Vector search performance (recall vs. latency)
- Connection pool exhaustion
- Memory usage within Render limits
- Async transaction management
- Monitoring for silent failures
- Cross-tenant data isolation

## 2. DATABASE MIGRATION: ALEMBIC TO MCP

### 2.1 PREPARATION PHASE
- **Task**: Install Supabase CLI ≥ v1.150
  - Command: `brew install supabase/tap/supabase`
  - Validation: `supabase --version` outputs ≥ 1.150
- **Task**: Create Supabase Personal Access Token
  - Source: Supabase dashboard → Settings → Personal access tokens
  - Name: "MCP Dev"
  - Store securely for subsequent steps
- **TO-DO**: Document token storage protocol for team access

### 2.2 ALEMBIC FREEZING
- **Task**: Run final Alembic migration
  - Command: `alembic upgrade head`
  - Validation: Database schema is at latest state
- **Task**: Remove Alembic dependency
  - Delete `/alembic` directory
  - Remove from `requirements.txt` or `pyproject.toml`
  - Update any CI/CD scripts referencing Alembic
- **Validation**: CI passes without Alembic references
- **TO-DO**: Verify no runtime dependencies on removed Alembic components

### 2.3 BASELINE MIGRATION CAPTURE
- **Task**: Generate baseline SQL from current schema
  - Command: `supabase db diff --schema public --file 000_init.sql`
  - Location: `supabase/migrations/000_init.sql`
- **Task**: Verify schema recreation
  - Command: `supabase db reset`
  - Validation: Database recreated identically to production
- **TO-DO**: Create SHA-256 checksum of production schema for verification

### 2.4 MCP SERVER CONFIGURATION
- **Task**: Create MCP configuration file
  - Path: `.cursor/mcp.json`
  - Content:
```json
{
  "mcpServers": {
    "supabase": {
      "command": "npx",
      "args": [
        "-y",
        "@supabase/mcp-server-supabase@latest",
        "--access-token",
        "<PAT>"
      ]
    }
  }
}
```
- **Task**: Test MCP server connection
  - Open Cursor/Windsurf IDE
  - Verify Supabase toolset appears green
  - Confirm >20 Supabase tools available
- **TO-DO**: Pin MCP server version to avoid breaking changes

### 2.5 MCP-DRIVEN WORKFLOW IMPLEMENTATION
- **Task**: Test schema creation via MCP
  - Create test table using natural language
  - Verify SQL migration generated in `supabase/migrations/`
  - Run `supabase db reset` to apply changes
- **Task**: Document new workflow
  - Update README.md with MCP-based schema management
  - Create examples for common schema operations
- **TO-DO**: Create template for standardized table creation with proper RLS

### 2.6 CI/CD UPDATE
- **Task**: Replace Alembic steps in pipeline
  - Update CI script to use `supabase db push --linked`
  - Add check to fail if raw SQL exists outside migrations directory
- **Task**: Add linting for SQL detection
  - Add pre-commit hook for SQL pattern detection
  - Implement grep or flake8-sql check in CI
- **TO-DO**: Set up migration test environment for PR validation

### 2.7 DOCUMENTATION AND STANDARDS
- **Task**: Update project documentation
  - Path: `README.md`, `CONTRIBUTING.md`
  - Content: "Use MCP to create/alter tables → run CLI to apply"
  - Add section on gotchas (long-running locks, RLS policy tips)
- **Task**: Create training material for team
  - Document common MCP commands
  - Provide examples for typical schema changes
- **TO-DO**: Document rollback procedures for failed migrations

## 3. PAGE CONTENT EXTRACTION FRAMEWORK

### 3.1 DATABASE SCHEMA (VIA MCP)

#### 3.1.1 Tables
- **page_content**: Stores metadata about crawled pages
  - Primary key: `id` (UUID)
  - Foreign key: `domain_id` references `domains`
  - Fields: `url` (varchar), `title` (varchar), `crawled_at` (timestamp), `content_hash` (varchar), `status` (enum), `html_storage_path` (varchar), `metadata` (jsonb)
  - RLS policy: Filter by `tenant_id`

- **page_content_chunks**: Stores chunked text content
  - Primary key: `id` (UUID)
  - Foreign key: `page_content_id` references `page_content`
  - Fields: `chunk_index` (int), `text_content` (text), `token_count` (int), `metadata` (jsonb)
  - RLS policy: Filter by `tenant_id` from parent

- **page_content_vectors**: Stores vector embeddings
  - Primary key: `id` (UUID)
  - Foreign key: `chunk_id` references `page_content_chunks`
  - Fields: `embedding` (vector), `embedding_model` (varchar), `created_at` (timestamp)
  - HNSW index: `CREATE INDEX on page_content_vectors USING hnsw (embedding vector_l2_ops) WITH (m=16, ef_construction=200)`
  - RLS policy: Filter by `tenant_id` from parent

- **domains_crawl_state**: Stores politeness settings and crawl state
  - Primary key: `domain` (varchar)
  - Fields: `last_crawled_at` (timestamp), `crawl_delay_ms` (int), `robots_txt_crawl_delay` (int), `error_backoff_until` (timestamp), `consecutive_errors` (int), `crawl_stats` (jsonb)
  - RLS policy: Filter by `tenant_id`

- **social_media_profiles**: Stores discovered social media accounts
  - Primary key: `id` (UUID)
  - Foreign key: `domain_id` references `domains`
  - Fields: `platform` (varchar), `handle` (varchar), `url` (varchar), `discovered_at` (timestamp), `metadata` (jsonb)
  - RLS policy: Filter by `tenant_id`

#### 3.1.2 Queue Tables (Automatic Embeddings)
- **embeddings_queue**: Managed by pgmq
  - Fields follow Supabase Automatic Embeddings template
  - Stores jobs for asynchronous embedding generation

#### 3.1.3 Edge Functions
- **generate_embedding**: Runs gte-small model on text chunks
  - Input: text content
  - Output: vector embedding (float array)
  - Runtime: Deno on Supabase Edge Functions

### 3.2 ROUTER IMPLEMENTATION

#### 3.2.1 API Endpoints
- **POST /api/v3/page-content/discover**
  - Purpose: Queue domain for content discovery
  - Parameters: `domain_id` (UUID), `max_pages` (int), `discover_subdomains` (boolean), `follow_external_links` (boolean)
  - Response: `job_id` (UUID)
  - Transaction: Creates job and queues for processing
  - TO-DO: Implement request validation with comprehensive error messages

- **GET /api/v3/page-content/status/{job_id}**
  - Purpose: Check discovery job status
  - Parameters: `job_id` (UUID)
  - Response: Job status and progress statistics
  - Transaction: Read-only
  - TO-DO: Add detailed progress metrics (pages crawled, errors encountered)

- **POST /api/v3/page-content/search**
  - Purpose: Search content using vector similarity
  - Parameters: `query` (string), `domain_id` (UUID, optional), `limit` (int), `ef_search` (int, optional)
  - Response: Array of matching content with scores
  - Transaction: Read-only with dynamic HNSW parameters
  - TO-DO: Implement faceted search options

- **GET /api/v3/page-content/domains/{domain_id}/stats**
  - Purpose: Get statistics for a domain's content
  - Parameters: `domain_id` (UUID)
  - Response: Counts, timestamps, and extraction metrics
  - Transaction: Read-only
  - TO-DO: Add content type breakdown (emails, social, etc.)

### 3.3 SERVICE IMPLEMENTATION

#### 3.3.1 Page Content Extractor Service
- **discover_domain_content**
  - Purpose: Entry point for content discovery
  - Parameters: domain_obj, max_pages, options
  - Logic: Creates job and queues domain for processing
  - Transaction behavior: Uses provided session

- **process_page**
  - Purpose: Processes a single page
  - Parameters: url, domain_obj, job_obj, session
  - Logic:
    1. Normalize URL
    2. Check content_hash for duplicates
    3. Fetch with politeness (via crawl4ai)
    4. Extract and clean text
    5. Store raw HTML in Supabase Storage
    6. Chunk content (~800 tokens with 200 overlap)
    7. Queue chunks for embedding
    8. Extract emails, social links, phones
    9. Update job progress
  - Transaction behavior: Uses provided session
  - TO-DO: Implement robust error handling with retry logic

- **normalize_url**
  - Purpose: Standardize URLs to prevent duplicates
  - Logic: Strip UTM parameters, lowercase hostname, remove trailing slashes, etc.
  - TO-DO: Create comprehensive test suite for edge cases

- **calculate_content_hash**
  - Purpose: Generate deduplication key
  - Logic: SHA-256 hash of text-only content (stripped of HTML)
  - TO-DO: Benchmark against alternative hashing algorithms

- **extract_and_clean_text**
  - Purpose: Convert HTML to plain text
  - Logic: BeautifulSoup with configuration for handling JavaScript-rendered content
  - TO-DO: Implement fallback mechanisms for complex pages

- **chunk_content**
  - Purpose: Split text into optimal chunks for embeddings
  - Logic: Token-based chunking with overlap
  - TO-DO: Research optimal chunk size for gte-small model

#### 3.3.2 Domain Politeness Manager
- **get_crawl_delay**
  - Purpose: Determine appropriate delay for a domain
  - Logic: Respects robots.txt and applies exponential backoff for errors
  - Transaction behavior: Read from domains_crawl_state table

- **update_crawl_state**
  - Purpose: Record crawl attempt and results
  - Logic: Updates timestamp and error counters as needed
  - Transaction behavior: Uses provided session
  - TO-DO: Implement circuit breaker pattern for consistently failing domains

#### 3.3.3 Vector Utilities
- **create_embedding**
  - Purpose: Generate vector for text chunk
  - Logic: Calls Supabase Edge Function with appropriate batching
  - TO-DO: Implement fallback to local embedding if Edge Function fails

- **similar_content_search**
  - Purpose: Find similar content using vector search
  - Logic: HNSW search with dynamic ef_search parameter
  - Transaction behavior: Read-only with SET LOCAL for search parameters
  - TO-DO: Research optimal ef_search values for different query types

### 3.4 SCHEDULER IMPLEMENTATION

#### 3.4.1 Page Content Scheduler
- **initialize**
  - Purpose: Set up scheduler in FastAPI lifespan
  - Logic: Configure APScheduler with appropriate job store and executors

- **process_pending_extractions**
  - Purpose: Process domains with QUEUED status
  - Logic:
    1. Fetch batch of domains with QUEUED status
    2. Apply concurrency limits based on resource availability
    3. Process each domain with appropriate politeness
    4. Handle errors with backoff strategy
    5. Update job status
  - Transaction behavior: Creates own session and transaction
  - TO-DO: Implement proper job locking for distributed environments

- **cleanup_stale_jobs**
  - Purpose: Handle jobs stuck in PROCESSING state
  - Logic: Reset jobs stuck for more than configurable threshold
  - Transaction behavior: Creates own session and transaction
  - TO-DO: Add alerting for frequently stalled jobs

### 3.5 MONITORING AND OBSERVABILITY

#### 3.5.1 OpenTelemetry Integration
- **crawl_span**
  - Purpose: Track crawl performance and errors
  - Metrics: Duration, success rate, politeness compliance

- **extraction_span**
  - Purpose: Monitor text extraction performance
  - Metrics: Duration, text length, chunking statistics

- **embedding_span**
  - Purpose: Track vector embedding generation
  - Metrics: Queue time, processing time, batch size impact

- **search_span**
  - Purpose: Measure vector search performance
  - Metrics: Query time, recall percentage, result count

#### 3.5.2 Grafana Dashboards
- **Crawl Performance Dashboard**
  - Panels: Success rate, deduplication rate, crawler memory usage

- **Vector Search Dashboard**
  - Panels: Query latency, recall percentage, ef_search impact

- **Job Processing Dashboard**
  - Panels: Queue length, processing rate, error distribution

### 3.6 SECURITY AND COMPLIANCE

#### 3.6.1 RLS Policies
- **Tenant Isolation**
  - Ensure all tables have RLS policies filtering by tenant_id
  - Validate policies with comprehensive test suite

- **PII Handling**
  - Implement Supabase Vault for emails and phone numbers
  - Set up audit logging for PII access

#### 3.6.2 Ethical Crawling
- **Robots.txt Compliance**
  - Strictly adhere to robots.txt directives
  - Implement proper User-Agent string

- **Rate Limiting**
  - Respect per-domain crawl delays
  - Implement exponential backoff for 429/503 responses

## 4. IMPLEMENTATION TIMELINE

### 4.1 WEEK 1: FOUNDATION
- Database migration from Alembic to MCP
- Service and scheduler skeletons
- Basic crawl4ai integration with URL normalization
- Initial monitoring setup with OpenTelemetry

### 4.2 WEEK 2: CRAWL & STORE
- Complete politeness manager implementation
- Content extraction and HTML storage
- URL normalization and content hashing for deduplication
- Basic API endpoints for discovery and status

### 4.3 WEEK 3: EMBED & SEARCH
- Implement Automatic Embeddings pipeline
- Chunking strategy with token-based approach
- Vector search implementation with HNSW
- Complete API endpoints including search

### 4.4 WEEK 4: HARDENING & POLISH
- Comprehensive error handling and retry logic
- Dead letter queue for failed processing
- Complete monitoring dashboards
- Performance optimization and tuning
- Documentation and deployment procedures

## 5. STRATEGIC LONG-TERM OPPORTUNITIES

### 5.1 ZERO-COPY FEDERATED SEARCH
- Extend extraction to JSON/GraphQL endpoints
- Maintain common vector pipeline across source types

### 5.2 ON-DEVICE INFERENCE
- Research WebGPU embeddings for client-side processing
- Design synchronization protocol for client-generated vectors

### 5.3 AI AGENT LAYER
- Leverage MCP for AI-driven competitive intelligence
- Create custom tools for domain-specific queries

### 5.4 QUANTUM-SAFE SECURITY
- Abstract PII encryption via Supabase Vault
- Design for future post-quantum cipher migration

## 6. TO-DO: DEEP DIVE TOPICS

1. Optimal HNSW parameters (m, ef_construction, ef_search) for ScraperSky's specific use case
2. Token chunking strategy tuning based on content types
3. Rate limiting algorithm refinement for ethical crawling
4. Transaction boundary optimization for background tasks
5. Memory usage profiling on Render instances
6. Comprehensive RLS policy coverage verification
7. Alerting thresholds for system health metrics
8. Disaster recovery procedures for embedding pipeline
9. Benchmark Supabase Edge Functions vs. local embedding performance
10. Security audit for PII handling compliance

## 7. RESOURCE REFERENCES

### 7.1 DOCUMENTATION RESOURCES
- Crawl4AI v0.6 Documentation: https://crawl4ai.readthedocs.io/
- Supabase Vector Documentation: https://supabase.com/docs/guides/ai/vector-search
- Supabase Automatic Embeddings: Released in Launch Week 14 (April 2025)
- FastAPI Transaction Management: https://fastapi.tiangolo.com/advanced/sql-databases-peewee/
- pgvector HNSW Documentation: https://github.com/pgvector/pgvector

### 7.2 PROJECT DOCUMENTATION
- ScraperSky Architecture: `/Docs/Docs_6_Architecture_and_Status/0.1_ScraperSky_Architecture_Flow_and_Components-Enhanced.md`
- Transaction Patterns: `/Docs/Docs_5_Project_Working_Docs/07-04-transaction-patterns-reference.md`
- API Versioning Standard: v3 prefix for all endpoints
- Database Connection Mandate: One standardized method via FastAPI dependency injection

### 7.3 CRITICAL CONFIG PARAMETERS
- Supabase Connection String: `postgresql+asyncpg://postgres.[project-ref]:password@aws-0-us-west-1.pooler.supabase.com:6543/postgres?raw_sql=true&no_prepare=true&statement_cache_size=0`
- HNSW Index Parameters: m=16, ef_construction=200, ef_search varies by query
- Token Chunking: ~800 tokens with 200-token overlap
- Default tenant ID: 550e8400-e29b-41d4-a716-446655440000 (used for development)

## 8. SUCCESS CRITERIA

1. Complete migration from Alembic to MCP with zero data loss
2. API endpoints follow v3 standard with proper transaction boundaries
3. Content extraction service processes pages with >95% success rate
4. Vector search provides relevant results with <100ms latency
5. Monitoring captures all critical metrics with appropriate alerting
6. System operates within Render resource constraints
7. RLS policies ensure complete tenant isolation
8. All database operations use SQLAlchemy ORM only (no raw SQL)
9. Comprehensive documentation for operations and maintenance
10. Ethical crawling with full robots.txt compliance and proper rate limiting
