# Implementation Details for Local Business Curation to Domain Table Trigger

**Document ID:** 04-03-Work-Order-Implementation-Details-Local_Business-Domain-Table
**Status:** Final
**Date:** April 2025
**Author:** Gemini Assistant & User
**Related Work Order:** `03-Work-Order-Local_Business-Row-Select-to-Domain-Table.md`

## 1. Objective

This document captures the specific code changes implemented to fulfill the requirements outlined in the related work order (`03-...md`). It serves as a technical completion report.

## 2. Summary of Changes

The following files were created or modified:

- **Created:**
  - `src/services/business_to_domain_service.py` (Worker Service)
  - `src/routers/local_businesses.py` (API Router)
- **Modified:**
  - `src/models/api_models.py` (Added Pydantic Models)
  - `src/models/domain.py` (Added FK to `local_businesses`)
  - `src/models/local_business.py` (Added Enum and Status Fields)
  - `src/main.py` (Registered new API Router)
  - `src/services/sitemap_scheduler.py` (Added new task processing block)

## 3. Code Implementation Details

### 3.1 `src/models/api_models.py`

Added the following Pydantic Enum and Request Model:

```python
# --- Models for Local Businesses Selection --- #

class LocalBusinessApiStatusEnum(str, Enum):
    """Possible statuses for a local business, matching PlaceStatusEnum."""
    New = "New"
    Selected = "Selected"
    Maybe = "Maybe"
    Not_a_Fit = "Not a Fit" # Ensure space is handled if API sends it
    Archived = "Archived"

class LocalBusinessBatchStatusUpdateRequest(BaseModel):
    """Request model to update the status for one or more local businesses."""
    local_business_ids: List[UUID] = Field(..., min_length=1, description="List of one or more Local Business UUIDs to update.")
    status: LocalBusinessApiStatusEnum = Field(..., description="The new main status to set.")

# --- End Models for Local Businesses Selection --- #
```

### 3.2 `src/models/domain.py`

Added the `local_business_id` foreign key column:

```python
    # Foreign key back to the originating local business (if created via that workflow)
    local_business_id = Column(PGUUID, ForeignKey("local_businesses.id", ondelete="SET NULL"), index=True, nullable=True)
```

### 3.3 `src/models/local_business.py`

Added the `DomainExtractionStatusEnum` and the new status columns:

```python
import enum
# ... other imports ...

# Define the enum for the domain extraction background process status
# Values MUST match the database enum values exactly (case-sensitive)
class DomainExtractionStatusEnum(enum.Enum):
    queued = "queued"
    processing = "processing"
    completed = "completed"
    failed = "failed"

class LocalBusiness(Base):
    # ... existing columns ...

    # Status field (reusing PlaceStatusEnum definition from place.py via DB constraint)
    status = Column(Enum("place_status_enum", name="place_status_enum", create_type=False), nullable=True, index=True, server_default='New')

    # Fields for Domain Extraction background process
    domain_extraction_status = Column(Enum(DomainExtractionStatusEnum, name="DomainExtractionStatusEnum", create_type=False), nullable=True, index=True)
    domain_extraction_error = Column(Text, nullable=True)

    # ... methods ...
```

### 3.4 `src/services/business_to_domain_service.py`

Created the new service file with the following content:

```python
"""
Service to handle the process of creating a pending Domain entry
from a selected LocalBusiness record.
"""
import logging
from uuid import UUID
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from sqlalchemy.orm import selectinload

from src.models.local_business import LocalBusiness
from src.models.domain import Domain
# Assuming a utility for domain extraction exists or will be created
# from src.utils.domain_extractor import extract_domain_from_url

logger = logging.getLogger(__name__)

class LocalBusinessToDomainService:
    """
    Handles the logic for extracting a domain from a LocalBusiness website URL
    and creating a corresponding 'pending' Domain record.
    """

    async def create_pending_domain_from_local_business(
        self,
        local_business_id: UUID,
        session: AsyncSession
    ) -> bool:
        """
        Fetches a LocalBusiness, extracts its domain, and creates a new Domain record
        with 'pending' status.

        Args:
            local_business_id: The UUID of the LocalBusiness record.
            session: The SQLAlchemy AsyncSession to use.

        Returns:
            True if a new pending Domain was successfully created or if the domain
            already existed, False if an error occurred (e.g., business not found,
            no URL, extraction error, DB error).
        """
        logger.info(f"Starting domain extraction for local_business_id: {local_business_id}")

        try:
            # 1. Fetch the LocalBusiness record
            stmt = select(LocalBusiness).where(LocalBusiness.id == local_business_id)
            result = await session.execute(stmt)
            business = result.scalar_one_or_none()

            if not business:
                logger.warning(f"LocalBusiness not found for id: {local_business_id}")
                return False # Indicate failure, business not found

            # 2. Check for website URL
            website_url = business.website_url
            # Explicit check for None or empty string
            if website_url is None or website_url == '':
                logger.info(f"No website_url found for local_business_id: {local_business_id}. Skipping domain creation.")
                # This is not necessarily an error, just nothing to process. Return True as the step completed.
                return True

            # 3. Extract the domain (Placeholder for actual extraction logic)
            # TODO: Implement robust domain extraction (e.g., using tldextract or similar)
            try:
                # extracted_domain = extract_domain_from_url(website_url) # Replace with actual call
                # Simplified placeholder:
                if website_url.startswith('http://'):
                    extracted_domain = website_url.split('/')[2]
                elif website_url.startswith('https://'):
                    extracted_domain = website_url.split('/')[2]
                else:
                     # Basic attempt if no scheme, might need refinement
                    extracted_domain = website_url.split('/')[0]

                if not extracted_domain:
                     raise ValueError("Could not extract domain from URL")

                # Remove www. if present for consistency
                extracted_domain = extracted_domain.replace("www.", "")

                logger.info(f"Extracted domain '{extracted_domain}' from URL '{website_url}' for business {local_business_id}")

            except Exception as extraction_error:
                logger.error(f"Error extracting domain from URL '{website_url}' for business {local_business_id}: {extraction_error}", exc_info=True)
                # Consider this a failure of the process
                return False

            # 4. Check if domain already exists
            stmt_domain_check = select(Domain).where(Domain.domain == extracted_domain)
            result_domain_check = await session.execute(stmt_domain_check)
            existing_domain = result_domain_check.scalar_one_or_none()

            if existing_domain:
                logger.info(f"Domain '{extracted_domain}' already exists (ID: {existing_domain.id}). Skipping creation.")
                # Domain already exists, maybe link it? For now, just consider it success.
                # Optionally, update existing_domain.local_business_id if null? TBD based on product reqs.
                return True

            # 5. Create new Domain record
            new_domain = Domain(
                domain=extracted_domain,
                status='pending', # Explicitly set status for the next processing stage
                local_business_id=local_business_id,
                # tenant_id will be set by default in the model
            )
            session.add(new_domain)
            # Flush to ensure the domain is added before the scheduler potentially updates the business status to completed
            await session.flush()
            logger.info(f"Successfully created new pending Domain record (ID: {new_domain.id}) for domain '{extracted_domain}' from business {local_business_id}")
            return True

        except Exception as e:
            logger.error(f"Unexpected error processing local_business_id {local_business_id}: {e}", exc_info=True)
            # Consider rolling back? The outer scheduler function should handle final status update.
            # Here, indicate failure.
            return False
```

### 3.5 `src/routers/local_businesses.py`

Created the new router file with the following content:

```python
"""
API Router for Local Business related operations.
"""
import logging
import math # Added for pagination calculation
from typing import List, Dict, Any, Optional
from uuid import UUID
from datetime import datetime

from fastapi import APIRouter, Depends, HTTPException, status, Query # Added Query
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from sqlalchemy import func # Added for count query

# Assuming DB Session, Auth dependencies are similar to other routers
from src.db.session import get_db_session # Adjust path if necessary
from src.auth.jwt_auth import get_current_user # Corrected import based on places_staging.py
from pydantic import BaseModel, Field # Added for GET endpoint models

from src.models.api_models import LocalBusinessBatchStatusUpdateRequest
from src.models.local_business import LocalBusiness, DomainExtractionStatusEnum
from src.models.place import PlaceStatusEnum # For DB mapping

logger = logging.getLogger(__name__)

# --- Local Pydantic Models for GET Endpoint --- #
# Define a Pydantic model that mirrors the LocalBusiness SQLAlchemy model
# This ensures API responses have a defined schema.
# Include fields needed by the frontend grid.
class LocalBusinessRecord(BaseModel):
    id: UUID
    business_name: Optional[str] = None
    full_address: Optional[str] = None
    phone: Optional[str] = None
    website_url: Optional[str] = None
    status: Optional[PlaceStatusEnum] = None # Use the DB enum here
    domain_extraction_status: Optional[DomainExtractionStatusEnum] = None
    created_at: datetime
    updated_at: datetime
    tenant_id: UUID

    class Config:
        from_attributes = True # Enable ORM mode for conversion (Updated from orm_mode)
        use_enum_values = True # Ensure enum values are used in response

# Response model for paginated results
class PaginatedLocalBusinessResponse(BaseModel):
    items: List[LocalBusinessRecord]
    total: int
    page: int
    size: int
    pages: int

# --- Router Definition --- #
router = APIRouter(prefix="/api/v3/local-businesses", tags=["Local Businesses"])

# --- GET Endpoint Implementation --- #
@router.get(
    "",
    response_model=PaginatedLocalBusinessResponse,
    summary="List Local Businesses (Paginated)",
    description="Retrieves a paginated list of local businesses, allowing filtering and sorting."
)
async def list_local_businesses(
    status_filter: Optional[PlaceStatusEnum] = Query(None, alias="status", description="Filter by main business status (e.g., New, Selected, Maybe)"),
    business_name: Optional[str] = Query(None, description="Filter by business name (case-insensitive partial match)"),
    page: int = Query(1, ge=1, description="Page number"),
    size: int = Query(50, ge=1, le=200, description="Page size"),
    sort_by: Optional[str] = Query("updated_at", description="Field to sort by (e.g., business_name, status, updated_at)"),
    sort_dir: Optional[str] = Query("desc", description="Sort direction: 'asc' or 'desc'"),
    session: AsyncSession = Depends(get_db_session),
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """Lists local businesses with pagination, filtering, and sorting."""
    tenant_id_str = current_user.get("tenant_id", "550e8400-e29b-41d4-a716-446655440000") # Use default tenant
    try:
        tenant_uuid = UUID(tenant_id_str)
    except ValueError:
        logger.error(f"Invalid tenant ID format in token: {tenant_id_str}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid tenant ID format")

    logger.info(f"Listing local businesses for tenant {tenant_uuid}, filter_status='{status_filter}', filter_name='{business_name}', page={page}, size={size}, sort_by='{sort_by}', sort_dir='{sort_dir}'")

    try:
        # --- Build Base Query --- #
        select_stmt = select(LocalBusiness).where(LocalBusiness.tenant_id == tenant_uuid)
        count_stmt = select(func.count()).select_from(LocalBusiness).where(LocalBusiness.tenant_id == tenant_uuid)

        # --- Apply Filters --- #
        if status_filter:
            # Ensure filtering uses the exact enum member
            select_stmt = select_stmt.where(LocalBusiness.status == status_filter)
            count_stmt = count_stmt.where(LocalBusiness.status == status_filter)

        if business_name:
            select_stmt = select_stmt.where(LocalBusiness.business_name.ilike(f"%{business_name}%")) # Case-insensitive search
            count_stmt = count_stmt.where(LocalBusiness.business_name.ilike(f"%{business_name}%"))

        # --- Calculate Total Count --- #
        total_result = await session.execute(count_stmt)
        total = total_result.scalar_one_or_none() or 0
        total_pages = math.ceil(total / size) if size > 0 else 0

        # --- Apply Sorting --- #
        sort_field = sort_by if sort_by else "updated_at" # Ensure sort_field is str
        sort_column = getattr(LocalBusiness, sort_field, LocalBusiness.updated_at)

        if sort_dir and sort_dir.lower() == "asc":
            select_stmt = select_stmt.order_by(sort_column.asc())
        else:
            select_stmt = select_stmt.order_by(sort_column.desc()) # Default desc

        # --- Apply Pagination --- #
        offset = (page - 1) * size
        select_stmt = select_stmt.offset(offset).limit(size)

        # --- Execute Query --- #
        logger.debug(f"Executing LocalBusiness query: {select_stmt}")
        result = await session.execute(select_stmt)
        db_items = result.scalars().all()

        # --- Prepare Response --- #
        # Pydantic models with from_attributes=True handle the conversion
        response_items = [LocalBusinessRecord.from_orm(item) for item in db_items]

        return PaginatedLocalBusinessResponse(
            items=response_items,
            total=total,
            page=page,
            size=size,
            pages=total_pages
        )

    except AttributeError:
         # Use the confirmed sort_field in the error message
         logger.warning(f"Invalid sort_by field requested: '{sort_field}'. Check available fields on LocalBusiness model.")
         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"Invalid sort field: {sort_field}")
    except Exception as e:
        logger.error(f"Error listing local businesses: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to list local businesses")


# --- PUT Endpoint Implementation --- #
@router.put("/status", status_code=status.HTTP_200_OK)
async def update_local_businesses_status_batch(
    update_request: LocalBusinessBatchStatusUpdateRequest,
    session: AsyncSession = Depends(get_db_session),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, Any]:
    """
    Update the main status for one or more local businesses identified by their UUIDs.

    If the target status is 'Selected', this will also queue the business
    for domain extraction.
    """
    local_business_ids_to_update: List[UUID] = update_request.local_business_ids
    new_api_status = update_request.status # This is LocalBusinessApiStatusEnum
    user_id = current_user.get("user_id", "unknown") # Or however user ID is stored

    logger.info(f"Received request to update status to '{new_api_status.value}' for {len(local_business_ids_to_update)} local businesses by user '{user_id}'.")

    if not local_business_ids_to_update:
        return {"message": "No local business IDs provided.", "updated_count": 0, "queued_count": 0}

    # Map the incoming API status enum member to the DB enum member (PlaceStatusEnum)
    # Compare by NAME for robustness against potential value differences/casing
    target_db_status_member = next((member for member in PlaceStatusEnum if member.name == new_api_status.name), None)

    if target_db_status_member is None:
        # This case handles potential mismatches, e.g., if API enum has 'Not_a_Fit' and DB has 'Not a Fit'
        # A more robust mapping might be needed if values differ significantly besides underscores.
        logger.error(f"API status '{new_api_status.name}' ({new_api_status.value}) has no matching member name in DB PlaceStatusEnum.")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"Invalid status mapping for '{new_api_status.value}'")

    # Determine if domain extraction should be triggered based on the target DB status
    trigger_domain_extraction = (target_db_status_member == PlaceStatusEnum.Selected)
    if trigger_domain_extraction:
        logger.info(f"Target DB status '{target_db_status_member.name}' will trigger domain extraction queueing.")

    # Define which existing domain extraction statuses allow re-queueing (e.g., only if failed or not yet processed)
    eligible_queueing_statuses = [
        None, # Not yet processed
        DomainExtractionStatusEnum.failed # Allow re-queueing failed ones
        # Add other statuses if needed (e.g., completed if re-running is desired)
    ]

    updated_count = 0
    actually_queued_count = 0
    now = datetime.utcnow()

    try:
        async with session.begin(): # Start transaction
            # Fetch the relevant LocalBusiness objects
            stmt_select = select(LocalBusiness).where(LocalBusiness.id.in_(local_business_ids_to_update))
            result = await session.execute(stmt_select)
            businesses_to_process = result.scalars().all()

            if not businesses_to_process:
                logger.warning(f"No local businesses found for the provided UUIDs: {local_business_ids_to_update}")
                # Return success but indicate nothing was updated/queued
                return {
                    "message": "No matching local businesses found for the provided IDs.",
                    "updated_count": 0,
                    "queued_count": 0
                }

            updated_count = len(businesses_to_process)

            # Loop and update attributes in Python before the commit
            for business in businesses_to_process:
                business.status = target_db_status_member # Assign the DB enum member
                business.updated_at = now
                # TODO: Potentially add updated_by field if tracking user modifications

                # Conditional logic for domain extraction queueing
                if trigger_domain_extraction:
                    # Check eligibility (only queue if not already completed/processing etc.)
                    current_extraction_status = business.domain_extraction_status
                    if current_extraction_status in eligible_queueing_statuses:
                        business.domain_extraction_status = DomainExtractionStatusEnum.queued
                        business.domain_extraction_error = None # Clear any previous error
                        actually_queued_count += 1
                        logger.debug(f"Queuing business {business.id} for domain extraction.")
                    else:
                        logger.debug(f"Business {business.id} not queued. Current domain_extraction_status ('{current_extraction_status}') not eligible.")

            # session.begin() handles commit/rollback
            logger.info(f"ORM updates prepared for {updated_count} local businesses.")
            if trigger_domain_extraction:
                logger.info(f"Attempted to queue {actually_queued_count} businesses for domain extraction.")

        # After successful commit
        return {
            "message": f"Successfully updated status for {updated_count} local businesses.",
            "updated_count": updated_count,
            "queued_count": actually_queued_count
        }

    except Exception as e:
        logger.error(f"Error updating local business statuses: {e}", exc_info=True)
        # Let the exception propagate if not handled by session.begin rollback, FastAPI will catch it
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="An internal error occurred while updating statuses.")

# Note: The GET endpoint is now documented above. The previous TODO is removed.
```

### 3.6 `src/main.py`

Added import and registration for the new router:

```python
# Import routers - Refactored to import the router instance directly
# ... other imports ...
from .routers.local_businesses import router as local_businesses_api_router # Import the new router instance

# ... other code ...

# Include all routers - Refactored to use imported instances
# ... other app.include_router calls ...
app.include_router(local_businesses_api_router, prefix="/api/v3") # Use the imported router instance
```

### 3.7 `src/services/sitemap_scheduler.py`

Modified `process_pending_jobs` to include logic for the new domain extraction task:

```python
# ... imports ...
from ..models.local_business import LocalBusiness, DomainExtractionStatusEnum # Import LocalBusiness model and new enum
from ..services.business_to_domain_service import LocalBusinessToDomainService # Import the new service

async def process_pending_jobs(limit: int = 10):
    # ... initialization, counters ...
    domain_extractions_processed = 0 # Counter for new task
    domain_extractions_successful = 0 # Counter for new task

    # ... Existing Sitemap Job Processing ...

    # ... Existing Deep Scan Processing ...

    # --- Process Pending Domain Extractions (New Curation-Driven Method) ---
    try:
        async with get_background_session() as session:
            # Fetch local businesses queued for domain extraction
            stmt_select_lb = (
                select(LocalBusiness)
                .where(LocalBusiness.domain_extraction_status == DomainExtractionStatusEnum.queued)
                .order_by(LocalBusiness.updated_at.asc()) # Process oldest first
                .limit(limit)
                .with_for_update(skip_locked=True) # Avoid race conditions
            )
            result_lb = await session.execute(stmt_select_lb)
            businesses_to_process = result_lb.scalars().all()
            logger.info(f"Found {len(businesses_to_process)} local businesses queued for domain extraction.")

            if not businesses_to_process:
                logger.debug("No local businesses found in 'queued' domain extraction state.")
            else:
                domain_extraction_service = LocalBusinessToDomainService() # Instantiate service

                for business in businesses_to_process:
                    domain_extractions_processed += 1
                    business_id_str = str(business.id)
                    logger.info(f"Processing domain extraction for local_business_id: {business_id_str}")

                    try:
                        # 1. Update domain_extraction_status to Processing
                        stmt_update_processing = (
                            update(LocalBusiness)
                            .where(LocalBusiness.id == business.id)
                            .values(domain_extraction_status=DomainExtractionStatusEnum.processing)
                            .execution_options(synchronize_session=False)
                        )
                        await session.execute(stmt_update_processing)
                        await session.flush() # Flush immediately
                        logger.debug(f"Updated business {business_id_str} domain_extraction_status to processing")

                        # 2. Perform the domain extraction and queuing
                        success = await domain_extraction_service.create_pending_domain_from_local_business(
                            local_business_id=business.id,
                            session=session # Pass the current session
                        )

                        if success:
                            # 3. Update domain_extraction_status to Complete
                            stmt_update_complete = (
                                update(LocalBusiness)
                                .where(LocalBusiness.id == business.id)
                                .values(
                                    domain_extraction_status=DomainExtractionStatusEnum.completed,
                                    domain_extraction_error=None # Clear error on success
                                )
                                .execution_options(synchronize_session=False)
                            )
                            await session.execute(stmt_update_complete)
                            await session.flush()
                            logger.info(f"Successfully completed domain extraction for business {business_id_str}. Status updated to completed.")
                            domain_extractions_successful += 1
                        else:
                            # 4. Domain extraction failed
                            error_message = "Domain extraction/queueing failed. See service logs for details."
                            logger.warning(f"Domain extraction failed for business {business_id_str}. Marking status as failed.")
                            stmt_update_failed = (
                                update(LocalBusiness)
                                .where(LocalBusiness.id == business.id)
                                .values(
                                    domain_extraction_status=DomainExtractionStatusEnum.failed,
                                    domain_extraction_error=error_message
                                )
                                .execution_options(synchronize_session=False)
                            )
                            await session.execute(stmt_update_failed)
                            await session.flush()

                    except Exception as extraction_error:
                        # Catch errors occurring *within the scheduler loop*
                        error_message_loop = str(extraction_error)
                        logger.error(f"Scheduler loop error during domain extraction for business {business_id_str}: {error_message_loop}", exc_info=True)
                        # Attempt to mark as failed using a separate session
                        try:
                            async with get_background_session() as failure_session:
                                async with failure_session.begin():
                                    stmt_update_failed_loop = (
                                        update(LocalBusiness)
                                        .where(LocalBusiness.id == business.id)
                                        .values(
                                            domain_extraction_status=DomainExtractionStatusEnum.failed,
                                            domain_extraction_error=error_message_loop[:1024]
                                        )
                                        .execution_options(synchronize_session=False)
                                    )
                                    await failure_session.execute(stmt_update_failed_loop)
                                    logger.info(f"Updated business {business_id_str} domain_extraction_status to failed via recovery session.")
                        except Exception as update_fail_error:
                            logger.critical(f"CRITICAL: Failed to update business {business_id_str} status to failed after scheduler loop error: {update_fail_error}", exc_info=True)

    except Exception as e:
        logger.error(f"Error fetching or processing pending domain extractions: {str(e)}", exc_info=True)

    # Log completion statistics
    logger.debug("--------------------------------------------------")
    logger.debug(f"BACKGROUND BATCH {batch_id} COMPLETE")
    logger.debug(f"Sitemaps: Processed={sitemaps_processed}, Successful={sitemaps_successful}")
    logger.debug(f"Deep Scans: Processed={deep_scans_processed}, Successful={deep_scans_successful}")
    logger.debug(f"Domain Extractions: Processed={domain_extractions_processed}, Successful={domain_extractions_successful}") # Add new stats
    logger.debug("--------------------------------------------------")
```

## 4. Conclusion

These changes implement the full backend logic required for the Local Business curation to Domain table trigger workflow, following the specifications in the associated work order and established architectural patterns.
