# Background Task Processing Patterns

Two primary patterns for handling background tasks are observed in this application:

1.  **Direct Task Queuing (Used for Email Scanner):**

    - **Trigger:** Initiated directly by an API endpoint call.
    - **Job Record:** A dedicated `Job` record is created _immediately_ upon the API request in the `jobs` table.
    - **Task Execution:** The background task function is added directly to FastAPI's `BackgroundTasks` runner, usually passing the `job_id` (UUID) of the newly created record. Execution starts almost immediately after the API response is sent.
    - **Feedback:** The API endpoint can return the `job_id` immediately, allowing the client to poll a status endpoint.
    - **Use Case:** Suitable for user-initiated actions requiring immediate feedback and dedicated tracking of that specific task instance (e.g., starting a scan, processing a specific user request).

2.  **Scheduler-Driven Polling Queue (Used for Sitemap/Domain/etc. processing):**
    - **Trigger:** Initiated by a scheduled job (e.g., via APScheduler) that runs periodically.
    - **Job Record:** May or may not use a dedicated `Job` record. Often relies on changing a status field on the primary entity's table (e.g., `domains.status = 'pending'`, `places_staging.deep_scan_status = 'Queued'`). A `Job` record might be created _when the scheduler picks up the item_, not upon the initial request (if any).
    - **Task Execution:** The scheduler queries the relevant table for entities with a specific status (e.g., 'pending', 'Queued'). When found, the scheduler calls the processing function for that entity, often in batches. Execution depends on the scheduler's interval.
    - **Feedback:** Feedback is less direct. Status is typically tracked by observing the status field on the primary entity or by querying related `Job` records if used.
    - **Use Case:** Suitable for system-driven batch processing, periodic maintenance tasks, or handling items queued indirectly (e.g., processing all domains marked for analysis).

---

## ASCII Flow Charts

**1. Direct Task Queuing (Email Scanner - `src/routers/email_scanner.py`)**

```mermaid
sequenceDiagram
    participant Client
    participant Router as API Router<br>(email_scanner.py)
    participant Auth as Auth Middleware<br>(jwt_auth.py)
    participant DB as Database
    participant BGTasks as BackgroundTasks<br>(FastAPI)
    participant Task as Background Task<br>(email_scraper.py)

    Client->>Router: POST /api/v3/scan/website (domain_id)
    Router->>Auth: Verify Token
    Auth-->>Router: User Info (user_id)
    Router->>DB: Check for existing PENDING/RUNNING Job for domain_id
    DB-->>Router: Existing Job? (Yes/No)
    alt Existing Job Found (PENDING/RUNNING)
        Router-->>Client: 202 Accepted (existing job_id)
    else No Existing Job / Completed / Failed
        Router->>DB: Create Job record (type='email_scan', status='pending', domain_id, user_id)
        DB-->>Router: New Job (with job_id UUID)
        Router->>BGTasks: Add task: scan_website_for_emails(job_id, user_id)
        Router-->>Client: 202 Accepted (new job_id)
    end
    BGTasks->>Task: Execute scan_website_for_emails(job_id, user_id)
    Task->>DB: Get Job by job_id
    Task->>DB: Update Job status to 'running'
    Task->>Task: Perform scraping...
    alt Scan Successful
        Task->>DB: Update Job (status='complete', progress=1.0, result_data=[emails])
    else Scan Failed
        Task->>DB: Update Job (status='failed', error='message')
    end

    Client->>Router: GET /api/v3/scan/status/{job_id}
    Router->>DB: Get Job by job_id
    DB-->>Router: Job Details (status, progress, etc.)
    Router-->>Client: 200 OK (Job Details)

```

**2. Scheduler-Driven Polling Queue (Example: Sitemap/DeepScan - `src/services/sitemap_scheduler.py`)**

```mermaid
sequenceDiagram
    participant Scheduler as APScheduler<br>(scheduler_instance.py)
    participant Service as Scheduler Service<br>(sitemap_scheduler.py)
    participant DB as Database
    participant Processor as Processing Logic<br>(e.g., places_deep_service.py)

    Scheduler->>Service: Run process_pending_jobs() (Scheduled Interval)
    Service->>DB: SELECT places WHERE deep_scan_status='Queued' LIMIT N FOR UPDATE SKIP LOCKED
    DB-->>Service: List of Place records
    loop For Each Place Record
        Service->>DB: Update Place status to 'Processing' (in memory or execute)
        Service->>Processor: process_single_deep_scan(place_id)
        Processor->>Processor: Perform deep scan...
        alt Scan Successful
            Service->>DB: Update Place status to 'Completed' (in memory or execute)
        else Scan Failed
            Service->>DB: Update Place status to 'Error', set error message (in memory or execute)
        end
    end
    Service->>DB: Commit Session (updates statuses for the batch)

```

_(Note: The exact implementation details like session management within the loop might vary, but this shows the general flow.)_

---

## Dependency/Interaction Overview

```mermaid
graph TD
    subgraph "Direct Task Queuing (Email Scan)"
        A[Client] --> B(API: /scan/website);
        B --> C{email_scanner.py};
        C --> D[Auth: jwt_auth.py];
        C --> E[DB: Job Model];
        C --> F[FastAPI BackgroundTasks];
        F --> G{email_scraper.py};
        G --> E;
        H[Client] --> I(API: /scan/status);
        I --> C;
    end

    subgraph "Scheduler-Driven Polling"
        J[APScheduler] --> K{Scheduler Service<br>e.g., sitemap_scheduler.py};
        K --> L[DB: Place/Domain Model];
        K --> M{Processing Logic<br>e.g., places_deep_service.py};
        M --> L;
    end

    E --> Z[(Database Tables)];
    L --> Z;

```

---

## Key File Paths

- **Direct Task Queuing (Email Scanner):**

  - Router: `src/routers/email_scanner.py`
  - Task: `src/tasks/email_scraper.py`
  - Job Model: `src/models/job.py`
  - Auth: `src/auth/jwt_auth.py`
  - Base Model/DB Session: `src/models/base.py`, `src/session/async_session.py`

- **Scheduler-Driven Polling (Example: Deep Scan):**
  - Scheduler Service: `src/services/sitemap_scheduler.py`
  - Entity Model: `src/models/place.py`
  - Processing Logic: `src/services/places/places_deep_service.py`
  - Shared Scheduler: `src/scheduler_instance.py`
  - Base Model/DB Session: `src/models/base.py`, `src/session/async_session.py`

---
