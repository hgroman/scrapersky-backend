# C.R.A.F.T. Framework: Domain Scheduler Debugging

## CRITICAL OPERATIONAL INSTRUCTIONS

**IMPORTANT:** All operations for this debugging task MUST be executed using Docker Compose. Use the following pattern for all commands:

```bash
docker-compose exec scrapersky <command>  # For executing commands inside the container
docker-compose logs --tail=100            # For viewing logs
```

**TEST METHOD:** To test domain processing, use CURL commands to add domains via the API endpoint:

```bash
curl -X POST "http://localhost:8000/api/v3/modernized_page_scraper/scan" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer scraper_sky_2024" \
  -d '{"base_url":"example.com", "max_pages":5}'
```

**CRITICAL JOB TRACKING:** After submission, you'll receive a response with a UUID job_id:

```json
{
  "job_id": "e9895e9a-5337-4830-b295-83e14c028f8c",
  "status_url": "/api/v3/modernized_page_scraper/status/e9895e9a-5337-4830-b295-83e14c028f8c",
  "created_at": "2025-03-31T13:00:35.649776"
}
```

When checking status using this UUID, you'll notice the response contains a different numeric job_id:

```bash
curl "http://localhost:8000/api/v3/modernized_page_scraper/status/e9895e9a-5337-4830-b295-83e14c028f8c"
```

Response:

```json
{
  "job_id": "310",  # This is the internal database ID
  "status": "processing",
  "domain": null,
  "progress": 0.0,
  ...
}
```

Use the UUID for API requests and the numeric ID when querying the database directly:

```bash
docker-compose exec scrapersky python scripts/db/simple_inspect.py "SELECT * FROM jobs WHERE id = 310"
# OR to find domains being processed:
docker-compose exec scrapersky python scripts/db/simple_inspect.py "SELECT * FROM domains WHERE batch_id = '310'"
```

This is the standard method for testing domain processing, simulating actual user interaction.

## C - Context

You are debugging a critical background task scheduler in the ScraperSky backend that processes domains from the database. There is one key issue:

**Domains are not being processed** - When users submit domains via the Single Domain Scanner interface, they remain unprocessed

The scheduler shows signs of initializing correctly:

```
2025-03-31 05:56:35,879 - src.services.domain_scheduler - INFO - Setting up domain processing scheduler
2025-03-31 05:56:35,880 - src.services.domain_scheduler - INFO - Domain processing scheduler set up successfully
```

There are deprecation warnings about using `@app.on_event()` instead of the newer lifespan pattern that should be addressed.

The interdependencies between the web interface, API, database, and scheduler are complex. The Single Domain Scanner frontend calls an API endpoint that adds domains to the database with a specific status, and the scheduler should pick these up on a 15-minute interval.

## R - Role

You are an expert FastAPI backend developer with deep experience in:

- APScheduler implementation within FastAPI applications
- Async database operations with SQLAlchemy
- Background task debugging and monitoring
- Transaction management in distributed systems

Your task is to diagnose and fix the domain scheduler issues by systematically verifying each component in the processing chain.

## A - Action

1. **Update to modern lifespan pattern**:

   - Migrate from deprecated `@app.on_event()` to the newer lifespan pattern
   - Implement proper shutdown handling for the scheduler

2. **Validate database connectivity**:

   - Use the provided database tools to inspect the domains table
   - Verify connection string format and Supavisor pooling settings
   - Test direct database queries to ensure connectivity

3. **Verify status value consistency**:

   - Check what status value the scheduler is looking for in domains
   - Confirm what status value the API endpoint sets
   - Resolve any mismatch between these values

4. **Test the metadata extractor directly**:

   - Use the standalone test script to isolate the metadata extraction functionality
   - Verify it can successfully update domain records
   - Identify any errors in the extraction process

5. **Monitor the scheduler operation**:
   - Start the server with the fixes in place
   - Add test domains with the correct status
   - Monitor logs to verify the scheduler processes domains

## F - Format

Present your findings and solutions as:

1. **Step-by-step diagnostic results** with:

   - Command outputs
   - Log snippets
   - Database query results

2. **Root cause analysis** identifying:

   - Primary issue(s)
   - Secondary contributing factors
   - System design considerations

3. **Implementation plan** with:

   - Code changes required (with diffs)
   - Configuration updates
   - Testing procedures

4. **Verification steps** to confirm:
   - Server starts properly
   - Scheduler runs on schedule
   - Domains are processed correctly

## T - Target Audience

- **Primary**: DevOps/Backend engineers who need to fix the scheduler
- **Secondary**: Future developers who will maintain the system
- **Tertiary**: Product managers who need to understand system limitations

---

# Work Order: Domain Background Scheduler Debugging

## Project Context

ScraperSky is a FastAPI-based web scraping and analytics system with:

- Modern async architecture
- SQLAlchemy 2.0 integration with async support
- Multi-tenant design with complete tenant isolation

**CRITICAL DATABASE REQUIREMENTS:**

- All database connections MUST use Supavisor connection pooling
- Connection string format: `postgresql+asyncpg://postgres.your-project:password@aws-0-us-west-1.pooler.supabase.com:6543/postgres`
- Connection pooling parameters must be properly configured

The domain scheduler is a background task that processes domains in the database, extracting metadata, and updating records. It's initialized during FastAPI application startup via `@app.on_event("startup")`.

## Current Status

The domain scheduler initializes successfully during application startup but we have not verified active domain processing.

```
2025-03-31 05:56:35,879 - src.services.domain_scheduler - INFO - Setting up domain processing scheduler
INFO:src.services.domain_scheduler:Setting up domain processing scheduler
2025-03-31 05:56:35,880 - src.services.domain_scheduler - INFO - Domain processing scheduler set up successfully
INFO:src.services.domain_scheduler:Domain processing scheduler set up successfully
```

The server starts correctly, but we need to verify if the scheduler is properly processing domains.

## Complete Single Domain Scanner Flow Analysis

After analyzing the single-domain-scanner.html frontend code and related dependencies, we've identified the complete flow of data through the system:

### 1. Frontend Interface (static/single-domain-scanner.html)

```
┌─────────────────────────────────────┐
│ HTML Form                           │
│  └── Submits domain for scanning    │
└────────────────┬────────────────────┘
                 │ AJAX POST to /api/v3/modernized_page_scraper/scan
                 ▼
┌─────────────────────────────────────┐
│ JavaScript Client                   │
│  └── Initiates polling for results  │
└────────────────┬────────────────────┘
                 │ Polls status_url every 2 seconds
                 ▼
┌─────────────────────────────────────┐
│ Results Display                     │
│  └── Shows domain metadata          │
└─────────────────────────────────────┘
```

The frontend code clearly shows:

- Domain submission creates a job but doesn't wait for processing
- Client polls a status URL until job completes
- Status changes from "pending" → "processing" → "completed"

### 2. API Layer (src/routers/modernized_page_scraper.py)

```
┌─────────────────────────────────────┐
│ Router Endpoint                     │
│ POST /api/v3/modernized_page_scraper/scan │
│  └── Creates job record in database │
└────────────────┬────────────────────┘
                 │ Returns immediately with job_id
                 ▼
┌─────────────────────────────────────┐
│ Status Endpoint                     │
│ GET /api/v3/modernized_page_scraper/status/{job_id} │
│  └── Checks current job status      │
└─────────────────────────────────────┘
```

Key insights:

- Router only creates job record with status="pending"
- No immediate processing occurs
- Returns job_id and status_url for polling

### 3. Backend Data Flow

```
┌─────────────────────────────────────┐
│ Database (domains table)            │
│  └── Stores domains with status="pending" │
└────────────────┬────────────────────┘
                 │ Read by scheduler every 15 minutes
                 ▼
┌─────────────────────────────────────┐
│ Domain Scheduler                    │
│ src/services/domain_scheduler.py    │
│  └── Processes pending domains      │
└────────────────┬────────────────────┘
                 │ Uses
                 ▼
┌─────────────────────────────────────┐
│ Metadata Extractor                  │
│ src/scraper/metadata_extractor.py   │
│  └── Extracts site metadata         │
└────────────────┬────────────────────┘
                 │ Updates
                 ▼
┌─────────────────────────────────────┐
│ Database (domains table)            │
│  └── Updates status to "processed"  │
└─────────────────────────────────────┘
```

Critical findings:

- The API endpoint creates domain records but doesn't process them
- The domain scheduler runs separately on a timer (every 15 minutes)
- Records remain in "pending" status until scheduler processes them
- Domains submitted via the interface aren't being processed despite the server running correctly

### 4. Dependency Chain

According to the dependency map:

1. The Single Domain Scanner (API) uses `src/scraper/metadata_extractor.py`
2. The Domain Scheduler also uses the same `detect_site_metadata()` function
3. Both systems are designed to work with the same domains table
4. The scheduler is designed to pick up any domains with "pending" status, regardless of how they were added

This confirms that submitted domains via the frontend interface should be processed by the background scheduler if both components are working correctly.

## Scheduler Dependency Trace

```
src/main.py
├── @app.on_event("startup")
│   └── src/services/domain_scheduler.py
│       ├── setup_domain_scheduler()
│       │   └── AsyncIOScheduler (from apscheduler.schedulers.asyncio)
│       └── process_pending_domains()
│           ├── src/session/background.py
│           │   └── get_background_session()
│           │       └── src/db/session.py or src/db/direct_session.py
│           ├── src/scraper/domain_utils.py
│           │   └── standardize_domain()
│           └── src/scraper/metadata_extractor.py
│               └── detect_site_metadata()
│                   ├── src/utils/scraper_api.py (likely)
│                   │   └── HTTP client functionality
│                   └── BeautifulSoup (likely)
└── @app.on_event("shutdown")
    └── src/services/domain_scheduler.py
        └── shutdown_domain_scheduler()
```

## Key Files & Locations

```
scraper-sky-backend/
├── src/
│   ├── main.py                        # Application entry point with scheduler initialization
│   ├── services/
│   │   └── domain_scheduler.py        # APScheduler configuration and domain processing
│   ├── scraper/
│   │   ├── metadata_extractor.py      # Contains detect_site_metadata() function
│   │   └── domain_utils.py            # URL standardization utilities
│   └── session/
│       └── background.py              # Async session management for background tasks
├── logs/                              # Location of application logs
└── project-docs/11-Background-Task-Scheduler/
    └── DOMAIN_SCHEDULER_MODERNIZATION.md  # Detailed documentation of scheduler
```

## Test Methodology for Scheduler Verification

When testing the scheduler, use the following approach:

### 1. Create a Domain via API

Use the API endpoint to create a domain:

```bash
curl -X POST "http://localhost:8000/api/v3/modernized_page_scraper/scan" \
  -H "Content-Type: application/json" \
  -d '{"base_url":"example.com", "max_pages":5}'
```

This will return a JSON response with a `job_id` and `status_url`:

```json
{
  "job_id": "12345678-1234-1234-1234-123456789abc",
  "status_url": "/api/v3/modernized_page_scraper/status/12345678-1234-1234-1234-123456789abc"
}
```

### 2. Check Job Status via API

Rather than querying the database directly, use the `job_id` to check the status:

```bash
curl "http://localhost:8000/api/v3/modernized_page_scraper/status/12345678-1234-1234-1234-123456789abc"
```

This will show the current processing status:

```json
{
  "job_id": "12345678-1234-1234-1234-123456789abc",
  "status": "processing", // Will change to "completed" when done
  "domain": "example.com",
  "progress": 0.5,
  "created_at": "2025-03-31T11:43:02.505051",
  "updated_at": "2025-03-31T11:43:02.505051",
  "metadata": {
    "domain": "example.com",
    "max_pages": 5
  }
}
```

### 3. Monitor Status Changes

The scheduler should pick up the domain within its processing interval (previously 15 minutes, now 1 minute) and process it. Status will change from:

- `pending` → Initial state when domain is added
- `processing` → When the scheduler picks it up
- `completed` → After successful processing
- `error` → If processing fails

This approach provides end-to-end verification of the entire system without needing to directly query the database.

## Relevant Architecture Guides

Before debugging, review these critical guides from `Docs/Docs_1_AI_GUIDES/`:

1. **[21-SCHEDULED_TASKS_APSCHEDULER_PATTERN.md](../../Docs/Docs_1_AI_GUIDES/21-SCHEDULED_TASKS_APSCHEDULER_PATTERN.md)**

   - Contains the official pattern for implementing APScheduler tasks
   - Details proper initialization, error handling, and shutdown procedures
   - Explains FastAPI integration best practices

2. **[07-DATABASE_CONNECTION_STANDARDS.md](../../Docs/Docs_1_AI_GUIDES/07-DATABASE_CONNECTION_STANDARDS.md)**

   - Critical connection pooling requirements
   - Background task database connection patterns
   - Connection cleanup protocols

3. **[20-DATABASE_CONNECTION_ASYNCPG_COMPATIBILITY.md](../../Docs/Docs_1_AI_GUIDES/20-DATABASE_CONNECTION_ASYNCPG_COMPATIBILITY.md)**

   - Specific requirements for asyncpg
   - Solutions for common async database issues

4. **[17-CORE_ARCHITECTURAL_PRINCIPLES.md](../../Docs/Docs_1_AI_GUIDES/17-CORE_ARCHITECTURAL_PRINCIPLES.md)**
   - Background task design principles
   - Error handling and recovery patterns

## Debug Objectives

1. Verify the scheduler processes domains as expected
2. Confirm database updates occur for processed domains
3. Validate metadata extraction is working properly
4. Ensure error handling functions as designed

## Debug Tasks

### 1. Add Enhanced Logging

- **Location**: `src/services/domain_scheduler.py`
- **Changes needed**:

  ```python
  async def process_pending_domains():
      job_id = f"domain_batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
      logging.info(f"Starting domain processing task - Job ID: {job_id}")
      domains_processed = 0
      domains_successful = 0

      # Add more logging throughout the function

      # At the end of the function:
      logging.info(f"Job {job_id} completed: Processed {domains_processed} domains, {domains_successful} successful")
  ```

### 2. Database Schema Alignment Fix

- **Issue**: The scheduler code was attempting to write to a non-existent column `error_message` in the domains table
- **Fix Applied**: Updated the error handling SQL to use the `last_error` column which does exist in the schema:

```diff
# In src/services/domain_scheduler.py
await session.execute(
    text("""
    UPDATE domains
    SET status = 'error',
        updated_at = NOW(),
-       error_message = :error_message
+       last_error = :error_message
    WHERE id = :id
    """).bindparams(id=domain_id, error_message=error_message)
)
```

- **Result**: This change allows the scheduler to properly record error messages in the database, fixing the SQL error:

```
sqlalchemy.exc.ProgrammingError: column "error_message" of relation "domains" does not exist
```

- **Verification**: Logs now show successful updates to error status with message:

```
Updated domain 9b53e88b-117a-4985-93e9-17d4d13a50bb status to 'error' with message: Invalid domain format: None
```

### 3. Scheduler Interval Change

- **Original Setting**: Scheduler was set to run every 15 minutes
- **Updated Setting**: Modified to run every 1 minute for faster testing:

```diff
# In src/services/domain_scheduler.py
# Add job to process pending domains every 1 minute
scheduler.add_job(
    process_pending_domains,
-   IntervalTrigger(minutes=15),
+   IntervalTrigger(minutes=1),
    id="process_pending_domains",
    replace_existing=True,
    kwargs={"limit": 10}  # Process up to 10 domains each time
)
```

### 4. Prepare Test Data

- Insert test domains into the database:
  ```sql
  INSERT INTO domains (url, status, created_at)
  VALUES
    ('example.com', 'pending', NOW()),
    ('test-site.org', 'pending', NOW()),
    ('brokensite.invalid', 'pending', NOW()),
    ('wordpress-example.com', 'pending', NOW()),
    ('contact-page-test.com', 'pending', NOW());
  ```

### 5. Connection to Single Domain Scanner

The domain scheduler is designed to work in conjunction with the `/api/v3/modernized_page_scraper/scan` endpoint used by the Single Domain Scanner interface. The workflow is:

1. User submits a domain via the single-domain-scanner.html interface
2. The API endpoint adds the domain to the database with `status="pending"`
3. The domain scheduler, running every 1 minute, picks up pending domains and processes them
4. The scheduler changes the domain status to `"processed"` when complete

This connection explains why domains submitted through the scanner interface aren't being processed - the scheduler is initialized but is not properly processing the pending domains.

### 6. Debugging Session

- Run the server for at least 30 minutes to capture multiple scheduler cycles

## Critical Root Cause: Column Name Mismatch

After thorough investigation, we examined the potential issue of column name mismatch. The expected code problem would be the scheduler accessing a column named `url` which doesn't exist in the database.

However, upon inspection of the code, we found it was already correctly using:

```python
# In src/services/domain_scheduler.py
domain_dict = dict(row)
domain_id = domain_dict.get('id')
url = domain_dict.get('domain')  # Correctly using 'domain' column
```

This suggests the issue must be elsewhere. Further analysis of failing domains shows they are correctly being picked up by the scheduler but failing during the scraping process with:

```
ERROR:src.utils.scraper_api:Attempt 2 failed: HTTP 500: Request failed. You will not be charged for this request. Please make sure your url is correct and try again. Protected domains may require adding premium=true OR ultra_premium=true parameter to your request.
```

Even when using `premium=True` as a fallback, the scraping still fails:

```
ERROR:src.utils.scraper_api:SDK Attempt 1 failed: Failed to scrape GET https://test-faster-scheduler.com?render_js=True&premium=True
```

### Revised Root Cause:

The actual issue appears to be with the ScraperAPI service or configuration:

1. The API is returning HTTP 500 errors consistently
2. The error message suggests domains might require `ultra_premium=true` parameter
3. The API key or account might have insufficient permissions/credits

To fix this, we should:

1. Examine the ScraperAPI key configuration in environment variables
2. Verify the account status and available credits
3. Try modifying the code to use `ultra_premium=true` parameter

### 8. Database Verification

- Check domain status changes:
  ```sql
  SELECT id, url, status, processed_at,
         metadata->>'title' as title,
         email_addresses, phone_numbers, wordpress_detected
  FROM domains
  WHERE status IN ('processed', 'error')
  AND processed_at > '2025-03-31 00:00:00';
  ```

## Expected Results

1. **Scheduler Initialization**: Successful (already confirmed)
2. **Periodic execution**: Log entries showing regular task runs (every 15 minutes)
3. **Domain Processing**: Successful extraction of metadata
4. **Database Updates**: Domains transition from 'pending' to 'processed'
5. **Metadata Extraction**: Email addresses, phone numbers, and WordPress detection functioning

## Critical Environment Variables

```bash
# Required for database connections
DATABASE_URL=postgresql+asyncpg://postgres.your-project:password@aws-0-us-west-1.pooler.supabase.com:6543/postgres

# Set for development
ENVIRONMENT=development

# For scheduler configuration
DOMAIN_PROCESSING_INTERVAL_MINUTES=15  # Default if not specified
```

## Connection Requirements

When implementing session handling in the scheduler:

- ALWAYS use Supavisor connection pooling
- Set appropriate pool size (min: 5, recommended: 10)
- Disable prepared statements for complex queries with `no_prepare=true`
- Consider setting `statement_cache_size=0` for problematic queries

## Known Issues

1. FastAPI deprecation warning for on_event (should migrate to lifespan handlers)
2. No verification of scheduler execution beyond initialization

## Migration to Lifespan Handlers

Per the deprecation warning in logs, we should upgrade from @app.on_event to the newer lifespan pattern:

```python
# In src/main.py

from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup: setup scheduler
    from src.services.domain_scheduler import setup_domain_scheduler
    scheduler = setup_domain_scheduler()
    yield
    # Shutdown: stop scheduler
    from src.services.domain_scheduler import shutdown_domain_scheduler
    shutdown_domain_scheduler()

app = FastAPI(lifespan=lifespan)
```

## References

- Primary implementation: `src/services/domain_scheduler.py`
- Metadata extraction: `src/scraper/metadata_extractor.py`
- FastAPI initialization: `src/main.py`
- Integration document: See `DOMAIN_SCHEDULER_MODERNIZATION.md`
- APScheduler Pattern Guide: `Docs/Docs_1_AI_GUIDES/21-SCHEDULED_TASKS_APSCHEDULER_PATTERN.md`

## Database Access Instructions

To debug the domain scheduler, you'll need to inspect the domains table directly. After testing various approaches, here are the exact commands and steps that work:

### 1. View Domains Table Schema and Data

```bash
# Run from project root - NOT from within scripts/db directory
cd $(pwd) && python scripts/db/simple_inspect.py domains --limit 20
```

This will display the full schema and 20 rows from the domains table. Key fields to examine:

- `status`: Overall domain status ('active', 'processing', 'completed', etc.)
- `content_scrape_status`: More specific status for content scraping ('queued', 'complete', 'error')
- `page_scrape_status`: Status for page scraping
- `sitemap_monitor_status`: Status for sitemap monitoring

### 2. Check Scheduler Status in Logs

```bash
# View the most recent logs looking for scheduler activity
tail -n 100 logs/app.log | grep -i scheduler
```

### 3. Add Test Domain for Scheduler Processing

The domain scheduler processes domains with `status='pending'`. To test if the scheduler is working:

```bash
# Run from project root
cd $(pwd) && python -c "
import asyncio
import uuid
from sqlalchemy import text
from src.session.background import get_background_session

async def add_test_domain():
    async with get_background_session() as session:
        async with session.begin():
            query = text(\"\"\"
            INSERT INTO domains (id, tenant_id, domain, status, created_at, updated_at)
            VALUES (:id, '550e8400-e29b-41d4-a716-446655440000', :domain, 'pending', NOW(), NOW())
            \"\"\")
            await session.execute(query, {
                'id': str(uuid.uuid4()),
                'domain': f'test-scheduler-{uuid.uuid4()}.com'
            })
            print('Test domain added with status=pending')

asyncio.run(add_test_domain())
"
```

### 4. Verify Domain Processing

After adding a test domain and ensuring the server is running properly for at least 15 minutes:

```bash
# Check if the test domain was processed (status should change from 'pending')
cd $(pwd) && python scripts/db/simple_inspect.py domains --limit 50
```

The scheduler successfully processed the domain if its status changed from `'pending'` to something else (likely `'processed'` or `'completed'`).

### 5. Common Status Values Observed in Production

Based on database inspection, these are the common status values found:

- 'active' - Default status for most domains
- 'processing' - Currently being processed
- 'completed' - Processing finished successfully
- 'error' - Processing failed
- 'pending' - Awaiting processing by the scheduler

Note that the scheduler specifically looks for domains with `status='pending'`, which may be different from the page scan-specific statuses like `page_scrape_status='queued'`.

## Status Value Verification

Our analysis suggests a critical issue might be a **mismatch between the status values** set by the web interface and what the scheduler looks for. Here's how to verify and fix this:

### 1. Verify What Status the Scheduler Looks For

Check the domain scheduler implementation:

```bash
# View the scheduler code to see what status it's looking for
grep -n "status" src/services/domain_scheduler.py
```

The scheduler likely has code similar to this:

```python
# Example of what to look for in scheduler code
query = text("SELECT id, domain FROM domains WHERE status = 'pending' LIMIT :limit")
```

### 2. Verify What Status the Web Interface Sets

Check the modernized page scraper API endpoint:

```bash
# View the API endpoint code that creates domain records
grep -n "status" src/routers/modernized_page_scraper.py
```

Look for code like:

```python
# Example of what to look for in API code
domain = Domain(url=url, status="pending")  # or some other status value
```

### 3. Test Domain Creation via API

Use the API endpoint to create a domain and check what status it sets:

```bash
# Call the API endpoint
curl -X POST -H "Content-Type: application/json" -H "Authorization: Bearer scraper_sky_2024" \
  -d '{"base_url":"test-api-check.com", "max_pages":5}' \
  http://localhost:8080/api/v3/modernized_page_scraper/scan

# Immediately check what status was set
cd $(pwd) && python scripts/db/simple_inspect.py "domains" --limit 5
```

### 4. Manually Fix Status Mismatch

If you find a mismatch between what the API sets and what the scheduler looks for, update domains manually:

```bash
# Run from project root
cd $(pwd) && python -c "
import asyncio
from sqlalchemy import text
from src.session.background import get_background_session

async def update_domain_statuses():
    async with get_background_session() as session:
        async with session.begin():
            # Replace 'status_api_sets' with what you found in step 2
            # Replace 'status_scheduler_expects' with what you found in step 1
            query = text(\"\"\"
            UPDATE domains
            SET status = :new_status
            WHERE status = :old_status
            RETURNING id, domain, status
            \"\"\")
            result = await session.execute(query, {
                'old_status': 'status_api_sets',
                'new_status': 'status_scheduler_expects'
            })
            rows = result.fetchall()
            print(f'Updated {len(rows)} domains to correct status')
            for row in rows:
                print(f'  - {row.domain}: {row.status}')

asyncio.run(update_domain_statuses())
"
```

### 5. Verify the Fix

After updating the statuses:

1. Restart the server with the correct port
2. Wait for at least one scheduler cycle (15 minutes)
3. Check if domains are being processed:

```bash
# Check for processed domains
cd $(pwd) && python scripts/db/simple_inspect.py domains --limit 50
```

If domains are successfully moving from the status the scheduler looks for to a completed status, the mismatch has been resolved.

## Direct Metadata Extractor Testing

Before spending hours debugging the scheduler, we should verify the metadata extractor itself works. This test bypasses the scheduler to isolate whether the domain processing works independently of scheduling issues:

### 1. Standalone Metadata Extractor Test

Create a test script in the project root:

```bash
# Create test script
cat > test_metadata_extractor.py << 'EOF'
import asyncio
import logging
import time
import uuid
from datetime import datetime
from sqlalchemy import text

from src.session.background import get_background_session
from src.scraper.metadata_extractor import detect_site_metadata
from src.scraper.domain_utils import standardize_domain

# Configure logging
logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("metadata-extractor-test")

async def test_domain_processing(test_domain: str):
    """Process a single domain and print results."""
    start_time = time.time()
    logger.info(f"Testing metadata extraction for: {test_domain}")

    # 1. Standardize domain (same as scheduler would do)
    standardized_domain = standardize_domain(test_domain)
    logger.info(f"Standardized domain: {standardized_domain}")

    # 2. Create test domain record
    domain_id = None
    try:
        async with get_background_session() as session:
            async with session.begin():
                query = text("""
                INSERT INTO domains (id, tenant_id, domain, status, created_at, updated_at)
                VALUES (:id, '550e8400-e29b-41d4-a716-446655440000', :domain, 'test-extraction', NOW(), NOW())
                RETURNING id
                """)
                result = await session.execute(query, {
                    'id': str(uuid.uuid4()),
                    'domain': standardized_domain
                })
                domain_id = result.scalar_one()
                logger.info(f"Created test domain record with ID: {domain_id}")
    except Exception as e:
        logger.error(f"Failed to create domain record: {e}")
        return

    # 3. Extract metadata
    try:
        logger.info("Calling detect_site_metadata...")
        metadata = await detect_site_metadata(standardized_domain)
        logger.info(f"Metadata extraction completed")

        # Print key metadata fields for quick inspection
        logger.info(f"Title: {metadata.get('title', 'None')}")
        logger.info(f"Description: {metadata.get('description', 'None')}")
        logger.info(f"WordPress: {metadata.get('is_wordpress', False)}")

        # Full metadata for debugging
        logger.info(f"Full metadata: {metadata}")

        # 4. Update domain record with results
        async with get_background_session() as session:
            async with session.begin():
                update = text("""
                UPDATE domains
                SET
                    status = 'extraction-complete',
                    title = :title,
                    description = :description,
                    is_wordpress = :is_wordpress,
                    phone_numbers = :phone_numbers,
                    email_addresses = :email_addresses,
                    domain_metadata = :metadata,
                    updated_at = NOW()
                WHERE id = :id
                """)
                await session.execute(update, {
                    'id': domain_id,
                    'title': metadata.get('title', ''),
                    'description': metadata.get('description', ''),
                    'is_wordpress': metadata.get('is_wordpress', False),
                    'phone_numbers': metadata.get('phone_numbers', []),
                    'email_addresses': metadata.get('email_addresses', []),
                    'metadata': metadata
                })
                logger.info("Updated domain record with metadata")

    except Exception as e:
        logger.error(f"Error in metadata extraction: {e}", exc_info=True)

        # Update domain with error
        try:
            async with get_background_session() as session:
                async with session.begin():
                    update = text("""
                    UPDATE domains
                    SET
                        status = 'extraction-error',
                        last_error = :error,
                        updated_at = NOW()
                    WHERE id = :id
                    """)
                    await session.execute(update, {
                        'id': domain_id,
                        'error': str(e)
                    })
        except Exception as update_error:
            logger.error(f"Error updating domain with error: {update_error}")

    # 5. Report execution time
    execution_time = time.time() - start_time
    logger.info(f"Metadata extraction test completed in {execution_time:.2f} seconds")

    # 6. Retrieve and display the updated domain record
    try:
        async with get_background_session() as session:
            query = text("SELECT id, domain, status, title, description, is_wordpress FROM domains WHERE id = :id")
            result = await session.execute(query, {'id': domain_id})
            domain_record = result.mappings().one()
            logger.info(f"Updated domain record: {dict(domain_record)}")
    except Exception as e:
        logger.error(f"Error retrieving updated domain: {e}")

if __name__ == "__main__":
    import sys

    # Get domain from command line or use default
    test_domain = sys.argv[1] if len(sys.argv) > 1 else "example.com"

    # Run the test
    asyncio.run(test_domain_processing(test_domain))
EOF

# Make the script executable
chmod +x test_metadata_extractor.py
```

### 2. Run the Test with a Sample Domain

```bash
python test_metadata_extractor.py example.com
```

Or test with one of the domains we saw in the database:

```bash
python test_metadata_extractor.py scrapersky.com
```

### 3. Examine the Results

This test will:

1. Create a domain record with special status "test-extraction"
2. Directly call `detect_site_metadata()` (the same function used by the scheduler)
3. Update the domain record with the results
4. Output logs of the entire process

### 4. What to Look For

- **Success**:

  - Log shows "Metadata extraction completed"
  - Domain record is updated with title, description, etc.

- **Failure**:
  - Error logs from metadata extraction
  - Domain record status set to "extraction-error"
  - Last error field populated with specific error

### 5. Troubleshooting Metadata Extraction

If the direct extraction test fails, check:

- Network connectivity to external sites
- Rate limiting or IP blocking by target sites
- Python dependencies for BeautifulSoup, requests, etc.
- Environment variables or configuration needed by the extractor

By isolating the metadata extractor from the scheduler, we can determine if:

1. The scheduler is failing to run at all
2. The scheduler runs but the metadata extraction fails
3. Both components work individually but fail to work together

This insight will save significant debugging time by narrowing down the problem scope.
