# 13.10 SITEMAP BATCH ENDPOINT IMPLEMENTATION WORK ORDER

**Document ID:** 13.10-SITEMAP_BATCH_ENDPOINT
**Status:** Pending Implementation
**Created:** April 2023
**Author:** System Architecture Team

## 1. EXECUTIVE SUMMARY

This work order describes the implementation of a batch endpoint for the sitemap scanner. This endpoint will allow users to submit multiple domains for sitemap discovery and processing in a single request, leveraging the existing batch processing architecture of the system.

## 2. OBJECTIVE

Create a batch endpoint for sitemap scanning that:

1. Accepts multiple domains in a single request
2. Creates individual job records for each domain
3. Processes domains concurrently with controlled parallelism
4. Maintains proper transaction boundaries
5. Provides comprehensive status tracking
6. Follows established architectural patterns

## 3. RATIONALE

The existing sitemap scanner endpoint (`/api/v3/sitemap/scan`) only accepts one domain at a time. Customers frequently need to scan multiple domains, which currently requires sending multiple separate requests. A batch endpoint will improve efficiency, reduce API call overhead, and provide a single tracking point for groups of related domains.

## 4. ARCHITECTURAL APPROACH

This implementation will repurpose the existing batch processing architecture used by the domain scraper, adapting it for sitemap processing. This ensures consistency across the codebase and leverages a proven, reliable implementation pattern.

Key components to reuse:

- Batch job model and database structure
- Concurrent processing with semaphore control
- Transaction management patterns
- Progress tracking mechanisms

The implementation will follow these architectural principles:

- ORM-Only principle for all database interactions
- UUID standardization for identifiers
- Proper transaction boundaries
- Background processing with own session management

## 5. IMPLEMENTATION DETAILS

### 5.1 New Files to Create

Create a new router file:

```
src/routers/batch_sitemap.py
```

### 5.2 Implementation Steps

#### Step 1: Create Request/Response Models

Create Pydantic models in `batch_sitemap.py`:

```python
from pydantic import BaseModel, Field, HttpUrl
from typing import List, Dict, Any, Optional

class SitemapBatchRequest(BaseModel):
    domains: List[str] = Field(..., min_items=1, max_items=100)
    max_pages: int = Field(1000, ge=1, le=10000)

class SitemapBatchResponse(BaseModel):
    batch_id: str
    status: str
    total_domains: int
    status_url: str
```

#### Step 2: Implement Batch Creation Endpoint

Create an endpoint to initiate batch processing:

```python
@router.post("/api/v3/sitemap/batch/create", response_model=SitemapBatchResponse)
async def create_sitemap_batch(
    request: SitemapBatchRequest,
    current_user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db_session),
    background_tasks: BackgroundTasks = None,
):
    """
    Create a batch of domains for sitemap processing.

    This endpoint accepts multiple domains and creates a batch job to process them.
    Each domain will be processed individually as part of the batch.
    """
    # Generate batch ID
    batch_id = str(uuid.uuid4())

    # Create batch record using existing function
    batch_result = await create_batch(
        session=session,
        batch_id=batch_id,
        domains=request.domains,
        user_id=current_user.user_id,
        options={"max_concurrent": 5, "max_pages": request.max_pages}
    )

    # Add background task to process the batch
    background_tasks.add_task(
        process_sitemap_batch_with_own_session,
        batch_id=batch_id,
        domains=request.domains,
        user_id=current_user.user_id,
        max_pages=request.max_pages
    )

    # Return immediate response with batch details
    return SitemapBatchResponse(
        batch_id=batch_id,
        status="pending",
        total_domains=len(request.domains),
        status_url=f"/api/v3/sitemap/batch/status/{batch_id}"
    )
```

#### Step 3: Implement Batch Status Endpoint

Create an endpoint to check batch status:

```python
@router.get("/api/v3/sitemap/batch/status/{batch_id}", response_model=BatchStatus)
async def get_sitemap_batch_status(
    batch_id: str,
    session: AsyncSession = Depends(get_db_session),
    current_user: User = Depends(get_current_user),
):
    """
    Get the status of a sitemap batch processing job.

    Returns detailed information about the batch, including progress and results.
    """
    return await get_batch_status(
        session=session,
        batch_id=batch_id
    )
```

#### Step 4: Implement Batch Processing Function

Create a function to process the batch:

```python
async def process_sitemap_batch_with_own_session(
    batch_id: str,
    domains: List[str],
    user_id: str,
    max_pages: int = 1000
) -> None:
    """
    Process a batch of domains for sitemap scanning with its own database session.

    This function creates its own session and manages its own transaction boundaries.
    It processes multiple domains concurrently with a maximum concurrency limit.

    Args:
        batch_id: Batch ID to process
        domains: List of domains to process
        user_id: User ID processing the batch
        max_pages: Maximum pages to process per domain
    """
    logger.info(f"Starting sitemap batch processing for {len(domains)} domains")

    # Update batch status to processing
    try:
        async with get_background_session() as session:
            batch = await BatchJob.get_by_batch_id(session, batch_id)
            if batch:
                setattr(batch, "status", BATCH_STATUS_PROCESSING)
                setattr(batch, "start_time", func.now())
                await session.flush()
    except Exception as e:
        logger.error(f"Error updating batch status to processing: {str(e)}")

    # Track domain processing results
    domain_results = {}
    start_time = datetime.utcnow().isoformat()

    # Define domain processor function that uses the existing sitemap processor
    async def process_single_domain(domain: str):
        domain_start_time = datetime.utcnow()
        try:
            # Process domain with its own session
            job_id = str(uuid.uuid4())
            await process_domain_with_own_session(
                job_id=job_id,
                domain=domain,
                user_id=user_id,
                max_urls=max_pages
            )

            # Domain processed successfully
            domain_end_time = datetime.utcnow()
            processing_time = (domain_end_time - domain_start_time).total_seconds()

            result = {
                "status": "completed",
                "job_id": job_id,
                "start_time": domain_start_time.isoformat(),
                "end_time": domain_end_time.isoformat(),
                "processing_time": processing_time,
                "error": None
            }
            logger.info(f"Successfully processed domain {domain} in {processing_time:.2f} seconds")
            return (domain, result, True)

        except Exception as e:
            # Domain processing failed
            domain_end_time = datetime.utcnow()
            processing_time = (domain_end_time - domain_start_time).total_seconds()

            result = {
                "status": "failed",
                "start_time": domain_start_time.isoformat(),
                "end_time": domain_end_time.isoformat(),
                "processing_time": processing_time,
                "error": str(e)
            }
            logger.error(f"Error processing domain {domain}: {str(e)}")
            return (domain, result, False)

    # Process domains concurrently with a limit on concurrency
    max_concurrent = 5
    semaphore = asyncio.Semaphore(max_concurrent)

    async def process_with_semaphore(domain):
        async with semaphore:
            return await process_single_domain(domain)

    # Start concurrent processing
    tasks = [process_with_semaphore(domain) for domain in domains]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Process results and update batch - follows same pattern as batch_functions.py
    completed_count = 0
    failed_count = 0

    for result in results:
        if isinstance(result, Exception):
            failed_count += 1
            continue

        domain, domain_result, success = result
        domain_results[domain] = domain_result

        if success:
            completed_count += 1
        else:
            failed_count += 1

        # Update batch progress periodically
        try:
            async with get_background_session() as session:
                batch = await BatchJob.get_by_batch_id(session, batch_id)
                if batch:
                    batch.update_progress(completed=completed_count, failed=failed_count)

                    # Update metadata with results
                    batch_dict = batch.to_dict()
                    metadata = batch_dict.get("batch_metadata") or {}
                    if not isinstance(metadata, dict):
                        metadata = {}
                    metadata["domain_results"] = domain_results
                    metadata["last_updated"] = datetime.utcnow().isoformat()
                    setattr(batch, "batch_metadata", metadata)

                    await session.flush()
        except Exception as update_error:
            logger.error(f"Error updating batch progress: {str(update_error)}")

    # Update final batch status
    try:
        async with get_background_session() as session:
            batch = await BatchJob.get_by_batch_id(session, batch_id)
            if batch:
                # Determine final status
                if completed_count > 0 and failed_count == 0:
                    final_status = BATCH_STATUS_COMPLETED
                elif completed_count == 0 and failed_count > 0:
                    final_status = BATCH_STATUS_FAILED
                else:
                    final_status = BATCH_STATUS_COMPLETED  # Partial success

                # Update batch
                setattr(batch, "status", final_status)
                setattr(batch, "end_time", func.now())

                # Update metadata with domain results
                batch_dict = batch.to_dict()
                metadata = batch_dict.get("batch_metadata") or {}
                if not isinstance(metadata, dict):
                    metadata = {}
                metadata["domain_results"] = domain_results
                metadata["last_updated"] = datetime.utcnow().isoformat()
                setattr(batch, "batch_metadata", metadata)

                await session.flush()
    except Exception as e:
        logger.error(f"Error updating final batch status: {str(e)}")
```

## 6. VERIFICATION PROCEDURES

### 6.1 Testing Requirements

1. **Unit Tests**:

   - Create unit tests for request validation
   - Ensure proper error handling for invalid inputs

2. **Integration Tests**:

   - Test batch creation with multiple domains
   - Verify batch status tracking
   - Ensure domains are processed correctly

3. **E2E Test Command**:

   ```bash
   curl -X POST http://localhost:8000/api/v3/sitemap/batch/create \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer <YOUR_TOKEN>" \
     -d '{"domains": ["example.com", "example.org", "example.net"], "max_pages": 500}'
   ```

4. **Status Check Command**:
   ```bash
   curl -X GET http://localhost:8000/api/v3/sitemap/batch/status/<BATCH_ID> \
     -H "Authorization: Bearer <YOUR_TOKEN>"
   ```

### 6.2 Verification Checklist

- [ ] Batch endpoint accepts multiple domains
- [ ] Individual job records are created for each domain
- [ ] Batch job record is properly stored and updated
- [ ] Domains are processed concurrently with proper resource management
- [ ] Progress tracking accurately reflects processing status
- [ ] Error handling preserves batch processing when individual domains fail
- [ ] Batch status endpoint provides comprehensive status information
- [ ] Response formats match API standards

## 7. ARCHITECTURAL COMPLIANCE

This implementation must comply with:

1. **ORM-Only Principle**: All database operations must use SQLAlchemy ORM patterns.
2. **UUID Standardization**: All identifiers must use UUID format.
3. **Transaction Management**: Proper transaction boundaries must be maintained.
4. **Session Management**: Background tasks must create their own sessions.
5. **Batch Processing Standards**: Follow existing batch processing patterns.

## 8. DEPENDENCIES

This implementation depends on:

1. The existing `process_domain_with_own_session` function in `src/services/sitemap/processing_service.py`
2. The batch processing infrastructure in `src/services/batch/batch_functions.py`
3. The `BatchJob` model in `src/models/batch_job.py`

## 9. ESTIMATED EFFORT

- Development: 1-2 days
- Testing: 1 day
- Documentation: 0.5 day
- Total: 2.5-3.5 days

## 10. CONCLUSION

This work order outlines the implementation of a sitemap batch endpoint that will enable processing multiple domains in a single request. By leveraging the existing batch processing architecture, we ensure a consistent, reliable implementation that follows established patterns while addressing customer needs for bulk processing.
