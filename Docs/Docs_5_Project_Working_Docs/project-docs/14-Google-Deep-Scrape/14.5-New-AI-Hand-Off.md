# 14.5 Handoff Document: Google Deep Scrape Implementation

**Objective:** Provide a clear and comprehensive status update and action plan for the next AI assistant taking over the implementation of the Google Maps Deep Scrape feature.

**Authoritative Plan:** The sole source of truth for the implementation requirements and workflow is **`project-docs/14-Google-Deep-Scrape/14.1-GOOGLE MAPS DEEP SCRAPE IMPLEMENTATION PLAN.md`**. Specifically, **Section 4.6: Curation-Driven Deep Scan Workflow** outlines the **REVISED APPROACH** which **supersedes** any previous descriptions or implementations of search-driven or job-driven deep scan initiation within that document or the codebase. The goal is to allow users to select places from the `places_staging` table for detailed enrichment, which are then processed asynchronously.

**Progress Log:** Detailed progress tracking is maintained in **`project-docs/14-Google-Deep-Scrape/14.2-DEEP_SCRAPE_IMPLEMENTATION_PROGRESS.md`**.

---

## Current Status & Accomplishments (Phase 1 Complete)

Phase 1 ("Core Deep Scan & Persistence Logic") is functionally complete. The core logic for processing a single place has been implemented and refined:

1.  **`LocalBusiness` Model (`src/models/local_business.py`):** Successfully created and aligned with the target `local_businesses` database table schema.
2.  **Database Schema Alignment:** Manually added the missing `place_id` column and constraints to the `local_businesses` table via SQL script (Task 1.2 in progress log). **Crucially, a formal Alembic migration is still required.**
3.  **Single Place Processing (`src/services/places/places_deep_service.py` -> `process_single_deep_scan`):**
    - Successfully implemented the Google Places Details API call using the `googlemaps` client.
    - Successfully implemented mapping logic (`_map_details_to_model`) to transform API responses into the `LocalBusiness` model structure.
    - Successfully implemented database upsert logic using `insert().on_conflict_do_update()` to save detailed data to the `local_businesses` table, keyed on `place_id`.
    - Resolved issues related to `JobStatus` (confirmed it's a string, not an Enum) and updated the service accordingly.
    - Addressed linter issues related to `place_id` type hinting (using `str()` cast) and `processed` flag assignment (using `# type: ignore`).

**In essence: The core capability to take a `place_id`, fetch detailed data from Google, and save/update it correctly in the `local_businesses` table is functional.**

---

## Outstanding Issues & Required Cleanup

1.  **Alembic Migration NEEDED:** The changes made to `src/models/place.py` (adding new `PlaceStatusEnum` values and the `deep_scan_error` column, adding an index) **HAVE NOT** been applied to the database. An Alembic migration MUST be generated and applied:

    - Generate: `alembic revision --autogenerate -m "Add deep scan status and error field to Place model"`
    - Apply: `alembic upgrade head`
    - **This is the most critical immediate technical debt.**

2.  **Obsolete Logic Called by Scheduler:** The method `PlacesDeepService.process_places_deep_scan_job` (in `src/services/places/places_deep_service.py`) is obsolete and not aligned with the Curation-Driven workflow. The existing scheduler (`src/services/sitemap_scheduler.py`) **incorrectly** calls this method for jobs of type `google_maps_deep_scan`.

    - **ACTION REQUIRED:** The `sitemap_scheduler.py` needs modification (as detailed in Task 3.1 below) to **remove the call to `process_places_deep_scan_job`** and instead implement the Curation-Driven logic (query `places_staging`, call `process_single_deep_scan`). The `process_places_deep_scan_job` method itself within `PlacesDeepService` can then be safely removed.

3.  **Manual `main.py` Edit Required:** The new router `src/routers/places_staging.py` (implemented in Phase 2) needs to be manually imported and included in `src/main.py`. Automated attempts failed. **(DONE)**

4.  **Router (`src/routers/places_staging.py`) Implementation Issues:**

    - **Dependencies:** There were persistent issues automatically resolving the correct import paths/names for `get_current_user` (from `src.auth.dependencies`) and `get_session_dependency` (from `src.session.async_session`). The code currently contains the correct names, but the imports might still be pointing to incorrect locations due to failed automated edits. **Manual verification of these imports is required.**
    - **Linter Errors:** Persistent (likely false positive) linter errors related to assigning the `PlaceStatusEnum` value required a `# type: ignore` workaround.

5.  **Linter Workarounds:** Review the `# type: ignore` comments added in `src/services/places/deep_service.py` (for `processed` assignment) and `src/routers/places_staging.py` (for `status` assignment) and the `str()` cast for `place_id` in `places_deep_service.py`. While likely necessary due to linter limitations, they should be noted.

---

## Remaining Tasks (Following Curation-Driven Workflow)

Based on the authoritative plan (`14.1-...PLAN.md`, Section 4.6 & Roadmap Section 6):

1.  **Complete Phase 2: Queueing Mechanism:**

    - **Task 2.1:** Modify `Place` model & Enum (`src/models/place.py`) - **DONE** (Alembic Migration `cf984a13c57e` Applied).
    - **Task 2.2:** Implement `PUT /api/v3/places-staging/queue-deep-scan` endpoint (`src/routers/places_staging.py`) - **DONE** (Imports verified, Included in `main.py`).
    - **Task 2.3:** Unit/Integration Tests for Queueing Endpoint - **TODO**.

2.  **Implement Phase 3: Background Automation & Final Integration:**
    - **Task 3.1: Modify Existing Sitemap Scheduler Service (`src/services/sitemap_scheduler.py`)** to integrate Curation-Driven Deep Scan processing. The `process_pending_sitemaps` job (or a newly added dedicated job within this file) needs modification to:
      - Query the `places_staging` table for records with `status == PlaceStatusEnum.QueuedForDeepScan`.
      - For each found record, update its status to `ProcessingDeepScan`.
      - Instantiate `PlacesDeepService` and call `process_single_deep_scan(place_id=record.place_id, tenant_id=record.tenant_id)`.
      - Handle the outcome: Update the `places_staging` record's status to `DeepScanComplete` on success, or `DeepScanFailed` (storing the error in `deep_scan_error`) on failure.
      - **Remove the existing logic** within `process_pending_sitemaps` that incorrectly queries the `jobs` table for `job_type == 'google_maps_deep_scan'` and calls the obsolete `PlacesDeepService.process_places_deep_scan_job`.
    - **Task 3.2:** Unit/Integration Tests for Background Service & Full Workflow - **TODO**.

---

## Key Document References

- **Authoritative Implementation Plan:** `project-docs/14-Google-Deep-Scrape/14.1-GOOGLE MAPS DEEP SCRAPE IMPLEMENTATION PLAN.md` (Especially Section 4.6 and Section 6)
- **Progress Log:** `project-docs/14-Google-Deep-Scrape/14.2-DEEP_SCRAPE_IMPLEMENTATION_PROGRESS.md`
- **Background Task Pattern:** `Docs/Docs_1_AI_GUIDES/21-SCHEDULED_TASKS_APSCHEDULER_PATTERN.md`
- **Core Architectural Principles:** `Docs/Docs_1_AI_GUIDES/17-CORE_ARCHITECTURAL_PRINCIPLES.md`
- **Transaction/Session Management:** `Docs/Docs_1_AI_GUIDES/13-TRANSACTION_MANAGEMENT_GUIDE.md`, `07-DATABASE_CONNECTION_STANDARDS.md`
