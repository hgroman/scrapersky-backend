# 14.1 GOOGLE MAPS DEEP SCRAPE IMPLEMENTATION PLAN

**Document ID:** 14.1-GOOGLE-MAPS-DEEP-SCRAPE
**Status:** Implementation Planning
**Created:** April 2023
**Author:** Engineering Team

## 1. EXECUTIVE SUMMARY

This document outlines the implementation plan for the Google Maps Deep Scrape feature. This feature will extend the existing Google Maps API functionality to retrieve detailed place information instead of just the basic information provided by the current implementation.

This implementation will strictly follow the architectural patterns established in:

- `project-docs/14-Google-Deep-Scrape/14.0-GOOGLE MAPS API DEPENDENCY MAP & ARCHITECTURAL PATTERN GUIDE.md`
- `Docs/Docs_1_AI_GUIDES/14-GOOGLE_MAPS_API_EXEMPLAR.md`

This implementation will strictly follow the architectural patterns established in the existing `src/routers/google_maps_api.py` codebase and documented in authoritative guides such as `Docs/Docs_2_Feature-Alignment-Testing-Plan/500-GOOGLE-MAPS-API-ARCHITECTURAL-PATTERNS.md` and relevant files within `Docs/Docs_1_AI_GUIDES/`.

## 2. FEATURE COMPARISON

| Feature                | Basic Google Maps API               | Deep Scrape API                |
| ---------------------- | ----------------------------------- | ------------------------------ |
| Information Detail     | Basic place data                    | Comprehensive place details    |
| Cost                   | Lower API usage                     | Higher API usage               |
| Endpoint Structure     | `/api/v3/localminer-discoveryscan/` | `/api/v3/localminer-deepscan/` |
| Implementation Pattern | Exemplar                            | Clone of Exemplar              |
| Background Processing  | Yes                                 | Yes                            |
| Job Tracking           | Yes                                 | Yes                            |

## 3. IMPLEMENTATION APPROACH

The implementation will clone the existing Google Maps API router and supporting services, modifying only the specific parts needed for detailed data retrieval. This approach ensures architectural consistency while adding new functionality.

### 3.1 Data Flow Overview

```
┌────────────────────────────────────────────────────────────────────────────────┐
│                                                                                │
│                       GOOGLE MAPS DEEP SCRAPE DATA FLOW                        │
│                                                                                │
│  ┌───────────┐       ┌───────────┐       ┌───────────────┐                    │
│  │ Client    │──────▶│ FastAPI   │──────▶│ Router        │                    │
│  │ Request   │       │ Framework │       │ (Deep API)    │                    │
│  └───────────┘       └───────────┘       └─────┬─────────┘                    │
│                                                 │                              │
│                                                 │  Transaction Boundary        │
│                                                 │  (session.begin)             │
│                                                 ▼                              │
│     ┌─────────────────────────────────────────────────────────────────┐       │
│     │                                                                 │       │
│     │  ┌───────────┐                                                  │       │
│     │  │ Create    │                                                  │       │
│     │  │ Job Record│                                                  │       │
│     │  └───────────┘                                                  │       │
│     │                                                                 │       │
│     └─────────────────────────────┬───────────────────────────────────┘       │
│                                   │                                           │
│                                   │  Transaction Commits                      │
│                                   │  Job ID returned to client                │
│                                   ▼                                           │
│                   ┌───────────────────────────────────┐                       │
│                   │       Background Task             │                       │
│                   │                                   │                       │
│                   │  ┌───────────────────────────┐   │                       │
│                   │  │  Basic Place Search       │   │                       │
│                   │  │  (Google Maps API)        │   │                       │
│                   │  └───────────┬───────────────┘   │                       │
│                   │              │                   │                       │
│                   │              ▼                   │                       │
│                   │  ┌───────────────────────────┐   │                       │
│                   │  │  For Each Place:          │   │                       │
│                   │  │  Get Detailed Info        │   │                       │
│                   │  │  (Details API Call)       │   │                       │
│                   │  └───────────┬───────────────┘   │                       │
│                   │              │                   │                       │
│                   │              ▼                   │                       │
│                   │  ┌───────────────────────────┐   │                       │
│                   │  │  Store Combined Data      │   │                       │
│                   │  │  (Database Transaction)   │   │                       │
│                   │  └───────────┬───────────────┘   │                       │
│                   │              │                   │                       │
│                   │              ▼                   │                       │
│                   │  ┌───────────────────────────┐   │                       │
│                   │  │  Update Job Status        │   │                       │
│                   │  │  (Processing → Complete)  │   │                       │
│                   │  └───────────────────────────┘   │                       │
│                   └───────────────────────────────────┘                       │
│                                                                                │
└────────────────────────────────────────────────────────────────────────────────┘
```

### 3.2 New Files to Create

```
/src/routers/google_maps_deep_api.py         # Clone of google_maps_api.py
/src/services/places/places_deep_service.py   # Extended service for detailed data
/src/services/places/places_deep_storage_service.py  # Storage for detailed data
/src/models/place_detail.py                  # Model for detailed place data
```

### 3.3 Existing Files to Reference

```
/src/routers/google_maps_api.py              # Base implementation to clone
/src/services/places/places_service.py       # Base service to extend
/src/services/places/places_storage_service.py # Base storage service
/src/models/place.py                         # Base place model
/src/models/place_search.py                  # Search job model (can be reused)
```

## 4. DETAILED IMPLEMENTATION STEPS

\*\*

---

** !!! CRITICAL WARNING !!! **

---

** DO NOT USE ALEMBIC FOR SCHEMA CHANGES RELATED TO THIS IMPLEMENTATION. **
** ALEMBIC IS CURRENTLY BROKEN/UNRELIABLE IN THIS ENVIRONMENT. **
** (See project-docs/15-Alembic-Fix/ for details on planned future fixes). **

---

** ANY REQUIRED DATABASE SCHEMA CHANGES (e.g., adding columns, creating **
** enums) MUST BE PROVIDED AS RAW SQL COMMANDS FOR MANUAL EXECUTION. **
** DO NOT ATTEMPT TO GENERATE OR APPLY ALEMBIC MIGRATIONS FOR THIS WORK. **

---

---

\*\*

### 4.1 Transaction and Session Management Flow

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│                DEEP SCRAPE API TRANSACTION MANAGEMENT                       │
│                                                                             │
│                                                                             │
│  [CLIENT] ─────► [ROUTER ENDPOINT] ─────┐                                  │
│                                          │                                  │
│                                          │                                  │
│                                          ▼                                  │
│                            ┌───────────────────────────┐                    │
│                            │ Router Transaction        │                    │
│                            │ async with session.begin()│                    │
│                            └─────────────┬─────────────┘                    │
│                                          │                                  │
│                                          │                                  │
│                                          ▼                                  │
│                            ┌───────────────────────────┐                    │
│                            │ Background Task Started   │                    │
│                            │ (Job ID Returned)         │                    │
│                            └─────────────┬─────────────┘                    │
│                                          │                                  │
│                                          │                                  │
│                                          ▼                                  │
│                            ┌───────────────────────────┐                    │
│                            │ Background Task Execution │                    │
│                            │ (Separate Thread)         │                    │
│                            └─────────────┬─────────────┘                    │
│                                          │                                  │
│                                          │                                  │
│                                          ▼                                  │
│                            ┌───────────────────────────┐                    │
│                            │ Background Session Create │                    │
│                            │ async with get_session()  │                    │
│                            └─────────────┬─────────────┘                    │
│                                          │                                  │
│                                          │                                  │
│                                          ▼                                  │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  ┌─────────────────┐      ┌─────────────────┐     ┌──────────────┐ │   │
│  │  │ Status Update   │      │ API Call &      │     │ Final Status │ │   │
│  │  │ Transaction     │─────►│ Store Results   │────►│ Transaction  │ │   │
│  │  │                 │      │ Transaction     │     │              │ │   │
│  │  └─────────────────┘      └─────────────────┘     └──────────────┘ │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 4.2 Create Router (`google_maps_deep_api.py`)

Clone the existing router with the following modifications:

````python
"""
Google Maps Deep API Router

This module provides API endpoints for retrieving detailed place information
from Google Maps API. It follows the same patterns as the standard Google Maps API.
"""
import uuid
import logging
import os  # Added os import based on original file
from typing import Dict, List, Any, Optional, Union
from datetime import datetime

from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends, Query, Body, Request # Added Request import
from pydantic import BaseModel
from sqlalchemy.ext.asyncio import AsyncSession

from ..session.async_session import get_session_dependency, get_session
from ..models import PlaceDetail, PlaceSearch # Assuming PlaceDetail will be created
from ..auth.jwt_auth import get_current_user, DEFAULT_TENANT_ID
from ..services.places.places_deep_service import PlacesDeepService
from ..services.places.places_deep_storage_service import PlacesDeepStorageService
# Removed unused job_service import for clarity in this snippet
# from ..services.job_service import job_service
from ..config.settings import settings

# Configure logger
logger = logging.getLogger(__name__)

# Create API models
class PlacesDeepSearchRequest(BaseModel):
    business_type: str
    location: str
    radius_km: int = 10
    max_details: int = 20  # New parameter for limiting detailed place requests
    tenant_id: Optional[str] = None

# Router with different prefix
router = APIRouter(
    prefix="/api/v3/localminer-deepscan",
    tags=["google-maps-deep-api"]
)

# Initialize services
places_deep_service = PlacesDeepService()
places_deep_storage_service = PlacesDeepStorageService()

# Get API key from environment (mirroring original file)
GOOGLE_MAPS_API_KEY = os.getenv("GOOGLE_MAPS_API_KEY", "")

@router.post("/search/places", response_model=Dict)
async def search_places_detailed(
    request: PlacesDeepSearchRequest,
    session: AsyncSession = Depends(get_session_dependency),
    current_user: Dict = Depends(get_current_user),
    # Removed unused BackgroundTasks parameter to match original pattern
) -> Dict[str, Any]:
    """
    Search for places with detailed information using Google Maps API.

    This endpoint extends the basic search functionality to retrieve comprehensive
    place details including reviews, opening hours, and other metadata.
    Follows the pattern of the discovery scan endpoint.
    """
    # Extract user information (same as original)
    user_info = current_user
    logger.info(f"🔍 Deep scan user details: user_id={user_info.get('user_id')}, tenant_id={request.tenant_id}")

    # Generate job ID (same as original)
    job_id = str(uuid.uuid4())

    try:
        # Router owns the transaction boundary (same as original)
        async with session.begin():
            # Create search record (same structure as original, add new params)
            search_record = PlaceSearch(
                id=job_id,
                tenant_id=request.tenant_id, # Will fallback in service if needed
                business_type=request.business_type,
                location=request.location,
                params={
                    "radius_km": request.radius_km,
                    "max_details": request.max_details,  # Store new parameter
                    "is_deep_scan": True  # Mark as deep scan
                },
                status="pending",
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow(),
                user_id=user_info.get("user_id", "unknown")
            )

            # Store search record
            session.add(search_record)

            # Set up background task arguments (same pattern)
            task_args = {
                "job_id": job_id,
                "business_type": request.business_type,
                "location": request.location,
                "radius_km": request.radius_km,
                "max_details": request.max_details,
                "user_info": user_info,
                "tenant_id": request.tenant_id,
                "api_key": GOOGLE_MAPS_API_KEY or None # Pass API key
            }

        # Define and run background processing task (MIRRORING ACTUAL CODE PATTERN)
        async def process_places_deep_search_background(args: Dict[str, Any]):
            """Background task to process places deep search."""
            try:
                # Create a new session for background task (CORRECT PATTERN)
                async with get_session() as bg_session:
                    # Call the deep search service method within its own transaction scope
                    # The service method itself will handle begin/commit/rollback for its steps
                    await places_deep_service.process_places_deep_search(
                        session=bg_session, # Pass the session
                        job_id=args["job_id"],
                        business_type=args["business_type"],
                        location=args["location"],
                        radius_km=args["radius_km"],
                        max_details=args["max_details"],
                        user_info=args["user_info"],
                        tenant_id=args["tenant_id"],
                        api_key=args["api_key"]
                    )
            except Exception as e:
                logger.error(f"Error in background deep search task for job {args.get('job_id')}: {str(e)}", exc_info=True)
                # Attempt to update job status to failed in a separate session/transaction
                try:
                    async with get_session() as error_session:
                        async with error_session.begin():
                            # Ideally call job_service.update_status here, but mimicking
                            # the original file's direct update for now if job_service isn't ready
                            from sqlalchemy import update
                            from ..models.place_search import PlaceSearch # Requires PlaceSearch model

                            stmt = update(PlaceSearch).where(
                                PlaceSearch.id == uuid.UUID(args["job_id"])
                            ).values(
                                status="failed",
                                error_message=str(e)[:1024], # Truncate error message if needed
                                updated_at=datetime.utcnow()
                            )
                            await error_session.execute(stmt)
                except Exception as db_error:
                    logger.error(f"Failed to update error status in database for job {args.get('job_id')}: {str(db_error)}")

        # Run background task concurrently (MIRRORING ACTUAL CODE PATTERN)
        await process_places_deep_search_background(task_args)

        # Return job ID and status URL (same format as original)
        return {
            "job_id": job_id,
            "status_url": f"/api/v3/localminer-deepscan/search/status/{job_id}", # Adjusted prefix
            "status": "processing" # Initial status returned to client
        }
    except Exception as e:
        logger.error(f"Error initiating deep places search: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error initiating deep search: {str(e)}")

# Clone remaining endpoints following the same pattern...
# Example: /search/status/{job_id}
@router.get("/search/status/{job_id}", response_model=Dict) # Using Dict for now, refine later
async def get_deep_search_status(
    job_id: str,
    request: Request, # Added Request import
    session: AsyncSession = Depends(get_session_dependency),
    current_user: Dict = Depends(get_current_user)
) -> Dict[str, Any]:
    """Get the status of a deep places search job."""
    # (Implementation would be very similar to the original get_search_status,
    # potentially calling job_service.get_by_id within a session.begin() block)
    # ... implementation needed ...
    # Example call to job_service (assuming it exists and works like in 500-...)
    try:
        async with session.begin():
            # Need job_service.get_by_id or similar function here
            # job = await job_service.get_by_id(session, job_id)
            # For now, query PlaceSearch directly as a placeholder
            from ..models.place_search import PlaceSearch # Requires PlaceSearch model
            from sqlalchemy import select
            query = select(PlaceSearch).where(PlaceSearch.id == uuid.UUID(job_id))
            result = await session.execute(query)
            job = result.scalar_one_or_none()

            if not job:
                raise HTTPException(status_code=404, detail="Job not found")

            # Basic response structure
            response = {
                "job_id": str(job.id),
                "status": job.status,
                "progress": job.progress or 0.0, # Add progress if PlaceSearch has it
                "created_at": job.created_at,
                "updated_at": job.updated_at,
                "error": job.error_message or None, # Add error if PlaceSearch has it
                "params": job.params # Include parameters used for the search
            }
            # Add results URL if completed? (Needs definition)
            # if job.status == "completed":
            #    response["results_url"] = f"/api/v3/localminer-deepscan/search/results/{job_id}"

        return response
    except ValueError: # Handle invalid UUID format
        raise HTTPException(status_code=400, detail="Invalid Job ID format")
    except HTTPException as http_exc: # Re-raise specific HTTP exceptions
        raise http_exc
    except Exception as e:
        logger.error(f"Error retrieving status for job {job_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving job status")

### 4.3 Create Deep Service (`places_deep_service.py`)

Extend the original service with additional functionality:

```python
"""
Places Deep Service

This service handles detailed Google Maps place data retrieval.
It follows the same architectural patterns as the original Places Service.
"""
import logging
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional

from sqlalchemy.ext.asyncio import AsyncSession

from ..places.places_service import PlacesService  # Extend the original service
from ..places.places_deep_storage_service import PlacesDeepStorageService
from ...models.place_detail import PlaceDetail
from ...models.place_search import PlaceSearch
from ...session.async_session import get_session
from ...config.settings import settings

logger = logging.getLogger(__name__)

class PlacesDeepService(PlacesService):
    """Service for retrieving and storing detailed place information."""

    def __init__(self):
        super().__init__()  # Initialize base service
        self.storage_service = PlacesDeepStorageService()  # Use deep storage service

    async def get_place_details(self, place_id: str, api_key: str = None) -> Dict:
        """
        Retrieve detailed information for a specific place.

        This makes an additional API call to get comprehensive details.
        """
        # Implementation details for calling the Google Maps Details API
        # ...

    async def process_places_deep_search(
        self,
        job_id: str,
        business_type: str,
        location: str,
        radius_km: int = 10,
        max_details: int = 20,
        user_info: Dict = None,
        tenant_id: str = None
    ):
        """Background task to process places deep search with proper session handling."""
        # Create a new session for background task
        async with get_session() as bg_session:
            try:
                # Update job status to processing (similar to original)
                async with bg_session.begin():
                    # Get search record and update status
                    # ...

                # First, retrieve basic place data (reuse existing functionality)
                basic_places = await self.search_places(
                    business_type=business_type,
                    location=location,
                    radius_km=radius_km
                )

                # Limit the number of detailed requests to control API costs
                places_for_details = basic_places[:max_details]
                total_places = len(basic_places)

                # Process each place for detailed information
                detailed_places = []
                for i, place in enumerate(places_for_details):
                    try:
                        # Get detailed information
                        details = await self.get_place_details(place["place_id"])

                        # Combine basic and detailed information
                        combined_data = {**place, **details}
                        detailed_places.append(combined_data)

                        # Update progress regularly
                        if i % 5 == 0 or i == len(places_for_details) - 1:
                            progress = (i + 1) / len(places_for_details)
                            async with bg_session.begin():
                                # Update progress
                                # ...
                    except Exception as place_error:
                        logger.error(f"Error getting details for place {place.get('name')}: {str(place_error)}")
                        # Continue with other places

                # Store detailed places in database
                async with bg_session.begin():
                    await self.storage_service.store_place_details(
                        session=bg_session,
                        places=detailed_places,
                        job_id=job_id,
                        tenant_id=tenant_id
                    )

                # Update job status to completed
                async with bg_session.begin():
                    # Update job status
                    # ...

            except Exception as e:
                logger.error(f"Error in deep search background task: {str(e)}")
                # Update job status to failed
                try:
                    async with bg_session.begin():
                        # Update status to failed
                        # ...
                except Exception as update_error:
                    logger.error(f"Failed to update error status in database: {str(update_error)}")
```

### 4.4 Service Method Call Sequence

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│                  DEEP SCAN SERVICE METHOD CALL SEQUENCE                     │
│                                                                             │
│                                                                             │
│                        process_places_deep_search()                         │
│                                   │                                         │
│                                   │                                         │
│                                   ▼                                         │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │ 1. Update Job Status to "processing"                               │    │
│  └────────────────────────────────┬───────────────────────────────────┘    │
│                                   │                                         │
│                                   ▼                                         │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │ 2. Call search_places() from base PlacesService                    │    │
│  │    (returns basic place data)                                      │    │
│  │                                                                    │    │
│  └────────────────────────────────┬───────────────────────────────────┘    │
│                                   │                                         │
│                                   ▼                                         │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │ 3. For each place:                                                 │    │
│  │    │                                                               │    │
│  │    ├─► 3.1 Call get_place_details(place_id)                       │    │
│  │    │    (retrieves detailed place information)                     │    │
│  │    │                                                               │    │
│  │    ├─► 3.2 Merge basic data with detailed information             │    │
│  │    │                                                               │    │
│  │    └─► 3.3 Update progress regularly (every 5 places)             │    │
│  │                                                                    │    │
│  └────────────────────────────────┬───────────────────────────────────┘    │
│                                   │                                         │
│                                   ▼                                         │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │ 4. Call storage_service.store_place_details()                      │    │
│  │    (stores all place details in a single transaction)              │    │
│  └────────────────────────────────┬───────────────────────────────────┘    │
│                                   │                                         │
│                                   ▼                                         │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │ 5. Update Job Status to "completed"                                │    │
│  └────────────────────────────────────────────────────────────────────┘    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 4.5 Create Place Detail Model (`place_detail.py`)

### 4.5 Define Local Business Model (`local_business.py`)

Create a model for storing detailed place information:
The implementation will use the existing `local_businesses` table, which is designed to store comprehensive data retrieved during the deep scan. A corresponding SQLAlchemy model needs to be created.

**Target Table:** `local_businesses`

**New Model File:** `src/models/local_business.py`

**Model Definition:**

```python
"""
Place Detail Model

This model extends the base Place model with detailed information.
SQLAlchemy model for the local_businesses table.

This table stores detailed business information, often populated from
deep scans of sources like Google Maps Places API Details.
"""
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any

from sqlalchemy import Column, String, Float, Integer, DateTime, Text, ForeignKey, Enum, Boolean, Numeric # Added Numeric
from sqlalchemy.dialects.postgresql import UUID, JSONB, ARRAY # Added JSONB, ARRAY
from sqlalchemy.sql import func # For server defaults like now()

from ..database.base import Base
# Removed unused Place import
# from ..models.place import Place

class LocalBusiness(Base):
    """Model for detailed place information from Google Maps API."
    """Model for the local_businesses table."""

    __tablename__ = "place_details"
    __tablename__ = "local_businesses"

    # --- Columns based on inspect_table output ---

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4) # Changed default based on inspect
    # id = Column(UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid()) # Using server default from inspect
    tenant_id = Column(UUID(as_uuid=True), nullable=False, index=True) # Added index=True based on potential common patterns

    place_id = Column(String, unique=True, nullable=False, index=True)
    job_id = Column(UUID(as_uuid=True), nullable=True)
    tenant_id = Column(UUID(as_uuid=True), nullable=True)
    lead_source = Column(Text, nullable=True)
    business_name = Column(Text, nullable=True, index=True) # Added index=True based on potential common patterns
    full_address = Column(Text, nullable=True)
    street_address = Column(Text, nullable=True)
    city = Column(Text, nullable=True, index=True) # Added index=True based on potential common patterns
    state = Column(Text, nullable=True, index=True) # Added index=True based on potential common patterns
    zip = Column(Text, nullable=True, index=True) # Added index=True based on potential common patterns
    country = Column(Text, nullable=True)
    phone = Column(Text, nullable=True)
    main_category = Column(Text, nullable=True)
    extra_categories = Column(ARRAY(Text), nullable=True)
    rating = Column(Numeric, nullable=True) # Changed from Float to Numeric
    reviews_count = Column(Integer, nullable=True)
    price_text = Column(Text, nullable=True)
    website_url = Column(Text, nullable=True)
    business_verified = Column(Boolean, nullable=True)

    # Basic information (duplicated from Place for query performance)
    name = Column(String, nullable=True)
    business_type = Column(String, nullable=True)
    # Operating hours
    monday_hours = Column(Text, nullable=True)
    tuesday_hours = Column(Text, nullable=True)
    wednesday_hours = Column(Text, nullable=True)
    thursday_hours = Column(Text, nullable=True)
    friday_hours = Column(Text, nullable=True)
    saturday_hours = Column(Text, nullable=True)
    sunday_hours = Column(Text, nullable=True)
    timezone = Column(Text, nullable=True)

    # Detailed place information
    formatted_address = Column(String, nullable=True)
    international_phone_number = Column(String, nullable=True)
    website = Column(String, nullable=True)
    rating = Column(Float, nullable=True)
    user_ratings_total = Column(Float, nullable=True)
    price_level = Column(Float, nullable=True)
    # Additional details
    image_url = Column(Text, nullable=True)
    latitude = Column(Numeric, nullable=True) # Changed from Float to Numeric
    longitude = Column(Numeric, nullable=True) # Changed from Float to Numeric
    food_featured = Column(Boolean, nullable=True)
    hotel_featured = Column(Boolean, nullable=True)

    # Complex data stored as JSON
    hours = Column(JSON, nullable=True)
    reviews = Column(JSON, nullable=True)
    photos = Column(JSON, nullable=True)
    address_components = Column(JSON, nullable=True)
    # Structured Details (Arrays)
    service_options = Column(ARRAY(Text), nullable=True)
    highlights = Column(ARRAY(Text), nullable=True)
    popular_for = Column(ARRAY(Text), nullable=True)
    accessibility = Column(ARRAY(Text), nullable=True)
    offerings = Column(ARRAY(Text), nullable=True)
    dining_options = Column(ARRAY(Text), nullable=True)
    amenities = Column(ARRAY(Text), nullable=True)
    atmosphere = Column(ARRAY(Text), nullable=True)
    crowd = Column(ARRAY(Text), nullable=True)
    planning = Column(ARRAY(Text), nullable=True)
    payments = Column(ARRAY(Text), nullable=True)
    children = Column(ARRAY(Text), nullable=True)
    parking = Column(ARRAY(Text), nullable=True)
    pets = Column(ARRAY(Text), nullable=True)

    # Raw/Additional Data
    additional_json = Column(JSONB, nullable=True, server_default='{}') # Set server_default based on inspect

    # Timestamps
    created_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    updated_at = Column(DateTime, nullable=False, default=datetime.utcnow, onupdate=datetime.utcnow)
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False) # Use timezone=True and server_default
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False) # Use timezone=True and server_default

    # Create indexes
    __table_args__ = (
        Index("idx_place_details_tenant_job", tenant_id, job_id),
        # Example Indexes (adjust as needed based on query patterns)
        Index("idx_local_businesses_tenant_city", tenant_id, city),
        Index("idx_local_businesses_tenant_zip", tenant_id, zip),
        Index("idx_local_businesses_name_tsvector", func.to_tsvector('english', business_name)), # Example full-text search index
    )

    def to_dict(self) -> Dict[str, Any]:
        """Convert the model to a dictionary for API responses."""
        return {
            "id": str(self.id) if self.id else None,
            "place_id": self.place_id,
            "name": self.name,
            "business_type": self.business_type,
            "formatted_address": self.formatted_address,
            "international_phone_number": self.international_phone_number,
            "website": self.website,
            "rating": self.rating,
            "user_ratings_total": self.user_ratings_total,
            "price_level": self.price_level,
            "hours": self.hours,
            "reviews": self.reviews,
            "photos": self.photos,
            "address_components": self.address_components,
            # Add all other relevant fields from the local_businesses schema here...
            "tenant_id": str(self.tenant_id) if self.tenant_id else None,
            "lead_source": self.lead_source,
            "business_name": self.business_name,
            "full_address": self.full_address,
            "street_address": self.street_address,
            "city": self.city,
            "state": self.state,
            "zip": self.zip,
            "country": self.country,
            "phone": self.phone,
            "main_category": self.main_category,
            "extra_categories": self.extra_categories,
            # rating already included
            "reviews_count": self.reviews_count,
            "price_text": self.price_text,
            "website_url": self.website_url,
            "business_verified": self.business_verified,
            "monday_hours": self.monday_hours,
            "tuesday_hours": self.tuesday_hours,
            "wednesday_hours": self.wednesday_hours,
            "thursday_hours": self.thursday_hours,
            "friday_hours": self.friday_hours,
            "saturday_hours": self.saturday_hours,
            "sunday_hours": self.sunday_hours,
            "timezone": self.timezone,
            "image_url": self.image_url,
            "latitude": float(self.latitude) if self.latitude is not None else None, # Convert Numeric to float
            "longitude": float(self.longitude) if self.longitude is not None else None, # Convert Numeric to float
            "food_featured": self.food_featured,
            "hotel_featured": self.hotel_featured,
            "service_options": self.service_options,
            "highlights": self.highlights,
            "popular_for": self.popular_for,
            "accessibility": self.accessibility,
            "offerings": self.offerings,
            "dining_options": self.dining_options,
            "amenities": self.amenities,
            "atmosphere": self.atmosphere,
            "crowd": self.crowd,
            "planning": self.planning,
            "payments": self.payments,
            "children": self.children,
            "parking": self.parking,
            "pets": self.pets,
            "additional_json": self.additional_json,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None,
        }
```

### 4.6 Curation-Driven Deep Scan Workflow [REVISED APPROACH]

**Note:** This section describes the revised workflow for initiating deep scans based on user selection from existing discovery scan results (`places_staging` table). It supersedes the search-driven approach previously described for the `/api/v3/localminer-deepscan/search/places` endpoint and its associated background task structure (Sections 4.2, 4.3). The original search-based endpoint should be considered deprecated for initiating deep scans, although the _processing logic_ within `PlacesDeepService` might be partially reused for single-place lookups.

This revised workflow allows users to curate leads from the basic discovery scan results and trigger detailed data retrieval (deep scan) for selected entries, populating the `local_businesses` table.

#### 4.6.1 Workflow Overview

1.  **User Selection (UI):** Users select one or more businesses from the results displayed from the `places_staging` table via a user interface, typically by changing their status to `Selected`.
2.  **Automatic Queueing (API):** When a place's main `status` is updated to `Selected` via the standard status update API (e.g., `PUT /api/v3/places-staging/{place_id}/status`), the system **automatically** sets the corresponding `deep_scan_status` field to `queued`. This is the primary trigger for initiating a deep scan based on user curation.
3.  **Background Monitoring (Scheduler):** A scheduled background service (e.g., using APScheduler, referencing `21-SCHEDULED_TASKS_APSCHEDULER_PATTERN.md`) periodically queries the `places_staging` table for records where `deep_scan_status == 'queued'`.
4.  **Deep Scan Processing (Background):** For each queued record found by the scheduler:
    - The background service updates the `places_staging.deep_scan_status` to `processing`.
    - It retrieves the unique Google `place_id` for the record.
    - It calls the Google Maps Place Details API using the `place_id`.
    - It maps the detailed results to the `LocalBusiness` model format.
    - It saves or updates the corresponding record in the `local_businesses` table (using the `place_id` as a likely key).
    - It updates the `places_staging.deep_scan_status` to `completed` or `failed` (and potentially stores error details in `deep_scan_error`).
    - **Crucially, the main `status` field remains unchanged by the deep scan process itself.**

#### 4.6.2 Required `Place` Model (`places_staging`) Changes

The existing `Place` model (representing the `places_staging` table) and its associated enums in `src/models/place.py` require modification:

- Add the `deep_scan_status` column (e.g., using `DeepScanStatusEnum` with values like `queued`, `processing`, `completed`, `failed`).
- Add the `deep_scan_error` column (nullable `Text`) to store error messages.
- The `PlaceStatusEnum` for the main `status` field remains for user-facing statuses (e.g., `New`, `Selected`, `Maybe`).
- Consider adding an index to the `deep_scan_status` column for efficient querying by the background service.

#### 4.6.3 Primary API Endpoint for Status Updates (Batch/Single)

**Requirement:** A single, unified API endpoint is needed to handle updates to the main `status` field for one or more `places_staging` records, triggered by user actions in the UI. This endpoint must incorporate the automatic deep scan queueing logic.

- **Purpose:** To efficiently update the main status for single or multiple selected records and automatically trigger the deep scan queueing when the status is set to `SELECTED_FOR_DEEP_SCAN`.
- **Method:** `PUT`
- **Suggested Path:** `/api/v3/places-staging/status` (Handles batch implicitly via request body)
- **Request Body:** Should accept a structure containing:
  - A list of identifiers for the records to update (e.g., `place_ids`: [`id1`, `id2`, ...], must contain at least one ID).
  - The target main `status` value (e.g., `status`: `SELECTED_FOR_DEEP_SCAN`).
- **Implementation:**
  1. Perform an `UPDATE` operation on the `places_staging` table for the provided `place_ids`.
  2. Set the main `status` field to the value provided in the request body.
  3. **Conditionally**, if the provided target `status` is `SELECTED_FOR_DEEP_SCAN`, also set the `deep_scan_status` field to `queued` and clear the `deep_scan_error` field for the same records.
  4. This entire operation (updating main status and potentially deep scan status) must occur within a single database transaction.
  5. Handle authentication and authorization appropriately.
  6. Return a response indicating success and the number of records updated (e.g., `{"message": "Status update successful for N places", "updated_count": N}`).

**Note on Deprecation:** This single batch endpoint replaces the need for a separate `PUT /{place_id}/status` endpoint. The single-item endpoint should be considered deprecated and potentially removed to simplify the API surface.

#### 4.6.4 Secondary API Endpoint for Direct Queueing

**Note:** While the primary trigger is the main status update (handled by 4.6.3), a dedicated endpoint for _only_ queueing items for deep scan (without changing their main status) might still be useful for programmatic or administrative purposes.

- **Purpose:** To allow setting `deep_scan_status` to `queued` directly for multiple places, bypassing the main `status` update trigger.
- **Method:** `PUT`
- **Suggested Path:** `/api/v3/places-staging/queue-deep-scan`
- **Request Body:** Should accept a list of identifiers for the records to queue (e.g., a JSON array of `place_id` strings).
- **Implementation:** This endpoint will perform an `UPDATE` operation on the `places_staging` table, setting the `deep_scan_status` to `queued` for the provided IDs and clearing `deep_scan_error`. It should handle authentication and authorization appropriately. **This endpoint does NOT change the main `status` field.**

#### 4.6.5 Background Monitoring Service (Scheduled Task)

A background service is required to monitor the queue and initiate processing. This involves **modifying the existing scheduler service (`src/services/sitemap_scheduler.py`)** to include a job or enhance an existing job (`process_pending_jobs`) that handles the deep scan queue.

- **Purpose:** Periodically check `places_staging` for records with `deep_scan_status = 'queued'` and trigger the single-place processing logic.
- **Location:** `src/services/sitemap_scheduler.py` (within the `process_pending_jobs` function).
- **Mechanism:** Likely using APScheduler, configured via `setup_sitemap_scheduler`.
- **Logic:**
  - Query `places_staging` for `deep_scan_status == 'queued'`, ordered by `updated_at` (oldest first), potentially using `LIMIT` and `FOR UPDATE SKIP LOCKED` for concurrency control.
  - For each found record:
    - Create a new session scope (`async with get_background_session() as session:`).
    - Within a transaction (`async with session.begin():`), update `deep_scan_status` to `processing`.
    - Call the `PlacesDeepService.process_single_deep_scan(place_id=..., tenant_id=...)` method.
    - Based on the outcome of the service call:
      - On success: Within a new transaction, update `deep_scan_status` to `completed` and clear `deep_scan_error`.
      - On failure: Within a new transaction, update `deep_scan_status` to `failed` and store the error message in `deep_scan_error`.
  - Handle errors gracefully within the loop and at the job level.

### 4.7 Detailed Curation-Driven Workflow Steps

This numbered sequence provides a step-by-step breakdown of the Curation-Driven Deep Scan workflow as implemented:

**1.0 User Initiation & API Trigger**
   *   **1.1** User interacts with the UI, selecting one or more places from `places_staging` (e.g., by setting their main `status` to `SELECTED_FOR_DEEP_SCAN`).
   *   **1.2** The UI triggers an API call.

**2.0 API Layer Processing (`src/routers/places_staging.py`)**
   *   **2.1** Request received at the `PUT /api/v3/places-staging/status` endpoint.
   *   **2.2** Endpoint logic performs a database update on the `places_staging` table for the specified `place_ids`:
        *   Sets the main `status` field to the value provided (e.g., `SELECTED_FOR_DEEP_SCAN`).
        *   Sets the `deep_scan_status` field to `queued`.
        *   Clears the `deep_scan_error` field.
        (This occurs within a single transaction).

**3.0 Background Scheduler Activation**
   *   **3.1** The state change in the database (`deep_scan_status = 'queued'`) makes the item eligible for background processing.
   *   **3.2** An independent background scheduler (APScheduler, configured and run likely via `main.py` or similar entry point based on `setup_sitemap_scheduler`) periodically executes its jobs.

**4.0 Background Processing Job Execution (`src/services/sitemap_scheduler.py`)**
   *   **4.1** The scheduler executes the `process_pending_jobs` function.
   *   **4.2** `process_pending_jobs` queries the `places_staging` table to find records where `deep_scan_status == DeepScanStatusEnum.queued`.
   *   **4.3** For *each* queued place found:
        *   **4.3.1** Update the specific place record in `places_staging`: Set `deep_scan_status` = `DeepScanStatusEnum.processing`. (Occurs in a transaction within the scheduler function).
        *   **4.3.2** Trigger the deep scan service by calling `PlacesDeepService.process_single_deep_scan(place_id=..., tenant_id=...)`.

**5.0 Deep Scan Service Execution (`src/services/places/places_deep_service.py`)**
   *   **5.1** The `process_single_deep_scan` method receives the `place_id` and `tenant_id`.
   *   **5.2** It calls the external Google Places Details API using the `place_id`.
   *   **5.3** It receives the detailed place information response from the API.
   *   **5.4** It maps the received data to the fields of the `LocalBusiness` model (defined in `src/models/local_business.py`).
   *   **5.5** It performs an upsert operation (insert or update on conflict based on `place_id`) into the `local_businesses` table. (Likely manages its own session/transaction for this database operation).
   *   **5.6** The method finishes and returns control to the caller (the scheduler function), implicitly indicating success or raising an exception on failure.

**6.0 Final Status Update (within `src/services/sitemap_scheduler.py`)**
   *   **6.1** Back in the `process_pending_jobs` function's loop for the specific place:
        *   **6.1.1 (Success Path):** If the call to `process_single_deep_scan` (Step 4.3.2) completed without error:
            *   Update the place record in `places_staging`: Set `deep_scan_status` = `DeepScanStatusEnum.complete`, Set `deep_scan_error` = `None`. (Occurs in a new transaction).
        *   **6.1.2 (Failure Path):** If the call to `process_single_deep_scan` raised an exception:
            *   Catch the exception.
            *   Update the place record in `places_staging`: Set `deep_scan_status` = `DeepScanStatusEnum.failed`, Set `deep_scan_error` = captured error message. (Occurs in a new transaction).

**7.0 Workflow Conclusion (for the specific item)**
   *   **7.1** The processing loop within `process_pending_jobs` continues to the next queued item, if any.
   *   **7.2** Once all queued items in the batch are processed (or the limit is reached), the `process_pending_jobs` execution finishes until the scheduler triggers it again.

## 5. VERIFICATION AND TESTING PLAN

This section outlines the testing strategy for the Curation-Driven Deep Scan workflow.

### 5.1 Unit Tests

Unit tests should focus on isolating and verifying individual components:

1.  **Models:**

    - Test the new `LocalBusiness` model (`src/models/local_business.py`): Creation, serialization, validation.
    - Test the modifications to the `Place` model (`src/models/place.py`): Ensure new `PlaceStatusEnum` values work correctly, test logic involving the new `deep_scan_error` field if added.

2.  **Queueing Endpoint (`PUT /api/v3/places-staging/queue-deep-scan`):**

    - Test successful queuing: Verify correct status update in `places_staging` for valid input IDs.
    - Test input validation: Invalid IDs, empty list, non-existent IDs.
    - Test permissions/authorization logic.
    - Test edge cases (e.g., attempting to queue already processed items, if applicable).

3.  **Background Monitoring Service (APScheduler Job):**

    - Test querying logic: Ensure it correctly identifies records with `status == 'QueuedForDeepScan'`.
    - Test status update logic: Verify it correctly updates status to `ProcessingDeepScan`.
    - Test triggering mechanism: Mock the call to the single-place processing logic and verify it's called with the correct `place_id`.
    - Test batching logic if implemented.
    - Test error handling within the scheduler job itself.

4.  **Single-Place Deep Scan Service Logic (`PlacesDeepService` or similar):**
    - Mock the `googlemaps` library call: Test logic for handling successful API responses (data mapping).
    - Mock the `googlemaps` library call: Test logic for handling various API errors (4xx errors, empty results).
    - Test data mapping logic: Ensure correct transformation from mocked API response to `LocalBusiness` model fields.
    - Test database interaction: Mock session/DB calls to verify `local_businesses` upsert logic and `places_staging` status updates (`DeepScanComplete`, `DeepScanFailed`, error message storage).

### 5.2 Integration Tests

Integration tests should verify the collaboration between components:

1.  **End-to-End Happy Path:**

    - Seed `places_staging` with test data (status `New`).
    - Call the `PUT /queue-deep-scan` endpoint with valid IDs.
    - Verify `places_staging` records are updated to `QueuedForDeepScan`.
    - Manually trigger or wait for the background scheduler job to run.
    - Verify `places_staging` records transition to `ProcessingDeepScan` and then `DeepScanComplete`.
    - Verify corresponding records are created/updated correctly in the `local_businesses` table with expected detailed data (requires mocking the actual Google API call at the boundary).

2.  **Error Scenarios:**

    - Test handling of invalid `place_id` during API call: Verify `places_staging` status updates to `DeepScanFailed` with an appropriate error message.
    - Test handling of Google API quota errors or other 4xx errors (requires mocking): Verify `DeepScanFailed` status and error message.
    - Test handling of database errors during `local_businesses` save: Verify `DeepScanFailed` status.
    - Test idempotency: Attempting to queue/process the same record multiple times (should ideally result in successful completion once or handle gracefully).

3.  **Concurrency:** (If applicable) Test scenarios with multiple background workers processing items concurrently.

### 5.3 Manual Testing

Manual testing should verify the user-facing interaction and observe the background process:

```bash
# 1. Seed places_staging table with test records (e.g., using a script or DB tool)

# 2. Queue specific places for deep scan (replace IDs and token)
# Assuming internal places_staging.id is used:
# curl -X PUT http://localhost:8000/api/v3/places-staging/queue-deep-scan \
#  -H "Content-Type: application/json" \
#  -H "Authorization: Bearer YOUR_TOKEN" \
#  -d '{
#    "staging_ids": [1, 5, 10]
#  }'

# Assuming Google place_id is used:
curl -X PUT http://localhost:8000/api/v3/places-staging/queue-deep-scan \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "place_ids": ["ChIJN1t_tDeuEmsRUsoyG83frY4", "ChIJ0-75VHzyEmsR5wnTz6-sdRM"]
  }'

# 3. Check status in places_staging table (using DB tool or a dedicated status API if built)
# Observe status changing from 'New' -> 'QueuedForDeepScan' -> 'ProcessingDeepScan' -> 'DeepScanComplete' / 'DeepScanFailed'

# 4. Check local_businesses table for newly added/updated detailed records
```

## 6. IMPLEMENTATION ROADMAP

The implementation will follow a phased approach, prioritizing core functionality and allowing for iterative testing:

1.  **Phase 1: Core Deep Scan & Persistence Logic**

    - **Goal:** Implement and thoroughly test the logic for retrieving, mapping, and saving detailed information for a _single_ place.
    - **Tasks:**
      - Create the `LocalBusiness` SQLAlchemy model (`src/models/local_business.py`) based on the `local_businesses` table schema. Create necessary database migrations.
      - Implement the service logic (e.g., in `PlacesDeepService` or a new service) that:
        - Accepts a Google `place_id` as input.
        - Calls the Google Place Details API using the `googlemaps` library.
        - Handles API responses and errors.
        - Maps the detailed data to the `LocalBusiness` model fields.
        - Saves the mapped data to the `local_businesses` table using an upsert strategy.
      - Develop unit tests for this service logic, mocking the API call and database interactions.
      - Perform manual testing (e.g., via a script or temporary test endpoint) to trigger this logic with known `place_id`s and verify data persistence in `local_businesses`.

2.  **Phase 2: Queueing Mechanism**

    - **Goal:** Implement the mechanism for users to mark places for deep scanning.
    - **Tasks:**
      - Modify the `Place` model (`src/models/place.py`) and `PlaceStatusEnum` to include the new deep scan statuses (`QueuedForDeepScan`, `ProcessingDeepScan`, `DeepScanComplete`, `DeepScanFailed`) and potentially an error field. Create necessary database migrations.
      - Implement the `PUT /api/v3/places-staging/queue-deep-scan` API endpoint.
      - Implement the endpoint logic to update the `status` field in the `places_staging` table for the provided IDs.
      - Add unit and integration tests for the queueing endpoint, verifying status updates and input validation.
      - Perform manual testing using `curl` or a similar tool to verify the endpoint updates the `places_staging` table correctly.

3.  **Phase 3: Background Automation & Final Integration**
    - **Goal:** Automate the process of picking up queued items and triggering the deep scan logic.
    - **Tasks:**
      - Implement or integrate the background monitoring logic using APScheduler by **modifying the existing scheduler service file (`src/services/sitemap_scheduler.py`)**. Specifically, enhance the `process_pending_jobs` function to include the deep scan workflow alongside sitemap processing.
      - Implement the scheduler job logic within `process_pending_jobs` to:
        - Query `places_staging` for records with `deep_scan_status == 'queued'`.
        - Update status to `processing`.
        - Call the single-place processing logic developed in Phase 1 (`PlacesDeepService.process_single_deep_scan`), passing the `place_id` and `tenant_id`.
        - Implement the final status updates (`completed`, `failed`) and error logging in `places_staging` based on the outcome of the single-place processing logic.
      - Add unit tests for the scheduler job logic (mocking service calls and DB interactions), focusing on the deep scan portion.
      - Perform integration testing covering the full flow: queueing via API -> scheduler picks up -> single-place logic runs -> final status and data verified.
      - Optimize scheduling interval and batch processing (if needed) based on testing.

## 7. ARCHITECTURAL COMPLIANCE

This implementation must strictly adhere to the established architectural patterns and principles defined in the project's authoritative guides, ensuring consistency with recent development efforts. Key reference documents include:

- `Docs/Docs_1_AI_GUIDES/17-CORE_ARCHITECTURAL_PRINCIPLES.md`
- `Docs/Docs_1_AI_GUIDES/13-TRANSACTION_MANAGEMENT_GUIDE.md`
- `Docs/Docs_1_AI_GUIDES/07-DATABASE_CONNECTION_STANDARDS.md`
- `Docs/Docs_1_AI_GUIDES/11-AUTHENTICATION_BOUNDARY.md`
- `Docs/Docs_1_AI_GUIDES/15-API_STANDARDIZATION_GUIDE.md`
- `Docs/Docs_1_AI_GUIDES/21-SCHEDULED_TASKS_APSCHEDULER_PATTERN.md`
- `Docs/Docs_2_Feature-Alignment-Testing-Plan/500-GOOGLE-MAPS-API-ARCHITECTURAL_PATTERNS.md`
- Relevant patterns demonstrated in recent implementations (e.g., `project-docs/11-Background-Task-Scheduler/`)

Key compliance requirements applied to the Curation-Driven Deep Scan components:

1.  **Transaction Boundaries:**

    - The `PUT /api/v3/places-staging/status` router endpoint **must own** the transaction for updating the `places_staging` status (both `status` and potentially `deep_scan_status`).
    - The APScheduler background job logic within **`src/services/sitemap_scheduler.py`** (`process_pending_jobs`) **must create its own session and manage transaction scopes** using `get_background_session()` and `async with session.begin()` for its operations (querying queue, updating status to processing, handling final status updates after the service call).
    - The single-place processing service logic (`PlacesDeepService.process_single_deep_scan`) **must operate within the transaction context provided by the caller** (typically, it doesn't manage its own transactions for the core API call and mapping, but handles the final upsert in `local_businesses` within its own scope if needed, or relies on the caller's context if designed that way - verify implementation). **Correction:** The service method `process_single_deep_scan` appears to handle its own session/transaction for the upsert based on `14.2`, which is acceptable as long as it's consistent. The scheduler handles transactions for updating the _staging_ table status.

2.  **Session Management:**

    - Background tasks (logic within `src/services/sitemap_scheduler.py`) **must** acquire sessions via the `get_background_session()` utility.
    - Routers **must** use the `get_session_dependency`. Direct database connections are prohibited.

3.  **Separation of Concerns:**

    - The router (`PUT /api/v3/places-staging/status`) handles HTTP request/response and initial status updates.
    - The background job logic in `src/services/sitemap_scheduler.py` orchestrates the background process: finds queued items, updates staging status, calls the deep scan service.
    - The `PlacesDeepService` contains the core business logic for interacting with the Google API, mapping data, and persisting to `local_businesses`.
    - Database interactions occur via SQLAlchemy models and sessions within appropriate transaction scopes defined by the routers or the background job.

4.  **Error Handling & Status Updates:**

    - Comprehensive `try...except` blocks must be used in the router, the background job logic (`process_pending_jobs`), and the deep scan service (`process_single_deep_scan`).
    - Errors must be logged appropriately.
    - The `places_staging` status (`failed`) and `deep_scan_error` field must be updated reliably by the background job logic in `sitemap_scheduler.py` when errors occur during the `process_single_deep_scan` call.

5.  **Standardization:**
    - API endpoint (`PUT /api/v3/places-staging/status`) must adhere to API standardization guidelines (`15-`).
    - Use of UUIDs, authentication, etc., must follow established project standards.

Adherence to these patterns is critical for maintainability, testability, and consistency with the overall system architecture.

## 8. IMPLEMENTATION SUCCESS METRICS

We will measure the success of this implementation using the following metrics:

### 8.1 Architectural Compliance Metrics

```
┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│                    ARCHITECTURAL COMPLIANCE                     │
│                                                                 │
│  ┌─────────────────────────┐     ┌─────────────────────────┐   │
│  │ Transaction Boundaries  │     │ Session Management      │   │
│  │                         │     │                         │   │
│  │ ☐ Router owns          │     │ ☐ No direct database   │   │
│  │   transactions          │     │   connections           │   │
│  │                         │     │                         │   │
│  │ ☐ Background tasks     │     │ ☐ Get_session used     │   │
│  │   manage own boundaries │     │   for background tasks  │   │
│  │                         │     │                         │   │
│  │ ☐ No nested            │     │ ☐ No session leaks     │   │
│  │   transactions          │     │                         │   │
│  └─────────────────────────┘     └─────────────────────────┘   │
│                                                                 │
│  ┌─────────────────────────┐     ┌─────────────────────────┐   │
│  │ Separation of Concerns  │     │ Error Handling          │   │
│  │                         │     │                         │   │
│  │ ☐ Router handles HTTP  │     │ ☐ Exceptions properly   │   │
│  │   only                  │     │   logged                │   │
│  │                         │     │                         │   │
│  │ ☐ Service handles      │     │ ☐ Job status updates   │   │
│  │   business logic        │     │   on errors             │   │
│  │                         │     │                         │   │
│  │ ☐ Storage service      │     │ ☐ Client receives      │   │
│  │   handles persistence   │     │   appropriate status    │   │
│  └─────────────────────────┘     └─────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 8.2 Performance Metrics

Performance will be assessed based on the efficiency and responsiveness of the new workflow:

1.  **API Efficiency & Cost:**

    - Average Google Place Details API call duration.
    - Success rate of Place Details API requests (excluding expected 404s for invalid IDs).
    - Monitoring of overall API usage costs related to deep scans.

2.  **Processing Throughput & Latency:**

    - Latency of the `PUT /queue-deep-scan` endpoint response.
    - Average time taken for a place to transition from `QueuedForDeepScan` to a final state (`DeepScanComplete` or `DeepScanFailed`) by the background service.
    - Throughput of the background service (e.g., number of places processed per minute/hour).
    - Average processing time per individual place deep scan (API call + mapping + DB save).

3.  **Resource Utilization:**
    - Database connection usage by the background scheduler and processing logic.
    - CPU and memory utilization of the background service.

### 8.3 Functionality Metrics

Functional success will be measured by data quality and robustness:

1.  **Data Completeness & Accuracy:**

    - Percentage of successfully processed places (`DeepScanComplete` status) where data is correctly populated in the `local_businesses` table.
    - Accuracy of mapped fields compared to the source Google Places API data.

2.  **Error Handling & Reporting:**
    - Percentage of failed API calls or processing errors that result in a correct `DeepScanFailed` status update in `places_staging`.
    - Accuracy and usefulness of error messages stored in the `places_staging` table (if error field is added).
    - Resilience to transient errors (e.g., temporary network issues, handled by library retries or application logic).

## 9. CONCLUSION

The Google Maps Deep Scrape feature will extend the existing functionality to provide comprehensive place information. By closely following the established architectural patterns and reusing core components, we can ensure a robust, maintainable implementation that seamlessly integrates with the broader system.

This implementation plan serves as a comprehensive guide for creating the deep scrape functionality in a way that maintains architectural integrity while adding significant new capabilities.

```

```
````
