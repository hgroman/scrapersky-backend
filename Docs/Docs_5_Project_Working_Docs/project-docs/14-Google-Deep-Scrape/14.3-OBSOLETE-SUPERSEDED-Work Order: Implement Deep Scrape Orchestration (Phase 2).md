# 14.3 Work Order: Implement Deep Scrape Orchestration (Phase 2)

**Related Feature:** 14 - Google Deep Scrape
**Implementation Plan:** `14.1-GOOGLE MAPS DEEP SCRAPE IMPLEMENTATION PLAN.md` (Sections 4.3, 4.4)
**Architectural Pattern Reference:** `project-docs/13-Sitemaps/13.6-SITEMAP_ARCHITECTURE.md` (Producer-Consumer Pattern), `project-docs/11-Background-Task-Scheduler/job_table_architecture_assessment.md`
**Progress Log:** `project-docs/14-Google-Deep-Scrape/14.2-DEEP_SCRAPE_IMPLEMENTATION_PROGRESS.md`

**Phase Goal:**
Implement the system to orchestrate and execute the Google Maps Deep Scrape process for **user-selected** places identified during an initial Discovery Scan, leveraging the existing background task architecture (Producer-Consumer via Job Table and Scheduler).

**Overview:**
This phase focuses on bridging the gap between the initial Discovery Scan (which populates `places_staging`) and the detailed deep scan logic (implemented in `PlacesDeepService.process_single_deep_scan`). It involves **providing a mechanism for users to select places for deep scanning**, creating specific "Deep Scan Jobs" based on these selections, scheduling their execution, and implementing the service logic to process the selected places.

**Key Components & Pattern Alignment:**

- **Producer (UI Selection & Job Creation Trigger):** Implement a UI/API for users to select places from `places_staging` and an API endpoint to trigger the creation of a "Deep Scan" job.
- **Queue (Job Table):** Utilize the existing `jobs` table (`Job` model) to queue and track deep scan tasks (`job_type = 'google_maps_deep_scan'`).
- **Consumer (Scheduler):** Adapt the existing scheduler (`src/scheduler/scheduler.py`) to detect and trigger pending deep scan jobs.
- **Processor (`PlacesDeepService`):** Implement the main orchestration logic (`process_places_deep_scan_job`) within the service responsible for deep scanning.
- **Storage:** Utilize `places_staging` (`Place` model) as the source of work items (`place_id`s and selection status) and `local_businesses` (`LocalBusiness` model) as the destination for results.

**Implementation Tasks:**

**1. Implement UI/API for Place Selection:** - **Goal:** Allow users to view places discovered by a specific Discovery Scan job and mark them for deep scanning. - **Action:** Create a new API endpoint (e.g., `/places/staging/{discovery_job_id}`) to retrieve records from `places_staging` associated with a given discovery job ID. - **Action:** Create a corresponding UI component (or modify existing views) to display these staged places. - **Action:** Implement functionality (UI controls and a backend API endpoint, e.g., `PUT /places/staging/{place_id}/status`) to allow users to update the `status` field of selected `places_staging` records (e.g., from `'New'` to `'SELECTED_FOR_DEEP_SCAN'`). Consider batch update capabilities.

**2. Define Deep Scan Job Triggering Mechanism:** - **Approach:** Manual API Trigger (based on user selection). - **Action:** Create a new API endpoint (e.g., `POST /jobs/trigger-deep-scan/{discovery_job_id}`) that allows a user/admin to explicitly trigger a deep scan for a specific discovery job ID _after_ they have marked places using the UI/API from Task 1. - **Note:** This single job acts as a batch processor for the one or more places selected by the user (via Task 1) belonging to the specified `discovery_job_id`. - **Action:** This endpoint will create a **single** `Job` record in the `jobs` table with: - `job_type = 'google_maps_deep_scan'` - `status = 'pending'` - `params = {'discovery_job_id': discovery_job_id}` - Associated `tenant_id`. - **Note:** Ensure appropriate authorization for this trigger endpoint.

**3. Adapt Scheduler to Detect Deep Scan Jobs:** - **File:** `src/scheduler/scheduler.py` (Confirm adaptation, no cloning). - **Action:** Modify the scheduler's polling logic (e.g., the function triggered by `APScheduler`) to query the `jobs` table for records where `job_type = 'google_maps_deep_scan'` AND `status = 'pending'`. - **Action:** For each pending deep scan job found, extract the `job_id` and `tenant_id`. - **Action:** Trigger the main orchestration method `PlacesDeepService.process_places_deep_scan_job`, passing the `job_id` and `tenant_id`. Ensure this triggering happens within a suitable async context (e.g., using `asyncio.create_task` or similar if the scheduler loop itself is synchronous). Handle potential errors during triggering.

**4. Implement `PlacesDeepService.process_places_deep_scan_job` Method:** - **File:** `src/services/places/places_deep_service.py` - **Method Signature:** `async def process_places_deep_scan_job(self, job_id: UUID, tenant_id: UUID)` - **Logic:** - Use `JobService` (or direct DB access) to fetch the deep scan `Job` record by `job_id`. Handle "not found" cases. - Update the job status to `processing`. - Extract the source _Discovery Scan_ `job_id` from the current job's `params` field. - Obtain a database session (`await get_session()`). - **Modify Query:** Query the `places_staging` table (`Place` model) for records where: - `search_job_id` matches the _Discovery Scan_ `job_id` - `tenant_id` matches - `status = 'SELECTED_FOR_DEEP_SCAN'` (or the status set in Task 1) - `processed = False` - Handle potential errors during query. - Initialize progress tracking variables (e.g., `processed_count`, `failed_count`, `total_count`). - Iterate through the retrieved `Place` records: - Extract the `place_id`. - Call `await self.process_single_deep_scan(place_id=place_id, tenant_id=tenant_id)`. - Handle the return value (success/failure). Increment `processed_count` or `failed_count`. - **Update Staging Record:** Upon successful processing of a place, update the corresponding `Place` record in `places_staging` to set `processed = True`. Consider also updating its `status` if appropriate (e.g., to `'DEEP_SCAN_COMPLETE'`). - Periodically update the `Job` record's `progress` field (e.g., every 10 places). Use `JobService` or direct DB access. - Implement robust error handling for individual `process_single_deep_scan` calls (log errors, increment `failed_count`, continue loop). Consider setting an error status on the specific `places_staging` record. - After the loop, determine the final job status (`completed` if `failed_count == 0`, potentially `completed_with_errors` or `failed` otherwise). - Store final results/summary (e.g., `processed_count`, `failed_count`) in the `Job` record's `result_data` field. - Update the final `Job` status and potentially `error_message`. - Ensure proper session closing/cleanup.

**5. Refine `JobService` (If Needed):** - **File:** `src/services/job_service.py` - **Action:** Ensure `JobService` has methods suitable for updating job progress, status, results, and error messages efficiently, especially for potentially frequent progress updates. Consider if existing methods are sufficient or if specialized ones are needed.

**6. Testing Considerations (Refer to Section 5 of Plan 14.1):** - **Unit Tests:** - Test the new API endpoints for selecting places and triggering jobs. - Test the updated logic within `process_places_deep_scan_job` in isolation (mocking DB calls, `process_single_deep_scan`), including the modified query. Test edge cases (no selected places found, all selected places fail). - **Integration Tests:** - Test the scheduler's ability to correctly detect and trigger the `process_places_deep_scan_job` method for the new job type. - Test the flow: Select places -> Trigger job -> Scheduler picks up job -> `process_places_deep_scan_job` executes (with mocked `process_single_deep_scan`) -> Job status updates correctly -> `places_staging` records are marked as processed. - **End-to-End Tests:** Simulate the creation of a discovery job, its completion, user selection of places via the new UI/API, triggering the deep scan job, and verify the final state of the deep scan job record, the `local_businesses` table enrichment, and the `places_staging` record statuses/processed flags.

**7. Documentation:** - Update `14.2-DEEP_SCRAPE_IMPLEMENTATION_PROGRESS.md` as tasks are completed. - Refine `14.1-GOOGLE MAPS DEEP SCRAPE IMPLEMENTATION PLAN.md` to reflect the user selection and manual trigger flow. - Update the architecture assessment in `project-docs/11-Background-Task-Scheduler/job_table_architecture_assessment.md` if any nuances arose. - Add sequence diagrams or update existing ones if helpful to visualize the updated orchestration flow, including the user selection step.

**Priority/Timing:** High. This is the next major functional step for the Deep Scrape feature.
