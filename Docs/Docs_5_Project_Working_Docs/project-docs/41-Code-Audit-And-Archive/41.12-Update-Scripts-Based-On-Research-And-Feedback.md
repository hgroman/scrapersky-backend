Below is a concrete upgrade plan that folds in everything you learned from **41.11‑Tracing Python Module Usage in a FastAPI Codebase** and replaces the fragile parts of the current tool‑chain.

---

## 1 · Update the static–analysis step

| Old approach                                                                          | Replacement                                                                                                                   | Why it’s better                                                                                                                                                                                    |
| ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Hand‑rolled `static_import_trace.py` that walks the AST from _only_ a couple of files | **pydeps** (or **ModuleGraph**) run once for **every entry script** (`src/main.py`, each `scripts/*.py`, and any test runner) | • Maintained & Python‑3‑only<br>• Follows _transitive_ imports automatically<br>• Handles `__init__.py`, namespace pkgs, and many corner‑cases without code you need to maintain citeturn1view0 |
| Manually recursing into imports                                                       | The tool’s own import graph (JSON list exported with `--show-deps` for pydeps, or the public API for ModuleGraph)             | Byte‑code analysis means it finds imports that an AST grep can miss (e.g. inside f‑strings, async defs, etc.) citeturn5view0                                                                    |

> **Recommended default:** `pydeps` — easiest CLI, actively released (v 3.0.1 in Feb 2025) and produces a plain‑text/JSON list of every reachable module citeturn1view0.

### How to generate the static list

```bash
# one run per entry‑point, write results to a temp file
pydeps src/main.py          --show-deps --noshow > /tmp/deps_main.json
for f in scripts/*.py; do
  pydeps "$f" --show-deps --noshow >> /tmp/deps_scripts.json
done

# merge and keep only in‑repo paths
jq -s 'add | keys[]' /tmp/deps_*.json \
  | grep -E '^src/' \
  | sort -u > reports/used_files_static.txt
```

_(If you prefer ModuleGraph, the snippet in Appendix A shows the equivalent Python code.)_

---

## 2 · Keep — but shrink — the custom helpers

| Helper                     | Keep? | Changes                                                                                                                                                                                                                                                                                                                                   |
| -------------------------- | ----- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `scheduler_trace.py`       | ✔     | No logic change – continue AST‑scanning every `scheduler.add_job(…)` so we don’t miss job modules referred to only by a string.                                                                                                                                                                                                           |
| `dynamic_imports.py`       | ✔     | Still valuable for **manual audit**; leave it as is.                                                                                                                                                                                                                                                                                      |
| `runtime_import_logger.py` | ✔     | Make sure it logs **file paths**, not only module names, then: <br>• enable it during  `pytest`<br>• add a lightweight harness that executes _each_ `scripts/*.py` with `--dry-run` flags to capture their imports<br>• in CI, start the FastAPI app, let the scheduler tick at least once, and hit one “happy‑path” endpoint per router. |

---

## 3 · New wrapper: `tools/run_full_trace.py`

```python
#!/usr/bin/env python
"""
Run the complete tracing pipeline:

1.  Static import scan (pydeps) for every entry‑point
2.  Scheduler trace
3.  Dynamic‑import pattern scan
4.  Merge static + pattern + runtime log into:
      • reports/all_used_modules.json
      • reports/unused_candidates.json
"""
import json, os, subprocess, sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]   # project root
REPORTS = ROOT / "reports"
REPORTS.mkdir(exist_ok=True)

ENTRY_SCRIPTS = [
    ROOT / "src" / "main.py",
    *Path(ROOT / "scripts").glob("*.py"),
]

def run_pydeps(target: Path) -> set[str]:
    result = subprocess.check_output(
        ["pydeps", str(target), "--show-deps", "--noshow"],
        text=True
    )
    return {line.strip() for line in result.splitlines()
            if line.startswith("src/") and line.endswith(".py")}

def collect_static() -> set[str]:
    all_mods: set[str] = set()
    for script in ENTRY_SCRIPTS:
        all_mods |= run_pydeps(script)
    return all_mods

def collect_scheduler() -> set[str]:
    data = json.loads((REPORTS / "scheduler_jobs.json").read_text())
    return {entry["file"] for entry in data}

def collect_dynamic_patterns() -> set[str]:
    data = json.loads((REPORTS / "dynamic_imports.json").read_text())
    return {entry["file"] for entry in data}

def collect_runtime() -> set[str]:
    paths = []
    log = REPORTS / "runtime_imports.log"
    if log.exists():
        for name in log.read_text().splitlines():
            if name.startswith("src."):
                paths.append(str(ROOT / (name.replace('.', '/') + ".py")))
    return set(filter(Path.exists, map(Path, paths)))

def main():
    static = collect_static()
    sched  = collect_scheduler()
    dyn    = collect_dynamic_patterns()
    run    = collect_runtime()

    used = static | sched | dyn | run
    with (REPORTS / "all_used_modules.json").open("w") as fp:
        json.dump(sorted(used), fp, indent=2)

    # full project scan
    all_py = {str(p) for p in ROOT.rglob("src/**/*.py")}
    unused = sorted(all_py - used)
    json.dump(unused, (REPORTS / "unused_candidates.json").open("w"), indent=2)

if __name__ == "__main__":
    sys.exit(main())
```

_Run order inside CI / locally:_

1. `python tools/scheduler_trace.py …`
2. `python tools/dynamic_imports.py …`
3. **Execute tests + scripts + start app** (with `runtime_import_logger` active).
4. `python tools/run_full_trace.py`

---

## 4 · CI integration guardrail

Add a job that fails the pipeline if **new** unused files appear compared to the previous merge‑base:

```bash
git diff --name-only origin/main...HEAD \
  | grep '\.py$' \
  | xargs -I{} bash -c \
      'grep -qxF "{}" reports/unused_candidates.json && \
       { echo "::error::File {} appears unused."; exit 1; } || true'
```

This prevents dead code from creeping back in.

---

## 5 · What to do next

1. **Replace** `static_import_trace.py` with the pydeps/modulegraph routine.
2. **Delete** the 21 “unused” entries you’re confident about after re‑running the new pipeline (expect the list to shrink dramatically once scripts are included).
3. **Document** a one‑liner in `CONTRIBUTING.md`:

   > “Run `make trace‑deps` before pushing; commit the updated `reports/*.json`.”

---

### Appendix A – Static scan with ModuleGraph (alternative)

```python
from modulegraph.modulegraph import ModuleGraph
from pathlib import Path

ROOT = Path(__file__).parents[1]
mg = ModuleGraph()

# add every entry script
mg.run_script(ROOT / "src" / "main.py")
for path in (ROOT / "scripts").glob("*.py"):
    mg.run_script(path)

mg = mg # graph ready
used = {
    n.filename for n in mg.flatten()
    if n.filename and n.filename.startswith(str(ROOT / "src"))
}
Path("reports/used_files_static.json").write_text(
    json.dumps(sorted(used), indent=2)
)
```

---

With these upgrades in place you’ll have a **single‑command, repeatable** process that:

- Considers **all** execution roots (`main.py`, scheduler, CLI scripts).
- Leverages a **maintained** static graph engine (pydeps or ModuleGraph).
- Keeps your custom sched‑trace / dynamic‑pattern scanners for edge cases.
- Confirms reality with runtime import logging.

…and surfaces a clean, trustworthy list of _truly_ unused Python files, ready for deletion.
