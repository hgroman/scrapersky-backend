Your “Hybrid AST + grep” approach is solid and pragmatic, and your document is clear and well‑structured. Below are some suggestions to make sure you’re as thorough as possible in identifying both used and truly unused code.

---

## 1. Strengths of Your Approach  
- **Static, deterministic analysis** via Python’s `ast` guarantees you’re reading the actual code structure rather than fragile regexes.  
- **Router‑aware** detection by parsing `app.include_router()` calls ensures you don’t miss FastAPI endpoints.  
- **Conservative “reachable only” criterion** ensures you won’t accidentally archive code that’s legitimately imported.  
- **Explicit manual review step** guards against false positives from dynamic or reflection‑based imports.  

---

## 2. Potential Gaps & Recommendations  

| Area                         | Gap / Risk                                                                              | Recommendation                                                                    |
|------------------------------|-----------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|
| **Dynamic imports**          | You note `__import__`, `importlib`, string‑based imports won’t be tracked.             | • Add an entry in your `cleanup_config.yaml` for any known dynamic‑import modules. <br>• Search codebase for patterns like `__import__\(` or `importlib.import_module` and flag those files for manual review. |
| **Test‑only utilities**      | Utilities or fixtures used only by pytest (or other test runners) won’t show up.       | • Run `pytest --collect-only` to see all test modules and cross‑reference. <br>• Temporarily include `src/tests/` in your import graph to catch any imports from those tests.|
| **CLI entry points**         | Scripts invoked via `python -m` or setup.py entry points may be missed.                | • Scan `setup.py`/`pyproject.toml` for `[console_scripts]` and include those modules. <br>• Search for `if __name__ == "__main__":` blocks and add them as additional roots.|
| **Background tasks & schedulers** | You trace scheduler modules but dynamic task registration (e.g. decorators) can hide imports. | • Search for decorators like `@app.on_event`, `@celery.task`, or scheduler‑registration calls and include those modules.|
| **Reflection / getattr**     | Code that uses `getattr(module, ...)` to call functions by name won’t be detected.      | • Flag any files containing `getattr(` or `hasattr(` for manual inspection.|
| **Resource‑only modules**    | Modules providing data via `importlib.resources`, or implicit package data, may not be “imported”. | • Search for `importlib.resources` or packaging data declarations (e.g., in `MANIFEST.in`) and treat corresponding modules as used.|
| **Within‑file dead code**    | You’ve deferred this to Step 5, but it’s worth noting that some functions or classes inside “used” modules may themselves be unused. | • Plan to run [Vulture](https://pypi.org/project/vulture/) with a high confidence threshold, then manually inspect its false positives (e.g., serializers, data classes).|

---

## 3. Additional Validation Layers  

1. **Runtime Import Tracing**  
   - Spin up a development server (e.g. `uvicorn src.main:app --reload`) with `PYTHONPATH=.`, and attach a simple import hook (e.g. via `sys.setprofile`) to log every module that’s actually loaded during:
     - Full API exercise (curl, Postman, or smoke tests).  
     - At least one run of any background‐task scheduler.  
   - Compare that log to your static `used_files.json`—anything loaded at runtime but not in the static graph should be whitelisted or reinvestigated.

2. **Coverage‑driven insights**  
   - Run `pytest --cov=src --cov-report=term --cov-report=xml` and pull out modules with zero coverage. Cross‑check these against your static “unused candidates.”  
   - Modules showing any coverage should be considered “used,” even if static analysis missed a dynamic import path.

3. **Automated Smoke Tests**  
   - For FastAPI, use a tool like [schemathesis](https://schemathesis.readthedocs.io/) or [pytest‑httpx](https://github.com/Colin-b/pytest-httpx) to generate requests against all endpoints; monitor server logs for any `ImportError` or 500 errors referencing unexpected modules.

---

## 4. Refining Your Candidate List  

- **Group by category** (e.g. `api/models`, `services/sitemap`, `utils`) in your PR description so reviewers can apply domain knowledge.  
- **Annotate each file** in your review PR with a one‑sentence hypothesis of its original purpose (e.g., “`services/sitemap/analyzer_service.py` likely powers our nightly sitemap generation”). This helps reviewers spot false positives faster.  
- **Mark “suspended”** any candidate touching email scanning until the `40.1` refactor is merged, to avoid accidental archiving under the paused step.

---

## 5. Next Steps  

1. **Enhance `cleanup_config.yaml`** with dynamic/CLI/test exclusions based on the above patterns.  
2. **Run supplemental analyses** (runtime tracing, coverage) and regenerate your reports.  
3. **Produce a review PR** listing the 24 candidates (grouped and annotated) plus any new ones discovered by dynamic tracing or coverage.  
4. **Collect human feedback**, then proceed with the safe archiving commands as per Step 4 of the work order.

With these additions, you’ll cover almost every edge case—static, dynamic, test‑only, and runtime—and give your reviewers the confidence that you’ve left no orphaned code unexamined.