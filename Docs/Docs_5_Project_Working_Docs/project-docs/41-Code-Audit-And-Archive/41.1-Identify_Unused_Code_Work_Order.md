# Work Order: Identify & Archive Unused Code in ScraperSky Backend

**Date:** 2024-07-27
**Author:** [Your Name/Team Name Here] // TODO: Fill in author
**Version:** 2.3 (Final context integration)

**Objective:** Systematically identify, review, and archive potentially unused Python files within the `src/` directory using a robust, automated, and safe process, building upon previous cleanup efforts.

**Background:** To maintain code health and reduce complexity, we need a reliable method to find and handle files potentially orphaned by refactoring or feature changes. This process emphasizes safety through backups, automation, explicit exclusions, manual review, and archiving rather than immediate deletion. This work order follows previous cleanup activities documented in `project-docs/28-CLEAN-UP-Pre-Render/`, `project-docs/33-*`, `project-docs/35-*` and coordinates with the planned audit in `project-docs/39-*`.

---

### ‚è™ -1. Already Completed Cleanup (Context from `28.1-*`, `33.2-*`, `35.1-*`, `39.1-*`, `40.1-*`)

_This section summarizes relevant cleanup actions already performed._

- **Critical File Removal:** `.env.bak` deleted.
- **Legacy Script Archiving:** `bin/`, `run_tests.sh`, `setup.py`, `CURSOR.md`, various `*.backup` files, and specific `.tsx`/`.sql`/`.py` files from `src/` moved to `_Archive_4.13.25/`.
- **Static File Cleanup:** Unused HTML/CSS/template files from `/static` moved to `/static/Static-Archive` (see `33.2`).
- **Codebase Reference Update:** References to `google-maps.html` updated to `scraper-sky-mvp.html` across codebase (see `35.1`).
- **Git Hygiene:** `.gitignore` updated to exclude archives, backups, etc.
- **Dependencies:** `requirements.txt` regenerated.
- **Initial Ruff Cleanup:** `ruff check . --select I --fix` run (fixed 125 unused imports).
- **Known Remaining Issues:** Subsequent `ruff check .` identified ~2500 issues, including many `F401` (unused import) and `F841` (unused variable).
- **Page Rename:** Documentation updated for `/static/scraper-sky-mvp.html` rename.
- **(Completed) Background Task Audit:** An audit (`39.1`) confirmed standard scheduler usage for many tasks but identified several non-standard `BackgroundTasks` triggers (incl. Email Extraction, Batch Sitemap/Page), which may influence cleanup review (see Step 4) and timing.

---

### ‚ú® 0. Pre-Flight & Safety

1.  **Create Reports Directory:** Ensure the output directory exists:
    ```bash
    mkdir -p reports
    ```
2.  **Create Branch:** Before starting, create a dedicated branch from the target base (e.g., `main`):
    ```bash
    git checkout main
    git pull origin main
    git checkout -b feature/code-cleanup-YYYYMMDD
    ```
3.  **Tag Backup:** Create a git tag on the starting commit for easy rollback if necessary:
    ```bash
    git tag pre-cleanup-$(date +%Y-%m-%d)
    # git push origin pre-cleanup-YYYY-MM-DD # Optional: push tag to remote
    ```
4.  **(Optional) Rollback Command:** If major issues arise, revert using the tag:
    ```bash
    # Example rollback (use with caution!)
    # git checkout main && git reset --hard pre-cleanup-YYYY-MM-DD
    ```

---

### ‚öôÔ∏è 1. Entry-Point & Automated Import Tracing

1.  **Develop/Utilize Trace Script:** Implement or use a script (e.g., `tools/trace_imports.py` leveraging `modulefinder` or [`importlab`](https://pypi.org/project/importlab/)) that:
    - Takes `src/main.py` as the primary entry point.
    - Parses `app.include_router` calls to find initial router modules.
    - Recursively follows all `import` statements within the `src/` directory.
    - Accounts for imports related to registered scheduler tasks (ensure functions in `src/services/*_scheduler.py` and their dependencies are traced).
    - Respects the `exclude_paths` defined in the specified config file (see Section 2).
    - Provides clear CLI options for configuration and output.
2.  **Execute Trace & Generate Reports:** Run the script with appropriate arguments:
    ```bash
    # Example Usage:
    python tools/trace_imports.py --entry src/main.py --config reports/cleanup_config.yaml \
      --out-used reports/used_files.json --out-unused reports/unused_candidates.json
    ```
    - **Output `reports/used_files.json`:** A JSON list containing the absolute or relative paths of all discovered modules/files deemed "in use" by the trace.
    - **Output `reports/unused_candidates.json`:** A JSON list of `.py` files found within `src/` that were _not_ present in `used_files.json` and are not excluded by the config.

---

### üóÇÔ∏è 2. Exclude-List & Configuration

1.  **Create Config File:** Maintain an explicit exclusion list at `reports/cleanup_config.yaml`:
    ```yaml
    # reports/cleanup_config.yaml
    # Paths (glob patterns allowed) to explicitly exclude from the "unused candidates" list.
    # Useful for dynamically loaded modules, CLI tools, or files known to be used
    # in ways static tracing might miss (e.g., via exec).
    exclude_paths:
      - src/scripts/** # Exclude utility scripts if they reside under src
      - src/tests/** # Exclude tests if they reside under src
      - src/migrations/** # Exclude migration-related code if under src
      - src/cli_tools/** # Example: Exclude standalone CLI tools
      - src/utils/dynamic_importer.py # Example: File known to be used dynamically
      # Add any other known exceptions here
    ```
2.  **Ensure Script Adherence:** Verify the tracing script correctly reads `reports/cleanup_config.yaml` (or the path provided via `--config`) and applies these exclusions when generating `reports/unused_candidates.json`.

---

### üîç 3. Coverage & Runtime Validation (Optional but Recommended)

_These steps help catch code missed by static tracing._

1.  **Run Test Suite with Coverage:** Execute the full test suite and generate a coverage report:
    ```bash
    pytest --cov=src --cov-report=xml --cov-report=term
    ```
    - Review the terminal report (`term`) and the XML (`coverage.xml`) / HTML report.
    - Manually cross-reference files listed in `unused_candidates.json` against the coverage report. If a candidate file shows coverage, investigate why the static trace missed it (e.g., dynamic import via tests, test-only utility) and potentially move it to `exclude_paths` in `cleanup_config.yaml`.
2.  **Automated UI Smoke Test:** (If feasible) Implement and run a simple automated UI test (e.g., using Selenium, Playwright) that navigates through all major tabs and features of `/static/scraper-sky-mvp.html` while the development server is running. Monitor server logs for any errors related to missing modules, especially if dynamic loading is suspected.
    - **Note:** This depends on ensuring automated tests are updated to use the correct URL (`/static/scraper-sky-mvp.html`) as per `28.3-Post-MVP-Page-Rename-Tasks.md`.

---

### üìã 4. Candidate Review & Archiving

1.  **Generate Review PR:**
    - Commit the generated reports (`used_files.json`, `unused_candidates.json`, `cleanup_config.yaml`).
    - Create a Pull Request targeting the base branch (e.g., `main`).
    - The PR description **must** clearly list all file paths from `reports/unused_candidates.json`, preferably grouped by directory, for easy review.
2.  **Mandatory Human Review:** Assign the PR to relevant team members. **No file movement or archiving occurs until explicit approval is granted on the PR after thorough review.** Reviewers should:
    - Verify each candidate file is genuinely unused by the core application.
    - Consider if any candidates are used by external processes or non-traced scripts.
    - Confirm candidates aren't needed for upcoming features or refactoring.
    - **Check Audit Report (`39.1`):** For candidates potentially related to background tasks, cross-reference with the audit report to understand if they are part of standard (scheduled) or non-standard (API `BackgroundTasks`) workflows.
3.  **Archive Confirmed Orphans:** Once approved, move the confirmed unused files into a dedicated archive directory _within_ the source tree (to preserve history easily):
    ```bash
    # Example commands (run after PR approval)
    ARCHIVE_DATE=$(date +%Y-%m-%d)
    mkdir -p src/archive/${ARCHIVE_DATE}
    git mv src/path/to/old_feature_x.py src/archive/${ARCHIVE_DATE}/
    git mv src/utils/orphaned_helper.py src/archive/${ARCHIVE_DATE}/
    # ... repeat for all confirmed candidates ...
    git commit -m "feat: Archive confirmed unused code modules identified on ${ARCHIVE_DATE}"
    ```
    _Note: Moving files (`git mv`) preserves their history better than copy/delete._

---

### üßπ 5. Within-File Dead-Code Sweep (Post-Archiving)

*After the main archiving PR (Step 4) is merged, focus on cleaning *within* the remaining active files.*

- **Context:** Previous runs (`28.1`) identified ~2500 Ruff issues remaining after initial unused import fixes, including many `F401` (unused import) and `F841` (unused variable). A detailed plan for tackling these exists in `28.2-RUFF-Cleanup-and-debug.md`.
- **üö® CRITICAL NOTE:** Execution of this step (especially for `email_scanner.py` and `email_scraper.py`) is currently **PAUSED / BLOCKED** pending completion of the Email Scraper refactor (**Work Order `40.1`**), as documented in `28.2`. Coordinate execution with that refactor **and consider findings from the Background Task Audit (`39.1`)** before finalizing within-file cleanups in task-related modules.

1.  **Run Vulture/Ruff:** Execute `vulture` and/or `ruff check . --select F` on the `src` directory (excluding `src/archive`), setting appropriate confidence/rules. Consider using configuration files (`.vultureignore`, `ruff.toml`) to manage known false positives or specific rule sets.
    ```bash
    # Example Vulture run
    vulture src/ --min-confidence=80 --exclude src/archive --ignore-names 'setup,test_*'
    # Example Ruff run for unused variables/imports
    # ruff check . --select F401,F841 --fix # Use --diff first for preview
    ```
2.  **Record Report:** Save the output to `reports/vulture_dead_code_YYYY-MM-DD.txt` and/or `reports/ruff_cleanup_YYYY-MM-DD.txt`.
3.  **Schedule Follow-Up:** Create separate tasks or PRs based on the reports to address the identified dead code within files, respecting the block on email scraper files until refactored.

---

### üìà 6. CI Integration & Guardrails

1.  **Create CI Check:** Implement a new CI workflow (e.g., `.github/workflows/cleanup-check.yml`) that:
    - Runs on PRs targeting the main branch.
    - Executes the automated import tracing script (from Step 1), ensuring it uses the committed `reports/cleanup_config.yaml`.
    - Compares the script's output against the current file structure.
    - **Fails the build** with a clear error message if any `.py` file in `src/` (excluding paths in `cleanup_config.yaml` and `src/archive/`) is _not_ found in the trace's 'used file' list.
      - _Example Failure Message:_ `‚ùå Cleanup Check: file src/some/orphan.py is untraced and not in cleanup_config.yaml exclude_paths. Please either add it to the config or remove/archive the file.`
      - The script should exit with a non-zero code to signal failure.
2.  **(Optional) Pre-commit Hook:** Consider adding a `pre-commit` hook that prevents `git commit` if it attempts to delete `.py` files from `src/` that are not explicitly listed in `cleanup_config.yaml` under a specific 'allow_deletion' key (to add friction against accidental deletions).

---

### üóìÔ∏è 7. Next Steps & Example Timeline

_This is a suggested flow after this work order is approved._

- **Day 1:** Execute Steps 0 & 1 (Create Reports Dir, Branch, Tag, Run Trace Script, Generate Reports). Create initial PR (Step 4.1) for review.
- **Day 2-3:** Team performs human review of candidate orphans listed in the PR (Step 4.2). Discuss any contentious files.
- **Day 3-4:** (Post-Approval) Execute archive movement (`git mv`) for confirmed orphans (Step 4.3). Update and merge the cleanup PR.
- **Day 4-5:** Initiate Within-File Sweep (Step 5), _respecting the PAUSED status_ for email scraper files. Run Vulture/Ruff on other areas and create follow-up tickets/PRs.
- **Ongoing:** Implement CI check (Step 6) to maintain hygiene. Monitor application stability in staging/production after merge. Coordinate completion of Step 5 with Email Scraper refactor.

---

This enhanced process provides multiple layers of safety and automation, creating a reliable playbook for current and future code cleanup efforts.
