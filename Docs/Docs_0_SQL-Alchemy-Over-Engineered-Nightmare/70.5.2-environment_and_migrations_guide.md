# Environment and Migrations Guide

## Environment Overview

### Project Structure

```
scraper-sky-backend/
├── alembic/
│   ├── versions/
│   ├── env.py
│   └── script.py.mako
├── src/
│   ├── main.py              # FastAPI application entry point
│   ├── models/              # SQLAlchemy ORM models
│   │   ├── base.py          # Base class for all models
│   │   └── rbac.py          # RBAC-specific models
│   ├── routers/             # API route definitions
│   ├── services/            # Business logic
│   │   └── rbac/            # RBAC-specific services
│   ├── auth/                # Authentication and authorization
│   ├── db/                  # Database configuration
│   │   ├── engine.py        # SQLAlchemy engine setup with Supabase
│   │   └── session.py       # Session management
│   ├── middleware/          # FastAPI middleware
│   ├── utils/               # Utility functions
│   └── config/              # Application configuration
├── docker/
│   ├── Dockerfile
│   ├── docker-compose.yml
│   └── docker-compose.dev.yml
└── scripts/
    └── check_tables.py
```

### Development Environment

- OS: Darwin 24.3.0 (macOS)
- Python: 3.11+
- Database: PostgreSQL 15 via Supabase
- FastAPI: Latest
- SQLAlchemy: 2.0+ (with async support)

### Key Environment Variables

```bash
# .env
DATABASE_URL=postgresql+asyncpg://postgres:password@aws-0-us-west-1.pooler.supabase.com:6543/postgres
JWT_SECRET_KEY=your_secret_key
DEFAULT_TENANT_ID=550e8400-e29b-41d4-a716-446655440000
ENVIRONMENT=development
SUPABASE_POOLER_HOST=aws-0-us-west-1.pooler.supabase.com
SUPABASE_POOLER_PORT=6543
SUPABASE_POOLER_USER=postgres.yourproject
```

## Database Configuration & Access Patterns

### SQLAlchemy Async Configuration

```python
# src/db/engine.py
from sqlalchemy.ext.asyncio import create_async_engine

# Connection string using Supabase Pooler
DATABASE_URL = f"postgresql+asyncpg://{settings.supabase_pooler_user}:{settings.supabase_db_password}@{settings.supabase_pooler_host}:{settings.supabase_pooler_port}/{settings.supabase_db_name}"

# Create the async engine with proper connection pooling settings
engine = create_async_engine(
    DATABASE_URL,
    pool_size=settings.db_min_pool_size,
    max_overflow=settings.db_max_pool_size - settings.db_min_pool_size,
    pool_timeout=settings.db_connection_timeout,
    pool_recycle=1800,
    echo=settings.sql_echo,
    connect_args={
        "ssl": "require",
        "statement_cache_size": 0  # Prevent prepared statement errors
    }
)
```

### Standardized Session Management

The application provides multiple patterns for session management:

```python
# src/db/session.py
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker
from contextlib import asynccontextmanager

# Create async session factory
async_session = async_sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False
)

# FastAPI dependency injection
async def get_db_session() -> AsyncGenerator[AsyncSession, None]:
    session = async_session()
    try:
        yield session
    finally:
        await session.close()

# Context manager pattern
@asynccontextmanager
async def get_session() -> AsyncGenerator[AsyncSession, None]:
    session = async_session()
    try:
        yield session
    finally:
        await session.close()

# Transaction context manager
@asynccontextmanager
async def transaction_context() -> AsyncGenerator[AsyncSession, None]:
    session = async_session()
    async with session.begin():
        try:
            yield session
        except Exception as e:
            logging.error(f"Transaction failed: {str(e)}")
            # Session will roll back automatically
            raise
        finally:
            await session.close()
```

### Required SQLAlchemy Usage Patterns

All database operations MUST use SQLAlchemy ORM or Query Builder - never raw SQL. Follow these patterns:

```python
# DO THIS: Use SQLAlchemy query builder
async def get_roles(session: AsyncSession, tenant_id: UUID) -> List[Role]:
    result = await session.execute(
        select(Role).where(Role.tenant_id == tenant_id)
    )
    return result.scalars().all()

# DO THIS: Use SQLAlchemy ORM
async def create_role(session: AsyncSession, name: str, tenant_id: UUID):
    role = Role(name=name, tenant_id=tenant_id)
    session.add(role)
    await session.flush()  # Get generated ID
    await session.commit()
    return role

# DON'T DO THIS: Raw SQL (only allowed for specific diagnostic scripts)
# await session.execute(text("SELECT * FROM roles"))
```

### Multi-Tenant Data Access

All queries must implement tenant isolation:

```python
# Standard pattern for tenant isolation
async def get_tenant_data(session: AsyncSession, tenant_id: UUID):
    # Always filter by tenant_id for tenant-specific data
    result = await session.execute(
        select(Model).where(Model.tenant_id == tenant_id)
    )
    return result.scalars().all()
```

## Docker Configuration

### Development Setup

```yaml
# docker-compose.yml
version: "3.8"
services:
  scrapersky:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=development
      - DATABASE_URL=${DATABASE_URL}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
    volumes:
      - ./static:/app/static:ro
      - ./src:/app/src:ro
      - ./.env:/app/.env:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
```

### Docker Commands

```bash
# Start development environment
docker-compose up -d

# View logs
docker-compose logs -f scrapersky

# Rebuild containers
docker-compose up -d --build

# Stop environment
docker-compose down

# Clean volumes
docker-compose down -v
```

## Alembic Migration Guide

### Initial Setup

```bash
# Initialize Alembic
alembic init alembic

# Configure alembic/env.py with async support
from asyncio import run
from src.models.base import Base
target_metadata = Base.metadata
```

### Creating Migrations

1. Generate Migration

```bash
# Create new migration
alembic revision --autogenerate -m "add_rbac_tables"
```

2. Review Generated Migration

```python
# alembic/versions/xxxx_add_rbac_tables.py
def upgrade():
    # Verify table creation order for foreign key dependencies
    op.create_table('roles',...)
    op.create_table('permissions',...)
    op.create_table('role_permissions',...)
    op.create_table('user_roles',...)

def downgrade():
    # Verify reverse order for clean rollback
    op.drop_table('user_roles')
    op.drop_table('role_permissions')
    op.drop_table('permissions')
    op.drop_table('roles')
```

### Migration Best Practices

1. **Handle Nullable Fields**

```python
# Bad
op.alter_column('roles', 'tenant_id', nullable=False)

# Good
# First add with default value
op.alter_column('roles', 'tenant_id',
    server_default='550e8400-e29b-41d4-a716-446655440000',
    nullable=True)
# Then make non-nullable after data migration
op.alter_column('roles', 'tenant_id', nullable=False)
```

2. **Manage Constraints**

```python
# Drop conflicting constraints first
op.drop_constraint('roles_name_key', 'roles', type_='unique')

# Add new constraints
op.create_unique_constraint('roles_tenant_name_unique', 'roles',
    ['tenant_id', 'name'])
```

3. **Data Migration**

```python
# Include data migration in upgrade
def upgrade():
    # Schema changes
    op.add_column('roles', sa.Column('tenant_id', sa.UUID()))

    # Data migration
    connection = op.get_bind()
    connection.execute("""
        UPDATE roles
        SET tenant_id = '550e8400-e29b-41d4-a716-446655440000'
        WHERE tenant_id IS NULL
    """)
```

### Running Migrations

1. **Development Environment**

```bash
# Apply migrations
docker-compose exec scrapersky alembic upgrade head

# Rollback one step
docker-compose exec scrapersky alembic downgrade -1

# Generate SQL without applying
docker-compose exec scrapersky alembic upgrade head --sql
```

2. **Production Environment**

```bash
# Check current state
alembic current

# Show migration history
alembic history --verbose

# Apply specific revision
alembic upgrade <revision_id>
```

## Troubleshooting Database Operations

### Common SQLAlchemy Issues

1. **Session Management**

Problem: Session not closed properly
Solution: Always use context managers or dependency injection:

```python
# Correct pattern
async with get_session() as session:
    # Use session here
    # Automatically closed when exiting context

# Also correct for route handlers
@router.get("/items")
async def get_items(session: AsyncSession = Depends(get_db_session)):
    # Session automatically closed after request
```

2. **Connection Pooling with Supabase**

Problem: Connection pool exhaustion
Solution: Monitor connection usage and adjust pool settings:

```python
engine = create_async_engine(
    DATABASE_URL,
    pool_size=5,               # Minimum connections
    max_overflow=10,           # Additional connections allowed
    pool_timeout=30,           # Seconds to wait for connection
    pool_recycle=1800,         # Recycle connections after 30 minutes
    pool_pre_ping=True         # Verify connections before use
)
```

3. **Tenant Isolation Failures**

Problem: Missing tenant filters
Solution: Create utility function for tenant filtering:

```python
def add_tenant_filter(query, tenant_id):
    model_class = query.column_descriptions[0]['entity']
    if hasattr(model_class, 'tenant_id'):
        return query.filter(model_class.tenant_id == tenant_id)
    return query
```

### Database Verification

```bash
# Connect to database through Docker
docker-compose exec scrapersky python scripts/check_tables.py

# Or with a simple script
python -c "
import asyncio
from sqlalchemy import text
from src.db.session import get_db_session

async def verify_tables():
    async for session in get_db_session():
        try:
            result = await session.execute(text('''
                SELECT table_name FROM information_schema.tables
                WHERE table_schema = 'public'
            '''))
            tables = result.fetchall()
            print('Available tables:')
            for table in tables:
                print(f'- {table[0]}')
        finally:
            await session.close()

asyncio.run(verify_tables())
"
```

## Development Workflow

### Local Development

```bash
# Start development environment
docker-compose up -d

# Create new migration after model changes
alembic revision --autogenerate -m "description"

# Apply migration
docker-compose exec scrapersky alembic upgrade head

# Verify changes
docker-compose exec scrapersky python scripts/check_tables.py

# Run specific tests
docker-compose exec scrapersky pytest tests/services/rbac/
```

### Database Access Helpers

Create a `db_tools.py` script for commonly needed database operations:

```python
#!/usr/bin/env python
"""
Database Tools

Usage:
  python db_tools.py tables      # List all tables
  python db_tools.py schema <table>   # Show table schema
  python db_tools.py count <table>    # Count rows in table
"""
import asyncio
import sys
import os
from sqlalchemy import text
from src.db.session import get_session

async def main():
    if len(sys.argv) < 2:
        print(__doc__)
        return

    command = sys.argv[1]
    async with get_session() as session:
        if command == "tables":
            result = await session.execute(text(
                "SELECT table_name FROM information_schema.tables WHERE table_schema='public'"
            ))
            tables = [row[0] for row in result.fetchall()]
            print("\nDatabase Tables:")
            for table in tables:
                print(f"- {table}")

        elif command == "schema" and len(sys.argv) > 2:
            table = sys.argv[2]
            result = await session.execute(text(
                f"SELECT column_name, data_type, is_nullable FROM information_schema.columns WHERE table_name='{table}'"
            ))
            print(f"\nSchema for {table}:")
            print(f"{'COLUMN':<30} {'TYPE':<20} {'NULLABLE':<10}")
            print("-" * 60)
            for row in result.fetchall():
                print(f"{row[0]:<30} {row[1]:<20} {row[2]:<10}")

        elif command == "count" and len(sys.argv) > 2:
            table = sys.argv[2]
            result = await session.execute(text(f"SELECT COUNT(*) FROM {table}"))
            count = result.scalar()
            print(f"\n{table} has {count} rows")

if __name__ == "__main__":
    asyncio.run(main())
```

## Health Checks

```bash
# Check API health
curl http://localhost:8000/health

# Check database connection
curl http://localhost:8000/health/database
```

## Monitoring and Maintenance

### Database Maintenance

```bash
# Check Supabase connection health
python scripts/test_connection.py

# Check for slow queries
docker-compose exec scrapersky python -c "
import asyncio
from sqlalchemy import text
from src.db.session import get_session

async def check_slow_queries():
    async with get_session() as session:
        result = await session.execute(text('''
            SELECT query, calls, total_time, mean_time
            FROM pg_stat_statements
            ORDER BY mean_time DESC
            LIMIT 10
        '''))
        print('Slow queries:')
        for row in result.fetchall():
            print(f'Query: {row[0][:100]}...')
            print(f'Calls: {row[1]}, Avg time: {row[3]}ms')
            print('-' * 80)

asyncio.run(check_slow_queries())
"
```
