# AI Onboarding — ScraperSky Backend

This summary distills the key project context and standards. A new AI assistant should read this top to bottom before any further questions.

---

## 1. ScraperSky Backend Overview

A FastAPI-based web scraping and analytics system with modern SQLAlchemy integration, RBAC security, and multi-tenant architecture.

### ⚠️ CRITICAL ARCHITECTURAL REQUIREMENTS ⚠️

#### 1. NEVER USE RAW SQL IN APPLICATION CODE

```
⚠️ ABSOLUTELY NON-NEGOTIABLE ⚠️
┌─────────────────────────────────────────────────────┐
│ NEVER USE RAW SQL IN APPLICATION CODE               │
│                                                     │
│ ✅ ALWAYS use ORM methods                           │
│ ❌ NEVER write raw SQL queries                      │
└─────────────────────────────────────────────────────┘
```

- **ALWAYS** use SQLAlchemy ORM methods for ALL database operations
- **NEVER** write raw SQL with `text()` or `session.execute()`
- Access data through model classes like `Domain`, `User`, etc.
- Use model methods like `update_from_metadata()` instead of raw queries

#### 2. CONNECTION CONFIGURATION REQUIREMENTS

**Always use Supavisor connection pooling with these parameters:**

- Connection string: `postgresql+asyncpg://postgres.your-project:password@aws-0-us-west-1.pooler.supabase.com:6543/postgres`
- Required parameters in connection configuration ONLY:
  - `raw_sql=true` - For Supavisor compatibility
  - `no_prepare=true` - For Supavisor compatibility
  - `statement_cache_size=0` - For Supavisor compatibility

**IMPORTANT**: These parameters are ONLY for connection configuration, NOT permission to use raw SQL in your code.

#### 3. MODEL REQUIREMENTS

- All SQLAlchemy models must match the database schema exactly
- Include proper relationship configurations

**Failure to follow these requirements will result in application failures and wasted debugging time.**

### Database Setup

**IMPORTANT:** This project connects to an **external Supabase database**.

- **Configuration:** Connection details are configured via environment variables (typically loaded from a `.env` file based on `.env.example`). Ensure the following variables are set correctly:
  - `SUPABASE_URL`
  - `SUPABASE_POOLER_HOST` (Recommended for connection pooling)
  - `SUPABASE_POOLER_PORT`
  - `SUPABASE_POOLER_USER`
  - `SUPABASE_DB_PASSWORD`
  - `DATABASE_URL` (Often constructed from the Supabase variables)
- **Local Docker:** The `docker-compose.yml` file in this project **does not** define or manage a local database service. It only runs the application service (`scrapersky`) which connects to the external database specified in the environment variables.
- **Inspection:** To manually inspect the database schema or data, use the primary inspection script. See the "Database Tools" section below.

### Project Overview

ScraperSky provides a robust backend platform for website metadata extraction, with a focus on security, scalability, and maintainability. It features:

- **Modern FastAPI Architecture**: Asynchronous API endpoints with dependency injection
- **SQLAlchemy 2.0 Integration**: Type-safe database operations with async support
- **Multi-Tenant Design**: Complete tenant isolation across all endpoints
- **Role-Based Access Control**: Fine-grained permission system with feature flags
- **Containerized Deployment**: Docker-based development and production environments

### Application Workflow

The system follows a multi-stage pipeline for discovering and processing web data:

1.  **Discovery (Single Search / Batch Search):** Initial discovery via Google Maps API.
2.  **Triage (Staging Editor):** Raw search results are reviewed and assigned initial status.
3.  **Business Curation (Local Business Curation):** Relevant businesses undergo further review and domain extraction.
4.  **Domain Curation (Domain Curation):** Select domains for deeper analysis (sitemap scraping).
5.  **Sitemap Curation (Sitemap Curation):** Review discovered sitemaps and queue for deep scraping.
6.  **Results Viewing (Results Viewer):** View consolidated and enriched data.
7.  **Dual-Status Queueing:** When you mark an item "Selected" in any of these Curation tabs—Staging Editor, Local Business Curation, Domain Curation, or Sitemap Curation—the API atomically sets both the main `*_curation_status` and the matching `*_analysis_status` field (e.g. `deep_scan_status`, `domain_extraction_status`, `sitemap_analysis_status`) to `Queued` within the same database transaction, which APScheduler then picks up.

Data progresses: `Discovery` → `places_staging` → `local_businesses` → `domains` → `sitemap_files` → `pages`.

### Quick Start

**Local Development Testing**

```bash
# Required Test Credentials
DEV_TOKEN=scraper_sky_2024
DEFAULT_TENANT_ID=550e8400-e29b-41d4-a716-446655440000

# Add to your .env file for local testing
ENVIRONMENT=development
```

These credentials are pre-configured in all test HTML files for local development.

**IMPORTANT**: While the `DEV_TOKEN` provides authentication, operations involving `created_by` or `updated_by` fields often require a valid user UUID to satisfy database foreign key constraints (e.g., `sitemap_files.updated_by` references `users.id`).

Use the specific test user UUIDs documented in [10-TEST_USER_INFORMATION.md](/Docs/Docs_1_AI_GUIDES/10-TEST_USER_INFORMATION.md) (like `5905e9fe-6c61-4694-b09a-6602017b000a`) for these operations, and ensure these users exist in your development database (they should be present by default). The `get_current_user` dependency in `src/auth/jwt_auth.py` is configured to map `DEV_TOKEN` to the primary test user UUID in development environments.

**Primary Development Interface:**

During active MVP development, the primary interface for testing backend functionality and workflows is `/static/scraper-sky-mvp.html`. This page consolidates various testing tabs related to core services like LocalMiner, ContentMap, etc., and should be used as the main entry point for interacting with the backend API via a UI.
For the canonical UI→backend mapping (six‑column table + Appendices A/B), see `Docs/Docs_0_Architecture_and_Status/0.1_ScraperSky_Architecture_Flow_and_Components.md`.

### Development Environment

```bash
# Clone the repository
git clone https://github.com/your-org/scraper-sky-backend.git
cd scraper-sky-backend

# Set up environment variables
cp .env.example .env
# Edit .env with your credentials

# Start development environment
docker-compose up -d

# Apply database migrations
docker-compose exec app alembic upgrade head

# Access API documentation
open http://localhost:8000/docs
```

### Testing

```bash
# Run all tests
pytest

# Run specific test modules
pytest tests/services/rbac/
```

### Project Structure

```
scraper-sky-backend/
├── src/                      # Application source code
│   ├── main.py               # Application entry point
│   ├── models/               # SQLAlchemy models
│   ├── routers/              # API route definitions
│   ├── services/             # Business logic
│   ├── auth/                 # Authentication & authorization
│   ├── db/                   # Database configuration
│   └── middleware/           # FastAPI middleware
├── tests/                    # Test suite
├── scripts/                  # Utility scripts
│   ├── batch/                # Batch processing scripts
│   ├── db/                   # Database tools (NOTE: Refactored April 2025 - see Database Tools section)
│   ├── fixes/                # Fix scripts
│   ├── maintenance/          # Maintenance tools
│   ├── migrations/           # Database migration helpers
│   ├── monitoring/           # Job and batch monitoring
│   ├── sitemap/              # Sitemap testing and analysis
│   ├── testing/              # Testing utilities
│   └── utility/              # Development utilities
├── examples/                 # Example implementations
├── migrations/               # Alembic migrations
├── Docs/                     # Project documentation
└── static/                   # Static files
```

### Key Features

- **FastAPI Endpoints:**

The application provides API endpoints for:

- Website metadata extraction
- Role-based access control
- User and tenant management
- Feature flag management

### Database Integration

- **SQLAlchemy 2.0**: Uses the latest SQLAlchemy with async support
- **Connection Pooling**: Properly configured for Supabase
- **Migration Management**: Alembic for database schema evolution
- **Prepared Statements**: The system uses SQLAlchemy ORM by default, but endpoints support multiple parameters to bypass prepared statements when needed:
  - `raw_sql=true` - Tells the backend to use raw SQL instead of ORM
  - `no_prepare=true` - Disables prepared statements
  - `statement_cache_size=0` - Sets the asyncpg statement cache size to 0
  - **Note**: All three parameters must be used together for the profiles endpoint

> **IMPORTANT**: The system exclusively uses Supavisor for connection pooling. Any PgBouncer configurations should be immediately flagged and removed as they are incompatible with our architecture.

### Authentication & Authorization

- **JWT Authentication**: Secure token-based auth
- **RBAC System**: Fine-grained permission management
- **Tenant Isolation**: Complete data separation between tenants

### Deployment

ScraperSky is deployed to Render.com using the configuration in `render.yaml`:

```bash
# Deploy to Render.com
render deploy
```

### Database Tools

The primary tool for manually inspecting the database (viewing schemas, data, enums) is `scripts/db/db_inspector.py`.

**How to Use:**

1.  Ensure you are in the project root directory.
2.  Ensure the required `SUPABASE_*` environment variables are set (e.g., loaded from your `.env` file).
3.  Run the script as a module:

```bash
# Show help and usage instructions
python -m scripts.db.db_inspector --help

# List all tables
python -m scripts.db.db_inspector

# Inspect a specific table (e.g., domains)
python -m scripts.db.db_inspector domains

# Inspect a table with filtering and limits
python -m scripts.db.db_inspector sitemap_files --where "status = 'Completed'" --limit 50
```

Refer to the extensive docstring within `scripts/db/db_inspector.py` for detailed argument explanations and more examples.

### Scheduler Configuration

#### Domain Scheduler

The domain scheduler processes domains with 'pending' status and can be configured with the following environment variables (defaults set in `docker-compose.yml`):

```bash
# How often the scheduler runs (in minutes)
# Default: 1 minute (set for MVP development, consider increasing later)
DOMAIN_SCHEDULER_INTERVAL_MINUTES=${DOMAIN_SCHEDULER_INTERVAL_MINUTES:-1}

# Number of domains processed in each batch
```

Also see **Appendix B** in the UI→Backend Mapping doc (`Docs/Docs_0_Architecture_and_Status/0.1_ScraperSky_Architecture_Flow_and_Components.md`) for APScheduler lifecycle (startup/shutdown), polling intervals, isolated‑session pattern, diagnostic endpoints, and error‑handling best practices.

## 2. Summarized Guides & Checklists

### 2.1 High‑Level Architecture (from **README.md**)

- **ORM only**: Use SQLAlchemy ORM (no raw SQL), with Supavisor connection parameters.
- **Workflow**:
  1. Discovery (Single/Batch Search)
  2. Triage (Staging Editor)
  3. Business Curation (Local Business Curation → Domain Extraction)
  4. Domain Curation (→ Sitemap Analysis)
  5. Sitemap Curation (→ Deep Scrape)
  6. Results Viewer
- **Deployment**: Docker Compose (`scrapersky` service), health check at `/health`.

### 2.2 Debugging Checklist & Gotchas (from **26‑Supplemental.md**)

- **API checks**: method, path, params, headers, body, auth token.
- **Schema match**: Pydantic request/response vs. actual payload.
- **Logs & Tracebacks**: unmask hidden exceptions.
- **DB vs. Model**: Enum case‑sensitivity, schema drift.
- **Transactions**: commit/rollback, created_by/updated_by.
- **Supabase FK**: reference `profiles.id` not `auth.users`.
- **Enum updates**: always use `Enum.value`.

### 2.3 Debugging Cheat Sheet: Lessons from Sitemap Files PUT Endpoint (April 11, 2025)

#### Introduction

This document summarizes the painful, multi‑step debugging process required to fix the `PUT /api/v3/sitemap-files/{id}` endpoint. It serves as a "lessons learned" guide to prevent future AI assistants (or humans) from repeating these time‑consuming mistakes. The core issue involved a cascade of errors stemming from incorrect API usage, schema mismatches, data integrity problems, flawed transaction handling, incorrect model definitions, and misunderstanding of authentication/test user conventions.

#### Core Principles Refresher & Checklist

Before diving deep, always confirm these fundamentals:

1. **✅ Verify API Call:**

   - **Method:** Is it correct (PUT, POST, GET, DELETE)?
   - **Path:** EXACTLY matches the router prefix + endpoint path? (e.g., `/api/v3/sitemap-files/{uuid}`). Check `src/routers/...` and `src/main.py`. Watch for trailing slashes.
   - **Query Parameters (GET):** Are the parameter names sent by the frontend (e.g., `?domain_filter=...`) EXACTLY matching the names expected by the backend router function? Mismatched names are a common cause of silent filter failures.
   - **Headers:** Correct `Content-Type`, `Authorization` (Bearer token)?
   - **Body (PUT/POST):** Valid JSON? Contains all required fields with correct field names?
   - **Test User/Token:** Using the correct token (`scraper_sky_2024`) and understanding the actual user UUID it maps to.

2. **✅ Check Pydantic Schemas (`src/schemas/...`):**

   - **Input Schema:** Includes all fields sent in the request body.
   - **Output Schema:** Includes all fields expected in the API response; use aliases if API names differ.

3. **✅ Check Logs FIRST on Errors:**

   - Run `docker-compose logs --tail=100 scrapersky` immediately after a failed request.
   - Look for underlying errors (e.g., `LookupError`, `IntegrityError`, `DatatypeMismatchError`).

4. **✅ Check Database Data & Schema:**

   - Use `scripts/db/simple_inspect.py` to inspect column types and enum definitions.
   - Verify data integrity (case‑sensitivity in enums) and fix bad data directly via SQL if needed.

5. **✅ Check SQLAlchemy Model Definition (`src/models/...`):**

   - Ensure `Column(...)` matches the actual DB schema type (`SQLAlchemyEnum` for enums).
   - Confirm `nullable`, `ForeignKey`, and `relationship` definitions.

6. **✅ Check Transaction Handling (`src/db/session.py`):**

   - Verify `commit()`, `rollback()`, and `close()` are correctly placed in `get_db_session`.

7. **✅ Check Auth & User ID Logic (`src/auth/jwt_auth.py`):**

   - Confirm `get_current_user` returns a valid UUID from `10-TEST_USER_INFORMATION.md`.
   - Ensure FK constraints on `created_by`/`updated_by` reference `profiles.id`.

8. **✅ Consult Documentation:**

   - `README.md`, `Docs/Docs_1_AI_GUIDES`, and relevant `project-docs/`.

9. **✅ Follow Process:**
   - Ask permission before editing. Explain reasoning before proposing changes. Add clear comments on significant edits.

#### Common Pitfalls & Solutions

- The following specific issues were encountered during the `PUT /sitemap-files/{id}` debugging but illustrate the types of problems the above checklist can uncover:

* **Initial 404:** Incorrect `curl` path (missing `/v3/`).
* **Query Param Mismatch:** Frontend vs. backend parameter names.
* **400 Bad Request:** Missing field in `SitemapFileUpdate` schema.
* **Silent 404:** Case mismatch in enum values.
* **Missing `commit()`:** Session never persisted changes.
* **DatatypeMismatchError:** Column defined incorrectly in model.
* **IntegrityError FK Violations:** `get_current_user` returning non-existent UUID or wrong schema (use `profiles.id`).
* **Enum Update Fragility:** Always use `.value` when calling `.update().values()`.
* **JS Init Errors:** Wrap tab‑specific setup in init functions triggered on tab activation to avoid `null` elements.
* **API Data Mismatch:** Match JS field access to actual Pydantic `Read` schema.
* **DB Enum Mismatch:** Ensure Python enum values exactly match DB enum labels; verify with `SELECT enum_range(NULL::your_enum_type);`.
* **Direct SQL Fixes:** Use a Python script inside the container to run `ALTER TYPE` or `UPDATE` statements when `psql` is unavailable.

### 2.4 Coding & AI Guides (from **Docs/Docs_1_AI_GUIDES**)

(from **Docs/Docs_1_AI_GUIDES**)

- **Connection standards**: AsyncPG, UUID conventions.
- **Enum handling**: model integrity, label consistency.
- **Router conventions**: prefix, versioning.
- **Auth & RBAC**: boundary enforcement, test UUIDs.
- **Testing**: unit/integration patterns, debug scripts.
- **Scheduler patterns**: APScheduler, shared job integration.

## 3. Mappings & Status Summaries

### 3.1 UI → Pipeline Mapping (from **static/scraper-sky-mvp.html**)

| Tab                     | Role                                         |
| ----------------------- | -------------------------------------------- |
| Single Search           | Discovery, 1‑off lookup                      |
| Batch Search            | Bulk discovery                               |
| Staging Editor          | Triage: Review "New" businesses              |
| Local Business Curation | Business Curation: Filter & status updates   |
| **Domain Curation**     | Domain Curation: Select domains for analysis |
| Sitemap Curation        | Sitemap Curation: Review discovered sitemaps |
| Results Viewer          | Results Viewing: Final enriched output       |

### 3.2 Service Startup & Scheduling (from **docker‑compose.yml**)

- **Service**: `scrapersky` container with healthcheck
- **Schedulers registered** (in `main.py` startup hook):
  - `process_pending_domains` (from `src/services/domain_scheduler.py`)
  - `process_pending_jobs` (from `src/services/sitemap_scheduler.py`)
  - `process_pending_domain_sitemap_submissions` (from `src/services/domain_sitemap_submission_scheduler.py`)
- **Next step**: scheduler scans for `sitemap_analysis_status = 'Queued'` and invokes internal scan API.

### 3.3 Service-to-Component Mapping (from Document One)

| Service Name         | Conceptual Goal                                     | Primary UI Element(s)                                                                                                                            | UI JS File(s)                                                                                                                                               | Backend Router(s)                                                                                                | Primary DB Table(s)                                     | Background Job(s) / Trigger Mechanism                                                                                                                                           |
| :------------------- | :-------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **LocalMiner**       | Find & process local business data from Google Maps | `scraper-sky-mvp.html` (`Single Search`, `Staging Editor`, `Local Business Curation`, `Domain Curation`, `Batch Search`, `Results Viewer` tabs)  | `single-search-tab.js`, `staging-editor-tab.js`, `local-business-curation-tab.js`, `domain-curation-tab.js`, `batch-search-tab.js`, `results-viewer-tab.js` | `localminer_discoveryscan.py`, `places_staging.py`, `local_businesses.py`, `domains.py`, `batch_page_scraper.py` | `jobs`, `places_staging`, `local_businesses`, `domains` | Direct Action (Search/Batch), Status Change (Staging/Local Business/Domain Curation) -> `sitemap_scheduler.py`, `domain_scheduler.py`, `domain_sitemap_submission_scheduler.py` |
| **ContentMap**       | Analyze website sitemaps for structure & content    | `scraper-sky-mvp.html` (`Sitemap Curation` tab), Potentially `contentmap.html` (Needs verification)                                              | `sitemap-curation-tab.js`                                                                                                                                   | `sitemap_files.py`, `sitemap.py` (for scan/status)                                                               | `sitemap_files`, `sitemap_urls`, `domains`              | Status Change (Sitemap Curation) -> `sitemap_scheduler.py`                                                                                                                      |
| **FrontendScout**    | (Likely) Analyze frontend technologies              | Screenshot exists; No specific UI tab/file identified in `scraper-sky-mvp.html`. Needs definition/implementation.                                | _TBD_                                                                                                                                                       | _TBD_                                                                                                            | _TBD_                                                   | _TBD_                                                                                                                                                                           |
| **SiteHarvest**      | Extract metadata & content from websites            | Screenshot exists; No dedicated UI tab/file identified. Functionality integrated into `Domain` processing.                                       | Integrated into other flows (e.g., `domain-curation-tab.js`)                                                                                                | `metadata_extractor.py` (Service, not Router), `domains.py`                                                      | `domains`, `pages`                                      | Triggered as part of Domain processing / Deep scraping (via `sitemap_scheduler.py` or `domain_scheduler.py`)                                                                    |
| **SocialRadar**      | Find & analyze social media profiles                | Screenshot shows "Social Media Staging"; No dedicated UI tab/file identified. Functionality integrated into `Domain` processing.                 | Integrated into other flows (e.g., `domain-curation-tab.js`)                                                                                                | `metadata_extractor.py` (Service, not Router), `domains.py`                                                      | `domains`                                               | Triggered as part of Domain processing (via `domain_scheduler.py`)                                                                                                              |
| **ContactLaunchpad** | Manage & enrich contact data                        | Screenshot shows "Contact Staging"; No dedicated UI tab/file identified. Email finding implemented. "Launchpad" (export/enrich) likely needs UI. | _TBD_ for Launchpad features. Email scraping is background only.                                                                                            | `email_scanner.py` (for scan initiation/status), _TBD_ for Launchpad API                                         | `contacts`, `jobs`                                      | Direct Action (Email Scan API Call) -> Background Task (`email_scraper.py` task)                                                                                                |
| **Domain Scanner**   | Scan domains for tech stacks & contacts             | Screenshot exists; Possibly integrated into `Domain Curation` tab or a separate view.                                                            | `domain-curation-tab.js` or _TBD_                                                                                                                           | `domains.py`                                                                                                     | `domains`                                               | Status Change -> `domain_scheduler.py`                                                                                                                                          |
| **Domains View**     | View & manage domain data                           | Screenshot exists; Possibly integrated into `Domain Curation` tab or a separate view.                                                            | `domain-curation-tab.js` or _TBD_                                                                                                                           | `domains.py`                                                                                                     | `domains`                                               | View Only                                                                                                                                                                       |

### 3.4 Implementation Status & Architecture (from Document Two)

- **Overall Status:**
  - Backend Workflow Logic: ~80%
  - Frontend Integration & UI: ~40%
  - Overall System (towards functional MVP): ~65-70%
- **Key Components:**
  - **LocalMiner (Discovery):** ~75%
  - **Staging Editor (Deep Scan Queueing):** ~70% backend, ~40% frontend
  - **Local Business Curation (Domain Extraction Queueing):** ~70% backend, ~40% frontend
  - **Domain Curation (Metadata/Sitemap Analysis Queueing):** ~70% backend, ~40% frontend
  - **Sitemap Curation (Deep Scrape Queueing):** ~70% backend, ~40% frontend
  - **Batch Search:** ~60%
  - **Results Viewer:** ~80% backend, ~30% frontend
  - **ContentMap (Sitemap Analysis):** ~60%
  - **FrontendScout:** ~20% (~interface skeleton present; backend endpoints minimal)
  - **SiteHarvest:** ~50%
  - **EmailHunter:** ~80%
  - **ActionQueue (Schedulers/Polling):** ~80%
  - **SocialRadar:** ~70%
  - **ContactLaunchpad:** ~30%
- **Technical Architecture:**
  - FastAPI, SQLAlchemy Async, PostgreSQL via Supavisor
  - APScheduler polling on status columns
  - Dockerized with health check & schedulers: `sitemap_scheduler.py`, `domain_scheduler.py`, `domain_sitemap_submission_scheduler.py`
- **Major Challenges Overcome:**
  - Multi‑Stage Async Workflow via DB status polling
  - Robust scraping & parsing components
  - Strict ORM enforcement
  - Supavisor configuration

### 3.5 Audit & Mapping Roadmap (from Document Three)\*\* Audit & Mapping Roadmap (from Document Three)

- **Purpose:** Track architectural audits, refinements, and detailed workflow mapping tasks.
- **Completed:** ORM enforcement, Supavisor pooling, Authentication simplification, APScheduler patterns, Job model, Enum standardization, JS modularity, concurrency control, linting, API standardization, transaction patterns.
- **Pending Audits & Tasks:**
  - Workflow Documentation: Email, SiteHarvest, ContactLaunchpad
  - Component Specification: FrontendScout
  - Verification & Testing: End‑to‑end status transitions, performance, test coverage
  - Code & Architecture Audit: Naming, unused code, error/log consistency
  - Frontend: Verify all static tabs, UI polish
- **Workflow Mapping Spec:**
  - ASCII diagrams per feature showing UI → Router → Service → DB → Scheduler
  - Include file paths, function names, and principle callouts
  - Initial workflows: LocalMiner Search, Curation steps, Domain, Sitemap, EmailHunter, Batch, Results Viewer
  - Expand to other services once defined

---

## 4. Master Prompt for AI Usage

```text
You are a specialized AI assistant for the ScraperSky backend. Reference `0.0_AI_Project_Primer.md` (this document) for:
1. Overview & Critical Requirements
2. Summarized Guides & Checklists
3. Mappings & Status Summaries

You are aware of all core components, UI flows, background jobs, and the remaining roadmap. Help with any further questions, code generation, debugging, or workflow map creation in the project's style and conventions.
```

---

## Environment & Tooling Pitfalls (Critical Onboarding Note)

> **Warning:**
> Developing, testing, or running scripts for ScraperSky requires careful attention to environment boundaries:
> - **Host venv ≠ Docker container**: Installing packages locally does NOT affect the container. Always run scripts and install dependencies inside the container.
> - **Volume mounts matter**: If you don’t mount all relevant directories (like `tools/`), your scripts may be invisible to the container.
> - **User site-packages confusion**: `pip install --user` can put packages where Python won’t see them, especially in Docker multi-stage builds.
> - **Docker build caching**: Use `docker-compose build --no-cache` after changing requirements.
> - **Entrypoint confusion**: `python -m ...` and `python ...` can resolve imports differently.
>
> **Always consult the Troubleshooting section in the [README](../../README.md) and the [Work Order Progress Log](../../project-docs/41-Code-Audit-And-Archive/Work-Order-Progress.md#common-pitfalls--lessons-learned-2025-04-18) for up-to-date solutions.**
