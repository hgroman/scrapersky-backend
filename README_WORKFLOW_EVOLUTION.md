# ScraperSky Backend - Complete Workflow Evolution Context

**For Future AI Development Partners - Get Up to Speed Quickly**

## üéØ **What You're Looking At**

This is a **production-grade B2B data enrichment platform** that has evolved through **multiple AI pairing sessions** to reach **operational excellence**. The system processes web data through a sophisticated **7-layer architecture** with **7 sequential workflows (WF1-WF7)**.

## üöÄ **Current State: Production Ready**

- **WF7 Contact Extraction**: Enhanced with concurrent processing (5-10x performance improvement)
- **Select All Functionality**: 3,254 pages can now be batch processed efficiently
- **Guardian Authority System**: 150% confidence validation patterns established
- **Documentation**: Comprehensively organized with evidence-based development approach

## üìã **Quick Context for AI Partners**

### **If You're Working on WF7 (Contact Extraction):**
```bash
# Essential reading order:
1. Read: Docs/40_WF7-Multi-Thread-ScraperAPI/README_Implementation_Context.md
2. Review: Docs/40_WF7-Multi-Thread-ScraperAPI/WF7_SELECT_ALL_ENHANCEMENT.md
3. Run tests: Docs/40_WF7-Multi-Thread-ScraperAPI/Testing_Suite/
4. Check: src/services/WF7_V2_L4_1of2_PageCurationService.py
```

### **If You're Working on System Architecture:**
```bash
# Start here:
1. Read: Docs/01_Architectural_Guidance/START_HERE_ARCHITECT_PROTOCOL.md
2. Review: Docs/02_State_of_the_Nation/CRITICAL_NEVER_MODIFY.md
3. Check: CLAUDE.md (development commands and patterns)
4. Understand: Docs/41_Production_Issues/ (known critical issues)
```

### **If You're Debugging Production Issues:**
```bash
# Critical bug context:
1. WF5 Sitemap Bug: Docs/41_Production_Issues/wf5_sitemap_parsing_bug_issue.md
2. Database Issues: Docs/41_Production_Issues/database_session_audit_report.txt
3. Debug Tools: Docs/30_Debug_Tools/ (API testing and sitemap debugging)
```

## üõ°Ô∏è **Guardian Authority System**

This project uses a **multi-layer Guardian system** for code validation:

- **L0 Architect**: Writes ALL code implementation
- **L1-L7 Guardians**: Review and approve by layer (Models, Schemas, Routers, Services, Config, UI, Testing)
- **WF7 Production Reality Guardian v2**: Has **150% confidence authority** for WF7 operations

**Critical**: Respect Guardian validation patterns - they represent battle-tested, production-validated approaches.

## üèóÔ∏è **Architecture Overview**

### **7-Layer Architecture:**
```
L1: Models & Enums (src/models/)
L2: Schemas (src/schemas/) 
L3: Routers (src/routers/)
L4: Services (src/services/)
L5: Configuration (src/config/)
L6: UI Components (static/)
L7: Testing (tests/, Testing_Suite/)
```

### **7 Sequential Workflows:**
```
WF1: Single Search Discovery ‚Üí WF2: Staging Editor ‚Üí WF3: Local Business Curation
     ‚Üì
WF4: Domain Curation ‚Üí WF5: Sitemap Curation ‚Üí WF6: Sitemap Import ‚Üí WF7: Contact Extraction
```

### **Database Architecture:**
- **Primary**: Supabase PostgreSQL with connection pooling (Supavisor on port 6543)
- **Critical**: Uses `raw_sql=true`, `no_prepare=true`, `statement_cache_size=0` for Supavisor compatibility
- **Pattern**: Async sessions via `src/session/async_session.py`

## üéØ **Recent Major Achievements**

### **WF7 Concurrent Processing Implementation** *(Most Recent)*
- **Problem**: Sequential page processing bottleneck (1 page at a time)
- **Solution**: asyncio.gather() with semaphore rate limiting + ScraperAPI connection pooling
- **Result**: 5-10x performance improvement, Select All functionality for 3,254 pages
- **Authority**: Guardian-validated with 150% confidence
- **Evidence**: Complete testing suite and performance benchmarks in `Docs/40_WF7-Multi-Thread-ScraperAPI/`

### **Documentation Organization** *(This Session)*
- **Problem**: Critical files scattered across repository root
- **Solution**: Comprehensive organization into logical `Docs/` structure
- **Result**: Clean repository with context-rich documentation for AI partners
- **Evidence**: 2 new documentation categories (Production Issues, Pipeline Execution Evidence)

### **Critical File Recovery** *(Crisis Management)*
- **Problem**: Git analyst recommended deleting critical operational files
- **Solution**: Manual review and recovery of all critical documentation
- **Result**: Zero data loss, all critical evidence preserved
- **Lesson**: AI agents need clear boundaries between librarian vs. purge authority

## üö® **Known Critical Issues**

### **WF5 Sitemap Bug - PRODUCTION CRITICAL**
- **Impact**: Only extracting 1 URL instead of 7 from sitemaps (86% data loss)
- **Location**: `src/scraper/sitemap_analyzer.py` parse_sitemap() method
- **Evidence**: Complete analysis in `Docs/41_Production_Issues/wf5_sitemap_parsing_bug_issue.md`
- **Debug Tools**: `Docs/30_Debug_Tools/test_sitemap_parser_debug.py`

### **Database Session Management** 
- **Impact**: 33 potential issues including double transaction management
- **Evidence**: `Docs/41_Production_Issues/database_session_audit_report.txt`
- **Pattern**: Avoid manual commit/rollback in context managers

## üîß **Development Patterns & Standards**

### **Database Connections - CRITICAL**
```python
# CORRECT - Supavisor compatible
postgresql+asyncpg://user:pass@host:6543/db?raw_sql=true&no_prepare=true&statement_cache_size=0

# WRONG - Will break connection pooling
postgresql+asyncpg://user:pass@host:5432/db
```

### **Async Session Pattern**
```python
# CORRECT
async with get_background_session() as session:
    # Context manager handles commit/rollback

# WRONG
async with get_background_session() as session:
    await session.commit()  # Double transaction management
```

### **Guardian Validation Pattern**
```python
# Before implementing, present to relevant Guardian
# Guardian reviews (doesn't rewrite)
# Guardian APPROVES or REJECTS
# Only proceed with approval
```

## üìä **Evidence-Based Development**

This project follows **evidence-based development** with:
- **Complete execution logs**: `Docs/42_Pipeline_Execution_Evidence/`
- **Guardian validation tests**: Confidence levels documented
- **Performance benchmarking**: Before/after comparisons required
- **Production verification**: MCP database tools for real-time validation

## üöÄ **Getting Started Quickly**

### **For WF7 Enhancement:**
1. Run the test suite: `python Docs/40_WF7-Multi-Thread-ScraperAPI/Testing_Suite/test_concurrent_simple.py`
2. Understand current performance: Review benchmark logs
3. Respect Guardian patterns: 150% confidence authority established

### **For Bug Fixing:**
1. Check `Docs/41_Production_Issues/` for known issues
2. Use `Docs/30_Debug_Tools/` for testing and validation
3. Follow async session patterns to avoid database issues

### **For Architecture Work:**
1. Read `Docs/01_Architectural_Guidance/` for patterns and protocols
2. Understand the 7-layer system before making changes
3. Respect the Guardian review process

## üéØ **Success Metrics Established**

- **Performance**: WF7 concurrent processing 5-10x improvement achieved
- **Scalability**: 3,254 pages can be batch processed (vs. 15-page limitation)
- **Reliability**: Guardian-validated patterns with 150% confidence
- **Maintainability**: Complete context documentation for future AI partners
- **Production Safety**: Critical files identified and protected

## üõ°Ô∏è **For Future AI Partners**

This repository represents **months of AI pairing evolution** with:
- **Battle-tested patterns** validated through production use
- **Guardian authority systems** that should be respected
- **Complete implementation stories** with evidence and benchmarks
- **Context-rich documentation** designed for rapid AI partner onboarding

**When in doubt**: Look for Guardian validation patterns, respect established authority levels, and maintain the evidence-based development approach that has made this system successful.

---

**This README represents the distilled wisdom of multiple AI pairing sessions. Use it to quickly understand the current state and continue the evolution of this sophisticated data enrichment platform.**